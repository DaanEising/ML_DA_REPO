{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "a96dba24",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "import httpx\n",
    "import re\n",
    "import requests\n",
    "import codecs\n",
    "import time\n",
    "import nltk\n",
    "\n",
    "import threading\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "import yfinance as yf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "eebd4d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #constants\n",
    "# HEADERS = {'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "#            'Accept-Encoding': 'gzip, deflate, br',\n",
    "#            'Accept-Language': 'en-US,en;q=0.9,nl;q=0.8',\n",
    "#            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'}\n",
    "\n",
    "# DATA_LOCATION = 'Datasets/'\n",
    "\n",
    "# HOMEPAGE_URL = 'https://www.investopedia.com/markets-news-4427704'\n",
    "\n",
    "# VUSA_DOWNLOAD_URL = 'https://datahub.io/core/s-and-p-500-companies'\n",
    "# SP_DOWNLOAD_URL = 'https://datahub.io/core/s-and-p-500'\n",
    "# DATAHUB_URL = 'https://datahub.io'\n",
    "\n",
    "# YAHOO_URL = 'https://finance.yahoo.com/'\n",
    "\n",
    "# #don\n",
    "# nltk.download(['punkt', 'wordnet', 'stopwords', 'omw-1.4', 'vader_lexicon'])\n",
    "\n",
    "# options = Options()\n",
    "# options.add_argument('--headless')\n",
    "# options.add_argument('--disable-gpu') \n",
    "\n",
    "# soup = get_response(HOMEPAGE_URL, HEADERS)\n",
    "\n",
    "# titles = get_card_titles()\n",
    "# urls = get_card_urls()\n",
    "\n",
    "# investopedia_df = pd.DataFrame({'url': urls, 'title': titles})\n",
    "\n",
    "# articles, raw_articles = get_url_content(investopedia_df['url'])\n",
    "\n",
    "# investopedia_df['article'] = articles\n",
    "\n",
    "# symbols = get_symbols(raw_articles, 'widgetsymbol')\n",
    "\n",
    "# investopedia_df['symbols'] = symbols\n",
    "\n",
    "# #holdings\n",
    "# get_datahub_csv(VUSA_DOWNLOAD_URL, 'vusa_holdings.csv')\n",
    "\n",
    "# #publieke dataset\n",
    "# get_datahub_csv(SP_DOWNLOAD_URL, 'time_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "28046216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# options = Options()\n",
    "# options.add_argument('--headless')\n",
    "# options.add_argument('--disable-gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "8569ca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_response(url, cookies={}, headers=HEADERS):\n",
    "#     r = httpx.get(url, cookies=cookies, headers=headers, follow_redirects=True)\n",
    "#     soup = bs(r.content)\n",
    "#     return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "c09c9c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup = get_response(HOMEPAGE_URL, HEADERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "146a16d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_card_titles():\n",
    "#     titles = []\n",
    "#     a_tags = soup.find_all('a', class_='card')\n",
    "#     for i in range(len(a_tags)):\n",
    "#         titles.append(a_tags[i].span.text)\n",
    "#     return titles\n",
    "\n",
    "# def get_card_urls():\n",
    "#     urls = []\n",
    "#     a_tags = soup.find_all('a', class_='card')\n",
    "#     for i in range(len(a_tags)):\n",
    "#         urls.append(a_tags[i]['href'])\n",
    "#     return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "8b05953a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# titles = get_card_titles()\n",
    "# urls = get_card_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "6a3b8fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# investopedia_df = pd.DataFrame({'url': urls, 'title': titles})\n",
    "# display(investopedia_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "58369553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_text(text):\n",
    "#     return re.sub(r'[^\\w\\s]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "00c0fc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_url_content(urls):\n",
    "#     articles = []\n",
    "#     raw_articles = []\n",
    "#     for url in urls:\n",
    "#         soup = get_response(url)\n",
    "#         paragraphs = soup.find('div', class_='article-content').find_all('p')\n",
    "#         article = ''.join([p.get_text(separator=' ') for p in paragraphs])\n",
    "#         raw_articles.append(paragraphs)\n",
    "#         articles.append(clean_text(article))\n",
    "        \n",
    "#     return articles, raw_articles\n",
    "\n",
    "# def get_symbols(raw_articles, href_ref):\n",
    "#     symbol_list = []\n",
    "#     for p_list in raw_articles:\n",
    "#         article_symbols = []\n",
    "#         for p in p_list:\n",
    "#             symbols = [a_tag.text for a_tag in p.find_all('a') if re.search(href_ref, a_tag['href'])]\n",
    "#             if symbols:\n",
    "#                 for symbol in symbols:\n",
    "#                     article_symbols.append(symbol)\n",
    "#         symbol_list.append(article_symbols)\n",
    "#     return symbol_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "8ac26660",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# articles, raw_articles = get_url_content(investopedia_df['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "ef5dcd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# investopedia_df['article'] = articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "17bf5e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# symbols = get_symbols(raw_articles, 'widgetsymbol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "bed7eae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# investopedia_df['symbols'] = symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "aeb93652",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# display(investopedia_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "a8d83664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_datahub_csv(URL, filename):\n",
    "#     soup = get_response(URL)\n",
    "#     download_url = soup.find_all('table')[1].find_all('a')[1]['href']\n",
    "#     download_url = DATAHUB_URL + download_url\n",
    "#     with open(DATA_LOCATION + filename, 'wb') as f:\n",
    "#         f.write(requests.get(download_url).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "41fb11cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get_datahub_csv(VUSA_DOWNLOAD_URL, 'vusa_holdings.csv')\n",
    "# get_datahub_csv(SP_DOWNLOAD_URL, 'time_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "ef3be3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vusa_df = pd.read_csv(DATA_LOCATION + 'vusa_holdings.csv')\n",
    "# symbols_in_vusa = vusa_df['Symbol'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "977dd48a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#investopedia filteren op vusa symbols\n",
    "# investopedia_df = investopedia_df[investopedia_df['symbols'].apply(lambda symbols: any(symbol in symbols_in_vusa for symbol in symbols))]\n",
    "# #combine dataframes\n",
    "# df = combine_dfs(investopedia_df, yahoo_news_df)\n",
    "\n",
    "# #clean and tokenize the articles\n",
    "# df['tokenized_article'] = clean_and_tokenize(df['article'])\n",
    "\n",
    "# #do sentiment analysis\n",
    "# analyzer = SentimentIntensityAnalyzer()\n",
    "# df['positivity_score'] = [analyzer.polarity_scores(article)['pos'] for article in df['article']]\n",
    "# df['negativity_score'] = [analyzer.polarity_scores(article)['neg'] for article in df['article']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "bd721962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_article_links(driver):\n",
    "#     links = driver.find_elements(By.TAG_NAME, 'a')\n",
    "#     return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "669ceeef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def get_yahoo_news_articles_df():\n",
    "#     yahoo_news_driver = webdriver.Chrome(options=options)\n",
    "\n",
    "#     yahoo_news_driver.get(YAHOO_URL)\n",
    "#     time.sleep(1)\n",
    "#     yahoo_news_driver.find_element(By.CLASS_NAME, 'reject-all').click()\n",
    "#     actions = ActionChains(yahoo_news_driver)\n",
    "#     for i in range(1500):\n",
    "#         actions.scroll_by_amount(0, 500)\n",
    "#     actions.perform()\n",
    "#     actions.reset_actions()\n",
    "\n",
    "#     links = get_article_links(yahoo_news_driver)\n",
    "#     df = pd.DataFrame({'title': [link.text for link in links], \n",
    "#                        'url': [link.get_attribute('href') for link in links], \n",
    "#                        'classes': [link.get_attribute('class') for link in links]})\n",
    "    \n",
    "#     yahoo_news_driver.quit()\n",
    "#     df = df[df['url'] \\\n",
    "#             .str.contains('/news/') & df['classes'].str.contains('js-content-viewer')]\\\n",
    "#             .drop_duplicates(subset='url', keep='last')\\\n",
    "#             .reset_index(drop=True)\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "082622a0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# yahoo_news_df = get_yahoo_news_articles_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "dc814d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(yahoo_news_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "0d71d904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def progress_bar(current, total, bar_length=20):\n",
    "#     fraction = current / total\n",
    "\n",
    "#     arrow = int(fraction * bar_length - 1) * '-' + '>'\n",
    "#     padding = int(bar_length - len(arrow)) * ' '\n",
    "\n",
    "#     ending = '\\n' if current == total else '\\r'\n",
    "\n",
    "#     print(f'Progress: [{arrow}{padding}] {int(fraction*100)}%', end=ending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "d1b2a369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scrape_articles(urls):\n",
    "#     articles = []\n",
    "#     raw_articles = []\n",
    "#     yahoo_driver = webdriver.Chrome(options=options)\n",
    "#     yahoo_driver.get(YAHOO_URL)\n",
    "#     yahoo_driver.find_element(By.CLASS_NAME, 'reject-all').click()\n",
    "\n",
    "#     for index, url in enumerate(urls):\n",
    "#         yahoo_driver.get(url)\n",
    "#         try:\n",
    "#             yahoo_driver.find_element(By.CLASS_NAME, 'collapse-button').click()\n",
    "#         except:\n",
    "#             pass\n",
    "#         article_p_elements = yahoo_driver.find_element(By.CLASS_NAME, 'caas-body').find_elements(By.TAG_NAME, 'p')\n",
    "#         article_by_paragraph = [p.text for p in article_p_elements if p.text!='']\n",
    "        \n",
    "#         raw_article = [bs(element.get_attribute('outerHTML'), \"html.parser\") for element in article_p_elements]\n",
    "#         article = \" \".join(article_by_paragraph)\n",
    "#         clean_article = clean_text(article)\n",
    "#         articles.append(clean_article)\n",
    "#         raw_articles.append(raw_article)\n",
    "#         progress_bar(index+1, len(urls))\n",
    "    \n",
    "        \n",
    "#     yahoo_driver.quit()\n",
    "    \n",
    "#     return articles, raw_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "fabc771b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# yahoo_news_articles, raw_yahoo_news_articles = scrape_articles(yahoo_news_df['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "d75618b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yahoo_news_df['article'] = yahoo_news_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "83c82f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yahoo_news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "0754b62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def combine_dfs(investopedia_df, yahoo_df):\n",
    "#     df1 = investopedia_df.drop(['symbols'], axis=1)\n",
    "#     df2 = yahoo_df.drop('classes', axis=1)\n",
    "#     article_df = pd.concat([df1, df2])\n",
    "#     article_df = article_df.reset_index(drop=True)\n",
    "#     return article_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "9fd17a24",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# display(investopedia_df)\n",
    "# df = combine_dfs(investopedia_df, yahoo_news_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "2f477d9a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# def clean_and_tokenize(articles):\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "#     tokenized_articles = []\n",
    "#     for article in articles:\n",
    "#         tokenized_article = nltk.word_tokenize(article)\n",
    "#         tokenized_article = [token for token in tokenized_article if token not in stopwords.words('english')]\n",
    "#         tokenized_article = [lemmatizer.lemmatize(token) for token in tokenized_article]\n",
    "        \n",
    "#         tokenized_articles.append(tokenized_article)\n",
    "        \n",
    "#     return tokenized_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "2a8e4eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['tokenized_article'] = clean_and_tokenize(df['article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "217c5875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "895b007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyzer = SentimentIntensityAnalyzer()\n",
    "# df['positivity_score'] = [analyzer.polarity_scores(article)['pos'] for article in df['article']]\n",
    "# df['negativity_score'] = [analyzer.polarity_scores(article)['neg'] for article in df['article']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "667613b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos = df.describe().T[:1].reset_index(drop=True)\n",
    "# neg = df.describe().T[1:].reset_index(drop=True)\n",
    "\n",
    "# pos.columns = ['pos_' + col for col in pos.columns]\n",
    "# neg.columns = ['neg_' + col for col in neg.columns]\n",
    "\n",
    "# pd.concat([pos, neg], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "e2cacf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df2 = pd.read_csv(DATA_LOCATION + 'time_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "cc7ed872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "e474c313",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    def __init__(self):\n",
    "        self.HEADERS = {'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "           'Accept-Encoding': 'gzip, deflate, br',\n",
    "           'Accept-Language': 'en-US,en;q=0.9,nl;q=0.8',\n",
    "           'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'}\n",
    "\n",
    "        self.DATA_LOCATION = 'Datasets/'\n",
    "\n",
    "        self.HOMEPAGE_URL = 'https://www.investopedia.com/markets-news-4427704'\n",
    "\n",
    "        self.VUSA_DOWNLOAD_URL = 'https://datahub.io/core/s-and-p-500-companies'\n",
    "        self.SP_DOWNLOAD_URL = 'https://datahub.io/core/s-and-p-500'\n",
    "        self.DATAHUB_URL = 'https://datahub.io'\n",
    "\n",
    "        self.YAHOO_URL = 'https://finance.yahoo.com/'\n",
    "\n",
    "        self.OPTIONS = Options()\n",
    "        self.OPTIONS.add_argument('--headless')\n",
    "        self.OPTIONS.add_argument('--disable-gpu') \n",
    "        \n",
    "        nltk.download(['punkt', 'wordnet', 'stopwords', 'omw-1.4', 'vader_lexicon'])\n",
    "        \n",
    "        \n",
    "    def get_response(self, url, headers, cookies={}):\n",
    "        r = httpx.get(url, cookies=cookies, headers=headers, follow_redirects=True)\n",
    "        soup = bs(r.content)\n",
    "        return soup\n",
    "    \n",
    "    def get_card_titles(self):\n",
    "        titles = []\n",
    "        a_tags = self.soup.find_all('a', class_='card')\n",
    "        for i in range(len(a_tags)):\n",
    "            titles.append(a_tags[i].span.text)\n",
    "        return titles\n",
    "\n",
    "    def get_card_urls(self):\n",
    "        urls = []\n",
    "        a_tags = self.soup.find_all('a', class_='card')\n",
    "        for i in range(len(a_tags)):\n",
    "            urls.append(a_tags[i]['href'])\n",
    "        return urls\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        return re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    def get_url_content(self, urls):\n",
    "        articles = []\n",
    "        raw_articles = []\n",
    "        for url in urls:\n",
    "            soup = self.get_response(url, self.HEADERS)\n",
    "            paragraphs = soup.find('div', class_='article-content').find_all('p')\n",
    "            article = ''.join([p.get_text(separator=' ') for p in paragraphs])\n",
    "            raw_articles.append(paragraphs)\n",
    "            articles.append(self.clean_text(article))\n",
    "\n",
    "        return articles, raw_articles\n",
    "\n",
    "    def get_symbols(self, raw_articles, href_ref):\n",
    "        symbol_list = []\n",
    "        for p_list in raw_articles:\n",
    "            article_symbols = []\n",
    "            for p in p_list:\n",
    "                symbols = [a_tag.text for a_tag in p.find_all('a') if re.search(href_ref, a_tag['href'])]\n",
    "                if symbols:\n",
    "                    for symbol in symbols:\n",
    "                        article_symbols.append(symbol)\n",
    "            symbol_list.append(article_symbols)\n",
    "        return symbol_list\n",
    "    \n",
    "    def get_datahub_csv(self, URL, filename):\n",
    "        soup = self.get_response(URL, self.HEADERS)\n",
    "        download_url = soup.find_all('table')[1].find_all('a')[1]['href']\n",
    "        download_url = self.DATAHUB_URL + download_url\n",
    "        with open(self.DATA_LOCATION + filename, 'wb') as f:\n",
    "            f.write(requests.get(download_url).content)\n",
    "            \n",
    "    def get_article_links(self, driver):\n",
    "        links = driver.find_elements(By.TAG_NAME, 'a')\n",
    "        return links\n",
    "    \n",
    "    def get_yahoo_news_articles_df(self):\n",
    "        yahoo_news_driver = webdriver.Chrome(options=self.OPTIONS)\n",
    "\n",
    "        yahoo_news_driver.get(self.YAHOO_URL)\n",
    "        time.sleep(1)\n",
    "        yahoo_news_driver.find_element(By.CLASS_NAME, 'reject-all').click()\n",
    "        actions = ActionChains(yahoo_news_driver)\n",
    "        for i in range(1500):\n",
    "            actions.scroll_by_amount(0, 500)\n",
    "        actions.perform()\n",
    "        actions.reset_actions()\n",
    "\n",
    "        links = self.get_article_links(yahoo_news_driver)\n",
    "        df = pd.DataFrame({'title': [link.text for link in links], \n",
    "                           'url': [link.get_attribute('href') for link in links], \n",
    "                           'classes': [link.get_attribute('class') for link in links]})\n",
    "\n",
    "        yahoo_news_driver.quit()\n",
    "        df = df[df['url'] \\\n",
    "                .str.contains('/news/') & df['classes'].str.contains('js-content-viewer')]\\\n",
    "                .drop_duplicates(subset='url', keep='last')\\\n",
    "                .reset_index(drop=True)\n",
    "        return df\n",
    "    \n",
    "    def progress_bar(self, current, total, bar_length=20):\n",
    "        fraction = current / total\n",
    "\n",
    "        arrow = int(fraction * bar_length - 1) * '-' + '>'\n",
    "        padding = int(bar_length - len(arrow)) * ' '\n",
    "\n",
    "        ending = '\\n' if current == total else '\\r'\n",
    "\n",
    "        print(f'Progress: [{arrow}{padding}] {int(fraction*100)}%', end=ending)\n",
    "    \n",
    "    def scrape_articles(self, urls):\n",
    "        articles = []\n",
    "        raw_articles = []\n",
    "        yahoo_driver = webdriver.Chrome(options=self.OPTIONS)\n",
    "        yahoo_driver.get(self.YAHOO_URL)\n",
    "        yahoo_driver.find_element(By.CLASS_NAME, 'reject-all').click()\n",
    "\n",
    "        for index, url in enumerate(urls):\n",
    "            yahoo_driver.get(url)\n",
    "            try:\n",
    "                yahoo_driver.find_element(By.CLASS_NAME, 'collapse-button').click()\n",
    "            except:\n",
    "                pass\n",
    "            article_p_elements = yahoo_driver.find_element(By.CLASS_NAME, 'caas-body').find_elements(By.TAG_NAME, 'p')\n",
    "            article_by_paragraph = [p.text for p in article_p_elements if p.text!='']\n",
    "\n",
    "            raw_article = [bs(element.get_attribute('outerHTML'), \"html.parser\") for element in article_p_elements]\n",
    "            article = \" \".join(article_by_paragraph)\n",
    "            clean_article = self.clean_text(article)\n",
    "            articles.append(clean_article)\n",
    "            raw_articles.append(raw_article)\n",
    "            self.progress_bar(index+1, len(urls))\n",
    "\n",
    "\n",
    "        yahoo_driver.quit()\n",
    "\n",
    "        return articles, raw_articles\n",
    "\n",
    "    def clean_and_tokenize(self, articles):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        tokenized_articles = []\n",
    "        for article in articles:\n",
    "            tokenized_article = nltk.word_tokenize(article)\n",
    "            tokenized_article = [token for token in tokenized_article if token not in stopwords.words('english')]\n",
    "            tokenized_article = [lemmatizer.lemmatize(token) for token in tokenized_article]\n",
    "\n",
    "            tokenized_articles.append(tokenized_article)\n",
    "\n",
    "        return tokenized_articles\n",
    "    \n",
    "    def download_yahoo_data(self, file_path):\n",
    "        \"\"\"\n",
    "        Downloadt gegevens van Yahoo Finance voor tickers die zijn vermeld in het opgegeven CSV-bestand.\n",
    "\n",
    "        Parameters:\n",
    "        - file_path (str): Bestandspad naar het CSV-bestand met ticker-symbolen.\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame: Gefilterd DataFrame met gedownloade gegevens.\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(file_path)\n",
    "        symbol_list = df.iloc[:, 0].tolist()\n",
    "\n",
    "        download = yf.download(symbol_list, group_by=\"ticker\")\n",
    "        data = download.copy()\n",
    "\n",
    "        filtered_data = data.dropna(axis=1, how='all')\n",
    "        filtered_data.columns = filtered_data.columns.remove_unused_levels()\n",
    "\n",
    "        remaining_tickers = list(filtered_data.columns.levels[0])\n",
    "        missing_tickers = list(set(symbol_list) - set(remaining_tickers))\n",
    "\n",
    "#         print(\"Ontbrekende tickers en het aantal:\", missing_tickers, len(missing_tickers))\n",
    "#         display(filtered_data, filtered_data.shape)\n",
    "\n",
    "        return filtered_data    \n",
    "    \n",
    "    def combine_data(self, api_df, investopedia_df, yahoo_df, time_data):\n",
    "        df1 = investopedia_df.drop(['symbols'], axis=1)\n",
    "        df2 = yahoo_df.drop('classes', axis=1)\n",
    "        text_df = pd.concat([df1, df2])\n",
    "        text_df = text_df.reset_index(drop=True)\n",
    "        \n",
    "        # tokenize the articles and add them to a DataFrame\n",
    "        text_df['tokenized_article'] = self.clean_and_tokenize(text_df['article'])\n",
    "        \n",
    "        # apply sentiment analysis and add it to the DataFrame\n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "        text_df['positivity_score'] = [analyzer.polarity_scores(article)['pos'] for article in text_df['article']]\n",
    "        text_df['negativity_score'] = [analyzer.polarity_scores(article)['neg'] for article in text_df['article']]\n",
    "        \n",
    "        # get statistics from positive and negative sentiment analysis\n",
    "        pos = text_df.describe().T[:1].reset_index(drop=True)\n",
    "        neg = text_df.describe().T[1:].reset_index(drop=True)\n",
    "        \n",
    "        # rename columns in pos and neg so they can be concattinated\n",
    "        pos.columns = ['pos_' + col for col in pos.columns]\n",
    "        neg.columns = ['neg_' + col for col in neg.columns]\n",
    "        \n",
    "        # concattinate the positve and negative sentiment analysis statistics to a 1 row DataFrame\n",
    "        pos_neg_row = pd.concat([pos, neg], axis=1)\n",
    "        \n",
    "        api_df = api_df.iloc[::-1].reset_index(drop=False, names='Date')\n",
    "        \n",
    "        api_df.columns = ['_'.join(col).strip() for col in api_df.columns.values]\n",
    "        \n",
    "        combined_df = pd.concat([api_df, pos_neg_row], axis=1)\n",
    "        \n",
    "        combined_df['Date_'] = combined_df['Date_'].astype(str)\n",
    "        time_data['Date'] = time_data['Date'].astype(str)\n",
    "        \n",
    "        display(self.api_df.columns)\n",
    "        display(combined_df, time_data)\n",
    "\n",
    "        final_df = combined_df.merge(time_data, how='left', left_on='Date_', right_on='Date')\n",
    "        \n",
    "        return final_df\n",
    "\n",
    "    def extract(self):\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "        # get site data\n",
    "        self.soup = self.get_response(self.HOMEPAGE_URL, self.HEADERS)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------   \n",
    "        # download csv data\n",
    "        # get_datahub_csv(self.VUSA_DOWNLOAD_URL, 'vusa_holdings.csv')\n",
    "        # get_datahub_csv(self.SP_DOWNLOAD_URL, 'time_data.csv')\n",
    "       \n",
    "        # get csv data\n",
    "        self.vusa_df = pd.read_csv(self.DATA_LOCATION + 'vusa_holdings.csv')\n",
    "        self.time_data_df = pd.read_csv(self.DATA_LOCATION + 'time_data.csv')\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "        # get yahoo articles with titles, urls and css classes\n",
    "        self.yahoo_news_df = self.get_yahoo_news_articles_df()\n",
    "    \n",
    "        # scrape news articles from yahoo article urls\n",
    "        self.yahoo_news_articles, self.raw_yahoo_news_articles = self.scrape_articles(self.yahoo_news_df['url'])\n",
    "        \n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "        # get api data\n",
    "        self.api_df = self.download_yahoo_data('Datasets/vusa_holdings.csv')\n",
    "    \n",
    "    \n",
    "#-------------------------------------------------------------------------------------------------------  \n",
    "    def transform(self):        \n",
    "        \n",
    "#-------------------------------------------------------------------------------------------------------          \n",
    "        # filter site data\n",
    "        titles = self.get_card_titles()\n",
    "        urls = self.get_card_urls()\n",
    "        \n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "        # filter symbols from the vusa_df csv data\n",
    "        symbols_in_vusa = self.vusa_df['Symbol'].tolist()\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "        # add scraped articles to DataFrame\n",
    "        self.yahoo_news_df['article'] = self.yahoo_news_articles\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "        # transform filtered data to DataFrame(investopedia_df)\n",
    "        investopedia_df = pd.DataFrame({'url': urls, 'title': titles})\n",
    "        \n",
    "        # get articles from urls\n",
    "        articles, raw_articles = self.get_url_content(investopedia_df['url'])\n",
    "        \n",
    "        # get symbols from article\n",
    "        symbols = self.get_symbols(raw_articles, 'widgetsymbol')\n",
    "        \n",
    "        # add articles and symbols to DataFrame(investopedia_df)\n",
    "        investopedia_df['article'] = articles\n",
    "        investopedia_df['symbols'] = symbols\n",
    "        \n",
    "        # filter DataFrame(investopedia_df) to only have symbols that occur in the symbols_in_vusa\n",
    "        investopedia_df = investopedia_df[investopedia_df['symbols'].apply(lambda symbols: any(symbol in symbols_in_vusa for symbol in symbols))]\n",
    "        \n",
    "#-------------------------------------------------------------------------------------------------------    \n",
    "        # combine DataFrames\n",
    "        self.df = self.combine_data(self.api_df, investopedia_df, self.yahoo_news_df, self.time_data_df)\n",
    "        \n",
    "#-------------------------------------------------------------------------------------------------------   \n",
    "    def load(self, filename):\n",
    "        self.df.to_csv(filename)\n",
    "    \n",
    "    def ETL(self, filename):\n",
    "        self.extract()\n",
    "        self.transform()\n",
    "        self.load(filename)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "e3225b65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mark\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Mark\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mark\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Mark\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Mark\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16129433",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [------------------->] 100%\n",
      "[*********************100%%**********************]  505 of 505 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "29 Failed downloads:\n",
      "['FBHS', 'XLNX', 'ANTM', 'DISCK', 'ATVI', 'BRK.B', 'NLOK', 'RE', 'DISCA', 'KSU', 'ABC', 'WLTW', 'SIVB', 'NLSN', 'CERN', 'FISV', 'BLL', 'FB', 'DRE', 'PKI', 'VIAC', 'CTXS', 'PBCT', 'TWTR', 'INFO', 'FRC']: Exception('%ticker%: No timezone found, symbol may be delisted')\n",
      "['BF.B', 'AVY', 'PEG']: Exception('%ticker%: No price data found, symbol may be delisted (1d 1925-02-16 -> 2024-01-23)')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultiIndex([('HPE',      'Open'),\n",
       "            ('HPE',      'High'),\n",
       "            ('HPE',       'Low'),\n",
       "            ('HPE',     'Close'),\n",
       "            ('HPE', 'Adj Close'),\n",
       "            ('HPE',    'Volume'),\n",
       "            ('NRG',      'Open'),\n",
       "            ('NRG',      'High'),\n",
       "            ('NRG',       'Low'),\n",
       "            ('NRG',     'Close'),\n",
       "            ...\n",
       "            ( 'WY',       'Low'),\n",
       "            ( 'WY',     'Close'),\n",
       "            ( 'WY', 'Adj Close'),\n",
       "            ( 'WY',    'Volume'),\n",
       "            ('JPM',      'Open'),\n",
       "            ('JPM',      'High'),\n",
       "            ('JPM',       'Low'),\n",
       "            ('JPM',     'Close'),\n",
       "            ('JPM', 'Adj Close'),\n",
       "            ('JPM',    'Volume')],\n",
       "           length=2856)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date_</th>\n",
       "      <th>HPE_Open</th>\n",
       "      <th>HPE_High</th>\n",
       "      <th>HPE_Low</th>\n",
       "      <th>HPE_Close</th>\n",
       "      <th>HPE_Adj Close</th>\n",
       "      <th>HPE_Volume</th>\n",
       "      <th>NRG_Open</th>\n",
       "      <th>NRG_High</th>\n",
       "      <th>NRG_Low</th>\n",
       "      <th>...</th>\n",
       "      <th>pos_75%</th>\n",
       "      <th>pos_max</th>\n",
       "      <th>neg_count</th>\n",
       "      <th>neg_mean</th>\n",
       "      <th>neg_std</th>\n",
       "      <th>neg_min</th>\n",
       "      <th>neg_25%</th>\n",
       "      <th>neg_50%</th>\n",
       "      <th>neg_75%</th>\n",
       "      <th>neg_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-23</td>\n",
       "      <td>15.63</td>\n",
       "      <td>15.700</td>\n",
       "      <td>15.460</td>\n",
       "      <td>15.50</td>\n",
       "      <td>15.50</td>\n",
       "      <td>8469332.0</td>\n",
       "      <td>52.700001</td>\n",
       "      <td>52.799999</td>\n",
       "      <td>52.285000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.216</td>\n",
       "      <td>241.0</td>\n",
       "      <td>0.050456</td>\n",
       "      <td>0.029141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-22</td>\n",
       "      <td>15.48</td>\n",
       "      <td>15.620</td>\n",
       "      <td>15.400</td>\n",
       "      <td>15.50</td>\n",
       "      <td>15.50</td>\n",
       "      <td>9414400.0</td>\n",
       "      <td>52.709999</td>\n",
       "      <td>53.080002</td>\n",
       "      <td>52.200001</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-19</td>\n",
       "      <td>15.03</td>\n",
       "      <td>15.425</td>\n",
       "      <td>14.925</td>\n",
       "      <td>15.37</td>\n",
       "      <td>15.37</td>\n",
       "      <td>13251800.0</td>\n",
       "      <td>51.090000</td>\n",
       "      <td>52.540001</td>\n",
       "      <td>50.939999</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-18</td>\n",
       "      <td>15.06</td>\n",
       "      <td>15.115</td>\n",
       "      <td>14.700</td>\n",
       "      <td>14.96</td>\n",
       "      <td>14.96</td>\n",
       "      <td>15106200.0</td>\n",
       "      <td>51.049999</td>\n",
       "      <td>51.189999</td>\n",
       "      <td>50.430000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-17</td>\n",
       "      <td>15.15</td>\n",
       "      <td>15.198</td>\n",
       "      <td>14.910</td>\n",
       "      <td>15.04</td>\n",
       "      <td>15.04</td>\n",
       "      <td>13466800.0</td>\n",
       "      <td>50.650002</td>\n",
       "      <td>52.029999</td>\n",
       "      <td>50.509998</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15616</th>\n",
       "      <td>1962-01-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15617</th>\n",
       "      <td>1962-01-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15618</th>\n",
       "      <td>1962-01-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15619</th>\n",
       "      <td>1962-01-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15620</th>\n",
       "      <td>1962-01-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15621 rows Ã— 2873 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date_  HPE_Open  HPE_High  HPE_Low  HPE_Close  HPE_Adj Close  \\\n",
       "0      2024-01-23     15.63    15.700   15.460      15.50          15.50   \n",
       "1      2024-01-22     15.48    15.620   15.400      15.50          15.50   \n",
       "2      2024-01-19     15.03    15.425   14.925      15.37          15.37   \n",
       "3      2024-01-18     15.06    15.115   14.700      14.96          14.96   \n",
       "4      2024-01-17     15.15    15.198   14.910      15.04          15.04   \n",
       "...           ...       ...       ...      ...        ...            ...   \n",
       "15616  1962-01-08       NaN       NaN      NaN        NaN            NaN   \n",
       "15617  1962-01-05       NaN       NaN      NaN        NaN            NaN   \n",
       "15618  1962-01-04       NaN       NaN      NaN        NaN            NaN   \n",
       "15619  1962-01-03       NaN       NaN      NaN        NaN            NaN   \n",
       "15620  1962-01-02       NaN       NaN      NaN        NaN            NaN   \n",
       "\n",
       "       HPE_Volume   NRG_Open   NRG_High    NRG_Low  ...  pos_75%  pos_max  \\\n",
       "0       8469332.0  52.700001  52.799999  52.285000  ...    0.139    0.216   \n",
       "1       9414400.0  52.709999  53.080002  52.200001  ...      NaN      NaN   \n",
       "2      13251800.0  51.090000  52.540001  50.939999  ...      NaN      NaN   \n",
       "3      15106200.0  51.049999  51.189999  50.430000  ...      NaN      NaN   \n",
       "4      13466800.0  50.650002  52.029999  50.509998  ...      NaN      NaN   \n",
       "...           ...        ...        ...        ...  ...      ...      ...   \n",
       "15616         NaN        NaN        NaN        NaN  ...      NaN      NaN   \n",
       "15617         NaN        NaN        NaN        NaN  ...      NaN      NaN   \n",
       "15618         NaN        NaN        NaN        NaN  ...      NaN      NaN   \n",
       "15619         NaN        NaN        NaN        NaN  ...      NaN      NaN   \n",
       "15620         NaN        NaN        NaN        NaN  ...      NaN      NaN   \n",
       "\n",
       "       neg_count  neg_mean   neg_std  neg_min  neg_25%  neg_50%  neg_75%  \\\n",
       "0          241.0  0.050456  0.029141      0.0    0.029    0.048    0.067   \n",
       "1            NaN       NaN       NaN      NaN      NaN      NaN      NaN   \n",
       "2            NaN       NaN       NaN      NaN      NaN      NaN      NaN   \n",
       "3            NaN       NaN       NaN      NaN      NaN      NaN      NaN   \n",
       "4            NaN       NaN       NaN      NaN      NaN      NaN      NaN   \n",
       "...          ...       ...       ...      ...      ...      ...      ...   \n",
       "15616        NaN       NaN       NaN      NaN      NaN      NaN      NaN   \n",
       "15617        NaN       NaN       NaN      NaN      NaN      NaN      NaN   \n",
       "15618        NaN       NaN       NaN      NaN      NaN      NaN      NaN   \n",
       "15619        NaN       NaN       NaN      NaN      NaN      NaN      NaN   \n",
       "15620        NaN       NaN       NaN      NaN      NaN      NaN      NaN   \n",
       "\n",
       "       neg_max  \n",
       "0        0.172  \n",
       "1          NaN  \n",
       "2          NaN  \n",
       "3          NaN  \n",
       "4          NaN  \n",
       "...        ...  \n",
       "15616      NaN  \n",
       "15617      NaN  \n",
       "15618      NaN  \n",
       "15619      NaN  \n",
       "15620      NaN  \n",
       "\n",
       "[15621 rows x 2873 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>SP500</th>\n",
       "      <th>Dividend</th>\n",
       "      <th>Earnings</th>\n",
       "      <th>Consumer Price Index</th>\n",
       "      <th>Long Interest Rate</th>\n",
       "      <th>Real Price</th>\n",
       "      <th>Real Dividend</th>\n",
       "      <th>Real Earnings</th>\n",
       "      <th>PE10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1871-01-01</td>\n",
       "      <td>4.44</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.40</td>\n",
       "      <td>12.46</td>\n",
       "      <td>5.32</td>\n",
       "      <td>89.00</td>\n",
       "      <td>5.21</td>\n",
       "      <td>8.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1871-02-01</td>\n",
       "      <td>4.50</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.40</td>\n",
       "      <td>12.84</td>\n",
       "      <td>5.32</td>\n",
       "      <td>87.53</td>\n",
       "      <td>5.06</td>\n",
       "      <td>7.78</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1871-03-01</td>\n",
       "      <td>4.61</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.40</td>\n",
       "      <td>13.03</td>\n",
       "      <td>5.33</td>\n",
       "      <td>88.36</td>\n",
       "      <td>4.98</td>\n",
       "      <td>7.67</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1871-04-01</td>\n",
       "      <td>4.74</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.40</td>\n",
       "      <td>12.56</td>\n",
       "      <td>5.33</td>\n",
       "      <td>94.29</td>\n",
       "      <td>5.17</td>\n",
       "      <td>7.96</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1871-05-01</td>\n",
       "      <td>4.86</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.40</td>\n",
       "      <td>12.27</td>\n",
       "      <td>5.33</td>\n",
       "      <td>98.93</td>\n",
       "      <td>5.29</td>\n",
       "      <td>8.14</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1763</th>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>2664.34</td>\n",
       "      <td>48.93</td>\n",
       "      <td>109.88</td>\n",
       "      <td>246.52</td>\n",
       "      <td>2.40</td>\n",
       "      <td>2700.13</td>\n",
       "      <td>49.59</td>\n",
       "      <td>111.36</td>\n",
       "      <td>32.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1764</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>2789.80</td>\n",
       "      <td>49.29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.87</td>\n",
       "      <td>2.58</td>\n",
       "      <td>2811.96</td>\n",
       "      <td>49.68</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1765</th>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2705.16</td>\n",
       "      <td>49.64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>248.99</td>\n",
       "      <td>2.86</td>\n",
       "      <td>2714.34</td>\n",
       "      <td>49.81</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766</th>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>2702.77</td>\n",
       "      <td>50.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>249.55</td>\n",
       "      <td>2.84</td>\n",
       "      <td>2705.82</td>\n",
       "      <td>50.06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1767</th>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>2642.19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>249.84</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2642.19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1768 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date    SP500  Dividend  Earnings  Consumer Price Index  \\\n",
       "0     1871-01-01     4.44      0.26      0.40                 12.46   \n",
       "1     1871-02-01     4.50      0.26      0.40                 12.84   \n",
       "2     1871-03-01     4.61      0.26      0.40                 13.03   \n",
       "3     1871-04-01     4.74      0.26      0.40                 12.56   \n",
       "4     1871-05-01     4.86      0.26      0.40                 12.27   \n",
       "...          ...      ...       ...       ...                   ...   \n",
       "1763  2017-12-01  2664.34     48.93    109.88                246.52   \n",
       "1764  2018-01-01  2789.80     49.29       NaN                247.87   \n",
       "1765  2018-02-01  2705.16     49.64       NaN                248.99   \n",
       "1766  2018-03-01  2702.77     50.00       NaN                249.55   \n",
       "1767  2018-04-01  2642.19       NaN       NaN                249.84   \n",
       "\n",
       "      Long Interest Rate  Real Price  Real Dividend  Real Earnings   PE10  \n",
       "0                   5.32       89.00           5.21           8.02    NaN  \n",
       "1                   5.32       87.53           5.06           7.78    NaN  \n",
       "2                   5.33       88.36           4.98           7.67    NaN  \n",
       "3                   5.33       94.29           5.17           7.96    NaN  \n",
       "4                   5.33       98.93           5.29           8.14    NaN  \n",
       "...                  ...         ...            ...            ...    ...  \n",
       "1763                2.40     2700.13          49.59         111.36  32.09  \n",
       "1764                2.58     2811.96          49.68            NaN  33.31  \n",
       "1765                2.86     2714.34          49.81            NaN  32.12  \n",
       "1766                2.84     2705.82          50.06            NaN  31.99  \n",
       "1767                2.80     2642.19            NaN            NaN  31.19  \n",
       "\n",
       "[1768 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline.extract()\n",
    "pipeline.transform()\n",
    "pipeline.load('Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad26226c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a131e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
