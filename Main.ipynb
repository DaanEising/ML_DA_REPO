{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "a96dba24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "import httpx\n",
    "import re\n",
    "import requests\n",
    "import codecs\n",
    "import time\n",
    "import nltk\n",
    "\n",
    "import threading\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "import yfinance as yf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838c05a4",
   "metadata": {},
   "source": [
    "# Probleemomschrijving\n",
    "Het doel is om een pipeline te creeën die data ophaalt, transformeert en laad om vervolgens gebruikt te kunnen worden om de prijs van de ETF VUSA te voorspellen. Hierbij gaan wij artikelen van investopedia en yahoo finance scrapen en hier sentiment analysis op toepassen. Ook halen wij de historische prijsdata op van de verschillende aandelen die zich in de ETF bevinden. En als laatst halen wij de historische prijsdata op van VUSA zelf op en voegen wij alles samen tot 1 DataFrame. Deze pipeline kan elke dag gerund worden om zo nieuwe artikelen op te halen en deze toe te voegen aan de dataset, zo krijg je een steeds beter bruikbare dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e43d80",
   "metadata": {},
   "source": [
    "# Domeinonderzoek\n",
    "Vusa (IE00B3XXRP09) is een ETF die beheert wordt door Vanguard group (Fondsbeheerder|NOT,Disclosed|Vanguard S&P 500 UCITS ETF (EUR)|ISIN:IE00B3XXRP09, n.d.). Een ETF is een aandeel wat eigenlijk bestaat uit meerdere aandelen, obligaties of andere effecten die op de markt verhandelt worden (Wat Zijn ETF’s | Educatie | BlackRock, n.d.). VUSA specifiek volgt de marktindex van de SP500, dat zijn aandelen van de 500 grootste bedrijven van de Verenigde Staten. \n",
    "\n",
    "De SP500 geeft een goed beeld van de amerikaanse markt, aangezien het veel verschillende sectoren en aandelen van grote bedrijven binnen deze sectoren bevat. SP500 heeft daarom een unieke ligging in de aandelenmarkt, omdat het zo groot kunnen er duidelijke veranderingen in de amerikaanse markt worden gemeten naar aanleiding van de SP500. Omdat de SP500 zo'n goed beeld geeft van de amerikaanse markt worden veel beleggingen hiermee vergeleken om te kijken hoe goed het gaat. (S&P 500 | Wat Is De S&P 500? | Beleggingswiki - Semmie.nl, n.d.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1c4575",
   "metadata": {},
   "source": [
    "# Vereiste informatie, databronnen en formaten ruwe data\n",
    "Wij willen willen artikelen scrapen om daar sentiment analysis op toe te passen. Wij zijn van mening dat dit belangrijke data is en deze bron kan dat bevestigen: (SWOCC, 2020). De rauwe data komt in de vorm van text voor yahoo finance en een beautifulsoup object voor investopedia.\n",
    "- https://www.investopedia.com/markets-news-4427704\n",
    "- https://finance.yahoo.com/\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Ook willen wij gebruik maken van de prijsgeschiedenis van de aandelen die te vinden zijn in de VUSA ETF, de API geeft deze data als dataframe terug. Wij hebben de API hier gevonden:\n",
    "- https://github.com/ranaroussi/yfinance\n",
    "\n",
    "Dit hebben wij vervolgens via pip geinstalleerd en geimporteerd via deze code: \"import yfinance as yf\" die aan het begin van het bestand wordt gebruikt.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Als laatst willen wij de prijsgeschiedenis gebruiken van de VUSA zelf, hier hebben wij een publieke dataset voor gedownload van:\n",
    "- https://datahub.io/core/s-and-p-500\n",
    "\n",
    "Echter is de website veranderd en is deze dataset niet meer beschikbaar in deze vorm. Hieronder is een link die laat zien hoe de website er voorheen uitzag:\n",
    "\n",
    "- https://web.archive.org/web/20230402111356/https://datahub.io/core/s-and-p-500\n",
    "\n",
    "De data kwam in een csv bestand en aangezien wij dit destijds hadden gedownload gebruiken wij nu het gedownloade bestand in de pipeline.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Om de juiste data uit de yfinance API te halen en de artikelen van investopedia te filteren hebben wij gebruik gemaakt van een csv bestand wat wij ook gedownload hebben van datahub, aangezien de gehele website is veranderd is ook deze URL niet meer bruikbaar en gebruiken wij de voorheen gedownloade CSV. De originele URL is:\n",
    "\n",
    "- https://datahub.io/core/s-and-p-500-companies\n",
    "\n",
    "En om te zien hoe de website eruit zag kan je deze link volgen:\n",
    "- https://web.archive.org/web/20230601125536/https://datahub.io/core/s-and-p-500-companies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c72c2f",
   "metadata": {},
   "source": [
    "# Het proces\n",
    "In deze cell zullen wij uitleggen en beschrijven wat er allemaal in de class Pipeline gebeurt. We zullen de methodes uitleggen en hoe wij de de verschillende nodige technieken hebben toegepast.\n",
    "\n",
    "## ETL\n",
    "Wij maken in dit project gebruik van ETL (Extract, transform, load). Hiervoor hebben wij ook een ETL methode aangemaakt onderaan de class, hier worden de Extract, transform en load methodes aangeroepen die vervolgens de stappen uivoeren die nodig zijn.\n",
    "\n",
    "## Extract\n",
    "In de Extract methode halen wij alle ruwe data uit de beschikbare bronnen. \n",
    "\n",
    "Het begint met het aanroepen van de get_response methode met de URL voor investopedia (https://www.investopedia.com/markets-news-4427704), get_response doet hier een get request en zet de html om in een beautifulsoup object.\n",
    "\n",
    "vervolgens werden de csv bestanden gedownload van datahub.io maar aangezien dit niet meer beschikbaar is, is dit uigecomment. En in plaats hiervan worden de gedownloade bestanden ingelezen als dataframes en in de variabelen: self.vusa_df en self.time_data_df gezet.\n",
    "\n",
    "Dan wordt de yahoo finance website gescraped doormiddel van de get_yahoo_news_articles_df methode, deze methode weigert de niet-essentiele cookies en scrollt dan door de yahoo_finance website(https://finance.yahoo.com/) om zo alle URLs op te halen van de artikelen die op de website staan. Dit doen we doormiddel van een headless selenium browser, aangezien de website dynamisch geladen word en zonder scrollen niet alle URLs opgehaald kunnen worden. Deze URLs worden in een DataFrame gezet met de titels van de artikelen en css klassen van de artikelen op de website. We halen de artikelen en css klassen ook op om duidelijk te kunnen zien of alles goed is gegaan en problemen makkelijk op te lossen als het niet zou gaan zoals wij verwachten.\n",
    "\n",
    "Dan hebben we alle URLs van de artikelen maar de artikelen zelf hebben we nog niet, dus dat is het volgende wat wij doen. \n",
    "Dit doen we met de methode scrape_articles waar wij de URLs als parameter meegeven. Deze methode klikt eerst op het weigeren van niet-essentiele cookies en laadt vervolgens 1 voor 1 de verschillende URLs. Sommige van de artikelen zijn groter en yahoo finance laadt dan niet het gehele artikel zien, hiervoor moet eerst op een knop gedrukt worden met de class \n",
    "'collapse-button'. Wij maken gebruik van een try, except om deze knop in te drukken wat zorgt voor robuustheid zodat alle artikelen goed geladen kunnen worden. vervolgens worden de artikelen opgehaald en schoongemaakt doormiddel van de regex functie die in de methode clean_text gebruikt wordt. scrape_articles geeft ook de rauwe html van de artikelen terug om zo makkelijk te kunnen zien of alles volgens verwachting is verlopen en fouten makkelijk op te lossen.\n",
    "\n",
    "Als laatst word yfinance API gebruikt om de prijsgeschiedenis te downloaden, dit wordt gedaan in de download_yahoo_data methode. In deze methode wordt ook de locatie van het bestand met de tickers van de bedrijven in de SP500 meegegeven zodat de methode dit kan gebruiken om te weten welke prijsdata gedownload moet worden. In dit bestand staan de tickers van de bedrijven in de SP500 en dit wordt omgezet naar een lijst en gebruikt voor de finance API om te weten wat te downloaden. Dit wordt gedownload en automatisch in een DataFrame gezet. Ook geeft yfinance automatisch aan welke downloads niet gelukt zijn, dit is goed voor de robuustheid aangezien we dan makkelijk kunnen zien waar het wellicht misgaat. Wij hebben een aantal downloads die sowieso mislukken, dit zijn downloads van bedrijven die niet meer verhandelt worden. Een goed voorbeeld hiervan is facebook, facebook wordt niet meer verhandelt aangezien het bedrijf is veranderd naar meta.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd6a84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def download_yahoo_data(self, file_path):\n",
    "        \"\"\"\n",
    "        Downloadt gegevens van Yahoo Finance voor tickers die zijn vermeld in het opgegeven CSV-bestand.\n",
    "\n",
    "        Parameters:\n",
    "        - file_path (str): Bestandspad naar het CSV-bestand met ticker-symbolen.\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame: Gefilterd DataFrame met gedownloade gegevens.\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(file_path)\n",
    "        symbol_list = df.iloc[:, 0].tolist()\n",
    "\n",
    "        download = yf.download(symbol_list, group_by=\"ticker\")\n",
    "        data = download.copy()\n",
    "\n",
    "        filtered_data = data.dropna(axis=1, how='all')\n",
    "        filtered_data.columns = filtered_data.columns.remove_unused_levels()\n",
    "\n",
    "        remaining_tickers = list(filtered_data.columns.levels[0])\n",
    "        missing_tickers = list(set(symbol_list) - set(remaining_tickers))\n",
    "        \n",
    "        return filtered_data\n",
    "    \n",
    "    def extract(self):\n",
    "        # get site data\n",
    "        self.soup = self.get_response(self.HOMEPAGE_URL, self.HEADERS)\n",
    "\n",
    "        # download csv data\n",
    "        # get_datahub_csv(self.VUSA_DOWNLOAD_URL, 'vusa_holdings.csv')\n",
    "        # get_datahub_csv(self.SP_DOWNLOAD_URL, 'time_data.csv')\n",
    "       \n",
    "        # get csv data\n",
    "        self.vusa_df = pd.read_csv(self.DATA_LOCATION + 'vusa_holdings.csv')\n",
    "        self.time_data_df = pd.read_csv(self.DATA_LOCATION + 'time_data.csv')\n",
    "\n",
    "        # get yahoo articles with titles, urls and css classes\n",
    "        self.yahoo_news_df = self.get_yahoo_news_articles_df()\n",
    "    \n",
    "        # scrape news articles from yahoo article urls\n",
    "        self.yahoo_news_articles, self.raw_yahoo_news_articles = self.scrape_articles(self.yahoo_news_df['url'])\n",
    "        \n",
    "        # get api data\n",
    "        self.api_df = self.download_yahoo_data('Datasets/vusa_holdings.csv')\n",
    "    \n",
    "    \n",
    "    def transform(self):        \n",
    "        # filter site data\n",
    "        titles = self.get_card_titles()\n",
    "        urls = self.get_card_urls()\n",
    "        \n",
    "        # filter symbols from the vusa_df csv data\n",
    "        symbols_in_vusa = self.vusa_df['Symbol'].tolist()\n",
    "\n",
    "        # add scraped articles to DataFrame\n",
    "        self.yahoo_news_df['article'] = self.yahoo_news_articles\n",
    "\n",
    "        # transform filtered data to DataFrame(investopedia_df)\n",
    "        investopedia_df = pd.DataFrame({'url': urls, 'title': titles})\n",
    "        \n",
    "        # get articles from urls\n",
    "        articles, raw_articles = self.get_url_content(investopedia_df['url'])\n",
    "        \n",
    "        # get symbols from article\n",
    "        symbols = self.get_symbols(raw_articles, 'widgetsymbol')\n",
    "        \n",
    "        # add articles and symbols to DataFrame(investopedia_df)\n",
    "        investopedia_df['article'] = articles\n",
    "        investopedia_df['symbols'] = symbols\n",
    "        \n",
    "        # filter DataFrame(investopedia_df) to only have symbols that occur in the symbols_in_vusa\n",
    "        investopedia_df = investopedia_df[investopedia_df['symbols'].apply(lambda symbols: any(symbol in symbols_in_vusa for symbol in symbols))]\n",
    "        \n",
    "        # combine DataFrames\n",
    "        self.df = self.combine_data(self.api_df, investopedia_df, self.yahoo_news_df, self.time_data_df)\n",
    "          \n",
    "    def load(self, filename):\n",
    "        self.df.to_csv(filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "e474c313",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    \"\"\"\n",
    "    Pipeline to get data relating to the stock vusa that tracks the SP500. Includes sentiment analysis from articles,\n",
    "    price history of stocks that are in the ETF and price history of vusa itself. \n",
    "    The class follows the Extract, Transform, Load functionality.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.HEADERS = {'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "           'Accept-Encoding': 'gzip, deflate, br',\n",
    "           'Accept-Language': 'en-US,en;q=0.9,nl;q=0.8',\n",
    "           'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'}\n",
    "\n",
    "        self.DATA_LOCATION = 'Datasets/'\n",
    "\n",
    "        self.HOMEPAGE_URL = 'https://www.investopedia.com/markets-news-4427704'\n",
    "\n",
    "        self.VUSA_DOWNLOAD_URL = 'https://datahub.io/core/s-and-p-500-companies'\n",
    "        self.SP_DOWNLOAD_URL = 'https://datahub.io/core/s-and-p-500'\n",
    "        self.DATAHUB_URL = 'https://datahub.io'\n",
    "\n",
    "        self.YAHOO_URL = 'https://finance.yahoo.com/'\n",
    "\n",
    "        self.OPTIONS = Options()\n",
    "        self.OPTIONS.add_argument('--headless')\n",
    "        self.OPTIONS.add_argument('--disable-gpu') \n",
    "        \n",
    "        nltk.download(['punkt', 'wordnet', 'stopwords', 'omw-1.4', 'vader_lexicon'])\n",
    "        \n",
    "        \n",
    "    def get_response(self, url, headers, cookies={}):\n",
    "        r = httpx.get(url, cookies=cookies, headers=headers, follow_redirects=True)\n",
    "        soup = bs(r.content)\n",
    "        return soup\n",
    "    \n",
    "     def get_datahub_csv(self, URL, filename):\n",
    "        soup = self.get_response(URL, self.HEADERS)\n",
    "        download_url = soup.find_all('table')[1].find_all('a')[1]['href']\n",
    "        download_url = self.DATAHUB_URL + download_url\n",
    "        with open(self.DATA_LOCATION + filename, 'wb') as f:\n",
    "            f.write(requests.get(download_url).content)\n",
    "    \n",
    "    def get_yahoo_news_articles_df(self):\n",
    "        yahoo_news_driver = webdriver.Chrome(options=self.OPTIONS)\n",
    "\n",
    "        yahoo_news_driver.get(self.YAHOO_URL)\n",
    "        time.sleep(1)\n",
    "        yahoo_news_driver.find_element(By.CLASS_NAME, 'reject-all').click()\n",
    "        actions = ActionChains(yahoo_news_driver)\n",
    "        for i in range(1500):\n",
    "            actions.scroll_by_amount(0, 500)\n",
    "        actions.perform()\n",
    "        actions.reset_actions()\n",
    "\n",
    "        links = self.get_article_links(yahoo_news_driver)\n",
    "        df = pd.DataFrame({'title': [link.text for link in links], \n",
    "                           'url': [link.get_attribute('href') for link in links], \n",
    "                           'classes': [link.get_attribute('class') for link in links]})\n",
    "\n",
    "        yahoo_news_driver.quit()\n",
    "        df = df[df['url'] \\\n",
    "                .str.contains('/news/') & df['classes'].str.contains('js-content-viewer')]\\\n",
    "                .drop_duplicates(subset='url', keep='last')\\\n",
    "                .reset_index(drop=True)\n",
    "        return df\n",
    "    \n",
    "    def scrape_articles(self, urls):\n",
    "        articles = []\n",
    "        raw_articles = []\n",
    "        yahoo_driver = webdriver.Chrome(options=self.OPTIONS)\n",
    "        yahoo_driver.get(self.YAHOO_URL)\n",
    "        yahoo_driver.find_element(By.CLASS_NAME, 'reject-all').click()\n",
    "\n",
    "        for index, url in enumerate(urls):\n",
    "            yahoo_driver.get(url)\n",
    "            try:\n",
    "                yahoo_driver.find_element(By.CLASS_NAME, 'collapse-button').click()\n",
    "            except:\n",
    "                pass\n",
    "            article_p_elements = yahoo_driver.find_element(By.CLASS_NAME, 'caas-body').find_elements(By.TAG_NAME, 'p')\n",
    "            article_by_paragraph = [p.text for p in article_p_elements if p.text!='']\n",
    "\n",
    "            raw_article = [bs(element.get_attribute('outerHTML'), \"html.parser\") for element in article_p_elements]\n",
    "            article = \" \".join(article_by_paragraph)\n",
    "            clean_article = self.clean_text(article)\n",
    "            articles.append(clean_article)\n",
    "            raw_articles.append(raw_article)\n",
    "            self.progress_bar(index+1, len(urls))\n",
    "\n",
    "\n",
    "        yahoo_driver.quit()\n",
    "\n",
    "        return articles, raw_articles\n",
    "    \n",
    "    def download_yahoo_data(self, file_path):\n",
    "        \"\"\"\n",
    "        Downloadt gegevens van Yahoo Finance voor tickers die zijn vermeld in het opgegeven CSV-bestand.\n",
    "\n",
    "        Parameters:\n",
    "        - file_path (str): Bestandspad naar het CSV-bestand met ticker-symbolen.\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame: Gefilterd DataFrame met gedownloade gegevens.\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(file_path)\n",
    "        symbol_list = df.iloc[:, 0].tolist()\n",
    "\n",
    "        download = yf.download(symbol_list, group_by=\"ticker\")\n",
    "        data = download.copy()\n",
    "\n",
    "        filtered_data = data.dropna(axis=1, how='all')\n",
    "        filtered_data.columns = filtered_data.columns.remove_unused_levels()\n",
    "\n",
    "        remaining_tickers = list(filtered_data.columns.levels[0])\n",
    "        missing_tickers = list(set(symbol_list) - set(remaining_tickers))\n",
    "\n",
    "        return filtered_data    \n",
    "    \n",
    "    def get_card_titles(self):\n",
    "        titles = []\n",
    "        a_tags = self.soup.find_all('a', class_='card')\n",
    "        for i in range(len(a_tags)):\n",
    "            titles.append(a_tags[i].span.text)\n",
    "        return titles\n",
    "\n",
    "    def get_card_urls(self):\n",
    "        urls = []\n",
    "        a_tags = self.soup.find_all('a', class_='card')\n",
    "        for i in range(len(a_tags)):\n",
    "            urls.append(a_tags[i]['href'])\n",
    "        return urls\n",
    "    \n",
    "    def get_url_content(self, urls):\n",
    "        articles = []\n",
    "        raw_articles = []\n",
    "        for url in urls:\n",
    "            soup = self.get_response(url, self.HEADERS)\n",
    "            paragraphs = soup.find('div', class_='article-content').find_all('p')\n",
    "            article = ''.join([p.get_text(separator=' ') for p in paragraphs])\n",
    "            raw_articles.append(paragraphs)\n",
    "            articles.append(self.clean_text(article))\n",
    "\n",
    "        return articles, raw_articles\n",
    "    \n",
    "    def get_symbols(self, raw_articles, href_ref):\n",
    "        symbol_list = []\n",
    "        for p_list in raw_articles:\n",
    "            article_symbols = []\n",
    "            for p in p_list:\n",
    "                symbols = [a_tag.text for a_tag in p.find_all('a') if re.search(href_ref, a_tag['href'])]\n",
    "                if symbols:\n",
    "                    for symbol in symbols:\n",
    "                        article_symbols.append(symbol)\n",
    "            symbol_list.append(article_symbols)\n",
    "        return symbol_list\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        return re.sub(r'[^\\w\\s]', '', text)\n",
    "            \n",
    "    def get_article_links(self, driver):\n",
    "        links = driver.find_elements(By.TAG_NAME, 'a')\n",
    "        return links\n",
    "    \n",
    "    def progress_bar(self, current, total, bar_length=20):\n",
    "        fraction = current / total\n",
    "\n",
    "        arrow = int(fraction * bar_length - 1) * '-' + '>'\n",
    "        padding = int(bar_length - len(arrow)) * ' '\n",
    "\n",
    "        ending = '\\n' if current == total else '\\r'\n",
    "\n",
    "        print(f'Progress: [{arrow}{padding}] {int(fraction*100)}%', end=ending)\n",
    "\n",
    "    def clean_and_tokenize(self, articles):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        tokenized_articles = []\n",
    "        for article in articles:\n",
    "            tokenized_article = nltk.word_tokenize(article)\n",
    "            tokenized_article = [token for token in tokenized_article if token not in stopwords.words('english')]\n",
    "            tokenized_article = [lemmatizer.lemmatize(token) for token in tokenized_article]\n",
    "\n",
    "            tokenized_articles.append(tokenized_article)\n",
    "\n",
    "        return tokenized_articles\n",
    "    \n",
    "    def combine_data(self, api_df, investopedia_df, yahoo_df, time_data):\n",
    "        df1 = investopedia_df.drop(['symbols'], axis=1)\n",
    "        df2 = yahoo_df.drop('classes', axis=1)\n",
    "        text_df = pd.concat([df1, df2])\n",
    "        text_df = text_df.reset_index(drop=True)\n",
    "        \n",
    "        # tokenize the articles and add them to a DataFrame\n",
    "        text_df['tokenized_article'] = self.clean_and_tokenize(text_df['article'])\n",
    "        \n",
    "        # apply sentiment analysis and add it to the DataFrame\n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "        text_df['positivity_score'] = [analyzer.polarity_scores(article)['pos'] for article in text_df['article']]\n",
    "        text_df['negativity_score'] = [analyzer.polarity_scores(article)['neg'] for article in text_df['article']]\n",
    "        \n",
    "        # get statistics from positive and negative sentiment analysis\n",
    "        pos = text_df.describe().T[:1].reset_index(drop=True)\n",
    "        neg = text_df.describe().T[1:].reset_index(drop=True)\n",
    "        \n",
    "        # rename columns in pos and neg so they can be concattinated\n",
    "        pos.columns = ['pos_' + col for col in pos.columns]\n",
    "        neg.columns = ['neg_' + col for col in neg.columns]\n",
    "        \n",
    "        # concattinate the positve and negative sentiment analysis statistics to a 1 row DataFrame\n",
    "        pos_neg_row = pd.concat([pos, neg], axis=1)\n",
    "        \n",
    "        api_df = api_df.iloc[::-1].reset_index(drop=False, names='Date')\n",
    "        \n",
    "        api_df.columns = ['_'.join(col).strip() for col in api_df.columns.values]\n",
    "        \n",
    "        combined_df = pd.concat([api_df, pos_neg_row], axis=1)\n",
    "        \n",
    "        combined_df['Date_'] = combined_df['Date_'].astype(str)\n",
    "        time_data['Date'] = time_data['Date'].astype(str)\n",
    "        \n",
    "        display(self.api_df.columns)\n",
    "        display(combined_df, time_data)\n",
    "\n",
    "        final_df = combined_df.merge(time_data, how='left', left_on='Date_', right_on='Date')\n",
    "        \n",
    "        return final_df\n",
    "\n",
    "    def extract(self):\n",
    "        # get site data\n",
    "        self.soup = self.get_response(self.HOMEPAGE_URL, self.HEADERS)\n",
    "\n",
    "        # download csv data\n",
    "        # get_datahub_csv(self.VUSA_DOWNLOAD_URL, 'vusa_holdings.csv')\n",
    "        # get_datahub_csv(self.SP_DOWNLOAD_URL, 'time_data.csv')\n",
    "       \n",
    "        # get csv data\n",
    "        self.vusa_df = pd.read_csv(self.DATA_LOCATION + 'vusa_holdings.csv')\n",
    "        self.time_data_df = pd.read_csv(self.DATA_LOCATION + 'time_data.csv')\n",
    "\n",
    "        # get yahoo articles with titles, urls and css classes\n",
    "        self.yahoo_news_df = self.get_yahoo_news_articles_df()\n",
    "    \n",
    "        # scrape news articles from yahoo article urls\n",
    "        self.yahoo_news_articles, self.raw_yahoo_news_articles = self.scrape_articles(self.yahoo_news_df['url'])\n",
    "        \n",
    "        # get api data\n",
    "        self.api_df = self.download_yahoo_data('Datasets/vusa_holdings.csv')\n",
    "    \n",
    "    \n",
    "    def transform(self):        \n",
    "        # filter site data\n",
    "        titles = self.get_card_titles()\n",
    "        urls = self.get_card_urls()\n",
    "        \n",
    "        # filter symbols from the vusa_df csv data\n",
    "        symbols_in_vusa = self.vusa_df['Symbol'].tolist()\n",
    "\n",
    "        # add scraped articles to DataFrame\n",
    "        self.yahoo_news_df['article'] = self.yahoo_news_articles\n",
    "\n",
    "        # transform filtered data to DataFrame(investopedia_df)\n",
    "        investopedia_df = pd.DataFrame({'url': urls, 'title': titles})\n",
    "        \n",
    "        # get articles from urls\n",
    "        articles, raw_articles = self.get_url_content(investopedia_df['url'])\n",
    "        \n",
    "        # get symbols from article\n",
    "        symbols = self.get_symbols(raw_articles, 'widgetsymbol')\n",
    "        \n",
    "        # add articles and symbols to DataFrame(investopedia_df)\n",
    "        investopedia_df['article'] = articles\n",
    "        investopedia_df['symbols'] = symbols\n",
    "        \n",
    "        # filter DataFrame(investopedia_df) to only have symbols that occur in the symbols_in_vusa\n",
    "        investopedia_df = investopedia_df[investopedia_df['symbols'].apply(lambda symbols: any(symbol in symbols_in_vusa for symbol in symbols))]\n",
    "        \n",
    "        # combine DataFrames\n",
    "        self.df = self.combine_data(self.api_df, investopedia_df, self.yahoo_news_df, self.time_data_df)\n",
    "          \n",
    "    def load(self, filename):\n",
    "        self.df.to_csv(filename)\n",
    "    \n",
    "    def ETL(self, filename):\n",
    "        self.extract()\n",
    "        self.transform()\n",
    "        self.load(filename)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "e3225b65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mark\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Mark\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mark\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Mark\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Mark\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16129433",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [------------------->] 100%\n",
      "[*********************100%%**********************]  505 of 505 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "29 Failed downloads:\n",
      "['FBHS', 'XLNX', 'ANTM', 'DISCK', 'ATVI', 'BRK.B', 'NLOK', 'RE', 'DISCA', 'KSU', 'ABC', 'WLTW', 'SIVB', 'NLSN', 'CERN', 'FISV', 'BLL', 'FB', 'DRE', 'PKI', 'VIAC', 'CTXS', 'PBCT', 'TWTR', 'INFO', 'FRC']: Exception('%ticker%: No timezone found, symbol may be delisted')\n",
      "['BF.B', 'AVY', 'PEG']: Exception('%ticker%: No price data found, symbol may be delisted (1d 1925-02-16 -> 2024-01-23)')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultiIndex([('HPE',      'Open'),\n",
       "            ('HPE',      'High'),\n",
       "            ('HPE',       'Low'),\n",
       "            ('HPE',     'Close'),\n",
       "            ('HPE', 'Adj Close'),\n",
       "            ('HPE',    'Volume'),\n",
       "            ('NRG',      'Open'),\n",
       "            ('NRG',      'High'),\n",
       "            ('NRG',       'Low'),\n",
       "            ('NRG',     'Close'),\n",
       "            ...\n",
       "            ( 'WY',       'Low'),\n",
       "            ( 'WY',     'Close'),\n",
       "            ( 'WY', 'Adj Close'),\n",
       "            ( 'WY',    'Volume'),\n",
       "            ('JPM',      'Open'),\n",
       "            ('JPM',      'High'),\n",
       "            ('JPM',       'Low'),\n",
       "            ('JPM',     'Close'),\n",
       "            ('JPM', 'Adj Close'),\n",
       "            ('JPM',    'Volume')],\n",
       "           length=2856)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date_</th>\n",
       "      <th>HPE_Open</th>\n",
       "      <th>HPE_High</th>\n",
       "      <th>HPE_Low</th>\n",
       "      <th>HPE_Close</th>\n",
       "      <th>HPE_Adj Close</th>\n",
       "      <th>HPE_Volume</th>\n",
       "      <th>NRG_Open</th>\n",
       "      <th>NRG_High</th>\n",
       "      <th>NRG_Low</th>\n",
       "      <th>...</th>\n",
       "      <th>pos_75%</th>\n",
       "      <th>pos_max</th>\n",
       "      <th>neg_count</th>\n",
       "      <th>neg_mean</th>\n",
       "      <th>neg_std</th>\n",
       "      <th>neg_min</th>\n",
       "      <th>neg_25%</th>\n",
       "      <th>neg_50%</th>\n",
       "      <th>neg_75%</th>\n",
       "      <th>neg_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-23</td>\n",
       "      <td>15.63</td>\n",
       "      <td>15.700</td>\n",
       "      <td>15.460</td>\n",
       "      <td>15.50</td>\n",
       "      <td>15.50</td>\n",
       "      <td>8469332.0</td>\n",
       "      <td>52.700001</td>\n",
       "      <td>52.799999</td>\n",
       "      <td>52.285000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.216</td>\n",
       "      <td>241.0</td>\n",
       "      <td>0.050456</td>\n",
       "      <td>0.029141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-22</td>\n",
       "      <td>15.48</td>\n",
       "      <td>15.620</td>\n",
       "      <td>15.400</td>\n",
       "      <td>15.50</td>\n",
       "      <td>15.50</td>\n",
       "      <td>9414400.0</td>\n",
       "      <td>52.709999</td>\n",
       "      <td>53.080002</td>\n",
       "      <td>52.200001</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-19</td>\n",
       "      <td>15.03</td>\n",
       "      <td>15.425</td>\n",
       "      <td>14.925</td>\n",
       "      <td>15.37</td>\n",
       "      <td>15.37</td>\n",
       "      <td>13251800.0</td>\n",
       "      <td>51.090000</td>\n",
       "      <td>52.540001</td>\n",
       "      <td>50.939999</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-18</td>\n",
       "      <td>15.06</td>\n",
       "      <td>15.115</td>\n",
       "      <td>14.700</td>\n",
       "      <td>14.96</td>\n",
       "      <td>14.96</td>\n",
       "      <td>15106200.0</td>\n",
       "      <td>51.049999</td>\n",
       "      <td>51.189999</td>\n",
       "      <td>50.430000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-17</td>\n",
       "      <td>15.15</td>\n",
       "      <td>15.198</td>\n",
       "      <td>14.910</td>\n",
       "      <td>15.04</td>\n",
       "      <td>15.04</td>\n",
       "      <td>13466800.0</td>\n",
       "      <td>50.650002</td>\n",
       "      <td>52.029999</td>\n",
       "      <td>50.509998</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15616</th>\n",
       "      <td>1962-01-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15617</th>\n",
       "      <td>1962-01-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15618</th>\n",
       "      <td>1962-01-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15619</th>\n",
       "      <td>1962-01-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15620</th>\n",
       "      <td>1962-01-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15621 rows × 2873 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date_  HPE_Open  HPE_High  HPE_Low  HPE_Close  HPE_Adj Close  \\\n",
       "0      2024-01-23     15.63    15.700   15.460      15.50          15.50   \n",
       "1      2024-01-22     15.48    15.620   15.400      15.50          15.50   \n",
       "2      2024-01-19     15.03    15.425   14.925      15.37          15.37   \n",
       "3      2024-01-18     15.06    15.115   14.700      14.96          14.96   \n",
       "4      2024-01-17     15.15    15.198   14.910      15.04          15.04   \n",
       "...           ...       ...       ...      ...        ...            ...   \n",
       "15616  1962-01-08       NaN       NaN      NaN        NaN            NaN   \n",
       "15617  1962-01-05       NaN       NaN      NaN        NaN            NaN   \n",
       "15618  1962-01-04       NaN       NaN      NaN        NaN            NaN   \n",
       "15619  1962-01-03       NaN       NaN      NaN        NaN            NaN   \n",
       "15620  1962-01-02       NaN       NaN      NaN        NaN            NaN   \n",
       "\n",
       "       HPE_Volume   NRG_Open   NRG_High    NRG_Low  ...  pos_75%  pos_max  \\\n",
       "0       8469332.0  52.700001  52.799999  52.285000  ...    0.139    0.216   \n",
       "1       9414400.0  52.709999  53.080002  52.200001  ...      NaN      NaN   \n",
       "2      13251800.0  51.090000  52.540001  50.939999  ...      NaN      NaN   \n",
       "3      15106200.0  51.049999  51.189999  50.430000  ...      NaN      NaN   \n",
       "4      13466800.0  50.650002  52.029999  50.509998  ...      NaN      NaN   \n",
       "...           ...        ...        ...        ...  ...      ...      ...   \n",
       "15616         NaN        NaN        NaN        NaN  ...      NaN      NaN   \n",
       "15617         NaN        NaN        NaN        NaN  ...      NaN      NaN   \n",
       "15618         NaN        NaN        NaN        NaN  ...      NaN      NaN   \n",
       "15619         NaN        NaN        NaN        NaN  ...      NaN      NaN   \n",
       "15620         NaN        NaN        NaN        NaN  ...      NaN      NaN   \n",
       "\n",
       "       neg_count  neg_mean   neg_std  neg_min  neg_25%  neg_50%  neg_75%  \\\n",
       "0          241.0  0.050456  0.029141      0.0    0.029    0.048    0.067   \n",
       "1            NaN       NaN       NaN      NaN      NaN      NaN      NaN   \n",
       "2            NaN       NaN       NaN      NaN      NaN      NaN      NaN   \n",
       "3            NaN       NaN       NaN      NaN      NaN      NaN      NaN   \n",
       "4            NaN       NaN       NaN      NaN      NaN      NaN      NaN   \n",
       "...          ...       ...       ...      ...      ...      ...      ...   \n",
       "15616        NaN       NaN       NaN      NaN      NaN      NaN      NaN   \n",
       "15617        NaN       NaN       NaN      NaN      NaN      NaN      NaN   \n",
       "15618        NaN       NaN       NaN      NaN      NaN      NaN      NaN   \n",
       "15619        NaN       NaN       NaN      NaN      NaN      NaN      NaN   \n",
       "15620        NaN       NaN       NaN      NaN      NaN      NaN      NaN   \n",
       "\n",
       "       neg_max  \n",
       "0        0.172  \n",
       "1          NaN  \n",
       "2          NaN  \n",
       "3          NaN  \n",
       "4          NaN  \n",
       "...        ...  \n",
       "15616      NaN  \n",
       "15617      NaN  \n",
       "15618      NaN  \n",
       "15619      NaN  \n",
       "15620      NaN  \n",
       "\n",
       "[15621 rows x 2873 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>SP500</th>\n",
       "      <th>Dividend</th>\n",
       "      <th>Earnings</th>\n",
       "      <th>Consumer Price Index</th>\n",
       "      <th>Long Interest Rate</th>\n",
       "      <th>Real Price</th>\n",
       "      <th>Real Dividend</th>\n",
       "      <th>Real Earnings</th>\n",
       "      <th>PE10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1871-01-01</td>\n",
       "      <td>4.44</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.40</td>\n",
       "      <td>12.46</td>\n",
       "      <td>5.32</td>\n",
       "      <td>89.00</td>\n",
       "      <td>5.21</td>\n",
       "      <td>8.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1871-02-01</td>\n",
       "      <td>4.50</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.40</td>\n",
       "      <td>12.84</td>\n",
       "      <td>5.32</td>\n",
       "      <td>87.53</td>\n",
       "      <td>5.06</td>\n",
       "      <td>7.78</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1871-03-01</td>\n",
       "      <td>4.61</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.40</td>\n",
       "      <td>13.03</td>\n",
       "      <td>5.33</td>\n",
       "      <td>88.36</td>\n",
       "      <td>4.98</td>\n",
       "      <td>7.67</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1871-04-01</td>\n",
       "      <td>4.74</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.40</td>\n",
       "      <td>12.56</td>\n",
       "      <td>5.33</td>\n",
       "      <td>94.29</td>\n",
       "      <td>5.17</td>\n",
       "      <td>7.96</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1871-05-01</td>\n",
       "      <td>4.86</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.40</td>\n",
       "      <td>12.27</td>\n",
       "      <td>5.33</td>\n",
       "      <td>98.93</td>\n",
       "      <td>5.29</td>\n",
       "      <td>8.14</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1763</th>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>2664.34</td>\n",
       "      <td>48.93</td>\n",
       "      <td>109.88</td>\n",
       "      <td>246.52</td>\n",
       "      <td>2.40</td>\n",
       "      <td>2700.13</td>\n",
       "      <td>49.59</td>\n",
       "      <td>111.36</td>\n",
       "      <td>32.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1764</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>2789.80</td>\n",
       "      <td>49.29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.87</td>\n",
       "      <td>2.58</td>\n",
       "      <td>2811.96</td>\n",
       "      <td>49.68</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1765</th>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2705.16</td>\n",
       "      <td>49.64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>248.99</td>\n",
       "      <td>2.86</td>\n",
       "      <td>2714.34</td>\n",
       "      <td>49.81</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766</th>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>2702.77</td>\n",
       "      <td>50.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>249.55</td>\n",
       "      <td>2.84</td>\n",
       "      <td>2705.82</td>\n",
       "      <td>50.06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1767</th>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>2642.19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>249.84</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2642.19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1768 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date    SP500  Dividend  Earnings  Consumer Price Index  \\\n",
       "0     1871-01-01     4.44      0.26      0.40                 12.46   \n",
       "1     1871-02-01     4.50      0.26      0.40                 12.84   \n",
       "2     1871-03-01     4.61      0.26      0.40                 13.03   \n",
       "3     1871-04-01     4.74      0.26      0.40                 12.56   \n",
       "4     1871-05-01     4.86      0.26      0.40                 12.27   \n",
       "...          ...      ...       ...       ...                   ...   \n",
       "1763  2017-12-01  2664.34     48.93    109.88                246.52   \n",
       "1764  2018-01-01  2789.80     49.29       NaN                247.87   \n",
       "1765  2018-02-01  2705.16     49.64       NaN                248.99   \n",
       "1766  2018-03-01  2702.77     50.00       NaN                249.55   \n",
       "1767  2018-04-01  2642.19       NaN       NaN                249.84   \n",
       "\n",
       "      Long Interest Rate  Real Price  Real Dividend  Real Earnings   PE10  \n",
       "0                   5.32       89.00           5.21           8.02    NaN  \n",
       "1                   5.32       87.53           5.06           7.78    NaN  \n",
       "2                   5.33       88.36           4.98           7.67    NaN  \n",
       "3                   5.33       94.29           5.17           7.96    NaN  \n",
       "4                   5.33       98.93           5.29           8.14    NaN  \n",
       "...                  ...         ...            ...            ...    ...  \n",
       "1763                2.40     2700.13          49.59         111.36  32.09  \n",
       "1764                2.58     2811.96          49.68            NaN  33.31  \n",
       "1765                2.86     2714.34          49.81            NaN  32.12  \n",
       "1766                2.84     2705.82          50.06            NaN  31.99  \n",
       "1767                2.80     2642.19            NaN            NaN  31.19  \n",
       "\n",
       "[1768 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline.ETL('Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad26226c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14929ad2",
   "metadata": {},
   "source": [
    "# Referentielijst\n",
    "- Fondsbeheerder|NOT,Disclosed|Vanguard S&P 500 UCITS ETF (EUR)|ISIN:IE00B3XXRP09. (n.d.). https://www.morningstar.nl/nl/etf/snapshot/snapshot.aspx?id=0P0000YXKB&tab=4&InvestmentType=FE\n",
    "- Wat zijn ETF’s | Educatie | BlackRock. (n.d.). BlackRock. https://www.blackrock.com/be/individual/nl/educatie/etfs-uitgelegd#Wat-zijn-ETF'\n",
    "- S&P 500 | Wat is de S&P 500? | Beleggingswiki - Semmie.nl. (n.d.). Semmie. https://semmie.nl/wiki/sp-500/#:~:text=De%20S%26P%20500%20is%20een,ontwikkeling%20van%20de%20financi%C3%ABle%20markt.\n",
    "- SWOCC. (2020, June 8). De invloed van mediaberichtgeving op beurskoersen - SWOCC. https://www.swocc.nl/kennisbank-item/de-invloed-van-mediaberichtgeving-op-beurskoersen/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
