{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a96dba24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? n\n",
      "Nothing done.\n"
     ]
    }
   ],
   "source": [
    "# !pip install git+https://github.com/psf/requests-html.git#egg=requests-html\n",
    "%reset\n",
    "import httpx\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eebd4d5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mark\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Mark\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mark\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Mark\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Mark\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "HEADERS = {'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "           'Accept-Encoding': 'gzip, deflate, br',\n",
    "           'Accept-Language': 'en-US,en;q=0.9,nl;q=0.8',\n",
    "           'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'\n",
    "          }\n",
    "\n",
    "DATA_LOCATION = 'Datasets/'\n",
    "\n",
    "HOMEPAGE_URL = 'https://www.investopedia.com/markets-news-4427704'\n",
    "\n",
    "VUSA_DOWNLOAD_URL = 'https://datahub.io/core/s-and-p-500-companies'\n",
    "SP_DOWNLOAD_URL = 'https://datahub.io/core/s-and-p-500'\n",
    "DATAHUB_URL = 'https://datahub.io'\n",
    "\n",
    "\n",
    "YAHOO_URL = 'https://finance.yahoo.com/'\n",
    "\n",
    "nltk.download(['punkt', 'wordnet', 'stopwords', 'omw-1.4', 'vader_lexicon'])\n",
    "\n",
    "options = Options()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--disable-gpu') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8569ca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(url, cookies={}, headers=HEADERS):\n",
    "    r = httpx.get(url, cookies=cookies, headers=headers, follow_redirects=True)\n",
    "    soup = bs(r.content)\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "146a16d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_card_titles(soup):\n",
    "    titles = []\n",
    "    a_tags = soup.find_all('a', class_='card')\n",
    "    for i in range(len(a_tags)):\n",
    "        titles.append(a_tags[i].span.text)\n",
    "    return titles\n",
    "\n",
    "def get_card_urls(soup):\n",
    "    urls = []\n",
    "    a_tags = soup.find_all('a', class_='card')\n",
    "    for i in range(len(a_tags)):\n",
    "        urls.append(a_tags[i]['href'])\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "625653e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00c0fc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_content(urls):\n",
    "    articles = []\n",
    "    raw_articles = []\n",
    "    for url in urls:\n",
    "        soup = get_response(url)\n",
    "        paragraphs = soup.find('div', class_='article-content').find_all('p')\n",
    "        article = ''.join([p.get_text(separator=' ') for p in paragraphs])\n",
    "        \n",
    "        raw_articles.append(paragraphs)\n",
    "        articles.append(clean_text(article))\n",
    "    return articles, raw_articles\n",
    "\n",
    "def get_symbols(raw_articles, href_ref):\n",
    "    symbol_list = []\n",
    "    for p_list in raw_articles:\n",
    "        article_symbols = []\n",
    "        for p in p_list:\n",
    "            symbols = [a_tag.text for a_tag in p.find_all('a') if re.search(href_ref, a_tag['href'])]\n",
    "            if symbols:\n",
    "                for symbol in symbols:\n",
    "                    article_symbols.append(symbol)\n",
    "        symbol_list.append(article_symbols)\n",
    "    return symbol_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8d83664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datahub_csv(URL, filename):\n",
    "    soup = get_response(URL)\n",
    "    download_url = soup.find_all('table')[1].find_all('a')[1]['href']\n",
    "    download_url = DATAHUB_URL + download_url\n",
    "    with open(DATA_LOCATION + filename, 'wb') as f:\n",
    "        f.write(requests.get(download_url).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd721962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_links(driver):\n",
    "    links = driver.find_elements(By.TAG_NAME, 'a')\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "669ceeef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_yahoo_news_articles_df():\n",
    "    yahoo_news_driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    yahoo_news_driver.get(YAHOO_URL)\n",
    "    time.sleep(1)\n",
    "    yahoo_news_driver.find_element(By.CLASS_NAME, 'reject-all').click()\n",
    "    actions = ActionChains(yahoo_news_driver)\n",
    "    for i in range(1500):\n",
    "        actions.scroll_by_amount(0, 500)\n",
    "    actions.perform()\n",
    "    actions.reset_actions()\n",
    "\n",
    "    links = get_article_links(yahoo_news_driver)\n",
    "    df = pd.DataFrame({'title': [link.text for link in links], 'url': [link.get_attribute('href') for link in links], 'classes': [link.get_attribute('class') for link in links]})\n",
    "    yahoo_news_driver.quit()\n",
    "    df = df[df['url'] \\\n",
    "            .str.contains('/news/') & df['classes'].str.contains('js-content-viewer')]\\\n",
    "            .drop_duplicates(subset='url', keep='last')\\\n",
    "            .reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d71d904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress_bar(current, total, bar_length=20):\n",
    "    fraction = current / total\n",
    "\n",
    "    arrow = int(fraction * bar_length - 1) * '-' + '>'\n",
    "    padding = int(bar_length - len(arrow)) * ' '\n",
    "\n",
    "    ending = '\\n' if current == total else '\\r'\n",
    "\n",
    "    print(f'Progress: [{arrow}{padding}] {int(fraction*100)}%', end=ending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1b2a369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_articles(urls):\n",
    "    articles = []\n",
    "    raw_articles = []\n",
    "    yahoo_driver = webdriver.Chrome(options=options)\n",
    "    yahoo_driver.get(YAHOO_URL)\n",
    "    yahoo_driver.find_element(By.CLASS_NAME, 'reject-all').click()\n",
    "\n",
    "    for index, url in enumerate(urls):\n",
    "        yahoo_driver.get(url)\n",
    "        try:\n",
    "            yahoo_driver.find_element(By.CLASS_NAME, 'collapse-button').click()\n",
    "        except:\n",
    "            pass\n",
    "        article_p_elements = yahoo_driver.find_element(By.CLASS_NAME, 'caas-body').find_elements(By.TAG_NAME, 'p')\n",
    "        article_by_paragraph = [p.text for p in article_p_elements if p.text!='']\n",
    "        \n",
    "        raw_article = [bs(element.get_attribute('outerHTML'), \"html.parser\") for element in article_p_elements]\n",
    "        article = \" \".join(article_by_paragraph)\n",
    "        clean_article = clean_text(article)\n",
    "        articles.append(clean_article)\n",
    "        raw_articles.append(raw_article)\n",
    "        progress_bar(index+1, len(urls))\n",
    "    \n",
    "        \n",
    "    yahoo_driver.quit()\n",
    "    \n",
    "    return articles, raw_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "63df26ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_text_dfs(investopedia_df, yahoo_df):\n",
    "    df1 = investopedia_df.drop(['symbols', 'appears_in_vusa'], axis=1)\n",
    "    df2 = yahoo_df.drop('classes', axis=1)\n",
    "    article_df = pd.concat([df1, df2])\n",
    "    article_df = article_df.reset_index(drop=True)\n",
    "    return article_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd0d18cf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def clean_and_tokenize(articles):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    tokenized_articles = []\n",
    "    for article in articles:\n",
    "        tokenized_article = nltk.word_tokenize(article)\n",
    "        tokenized_article = [token for token in tokenized_article if token not in stopwords.words('english')]\n",
    "        tokenized_article = [lemmatizer.lemmatize(token) for token in tokenized_article]\n",
    "        \n",
    "        tokenized_articles.append(tokenized_article)\n",
    "        \n",
    "    return tokenized_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dbc7ec04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_api_df(vusa_df): \n",
    "    symbol_list = list(vusa_df['Symbol'])\n",
    "    data = yf.download(symbol_list, group_by=\"ticker\")\n",
    "    filtered_data = data.dropna(axis=1, how='all')\n",
    "    filtered_data.columns = filtered_data.columns.remove_unused_levels()\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba55a113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dfs(api_df, text_df, downloaded_df):\n",
    "    pos = text_df.describe().T[:1].reset_index(drop=True)\n",
    "    neg = text_df.describe().T[1:].reset_index(drop=True)\n",
    "\n",
    "    pos.columns = ['pos_' + col for col in pos.columns]\n",
    "    neg.columns = ['neg_' + col for col in neg.columns]\n",
    "\n",
    "    pos_neg_row = pd.concat([pos, neg], axis=1)\n",
    "    \n",
    "    prices_df = api_df.merge(downloaded_df, how='left', left_index=True, right_on='Date')\n",
    "    \n",
    "    prices_df = prices_df.iloc[::-1]\n",
    "    \n",
    "    final_df = pd.concat([prices_df, pos_neg_row], axis=1)\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c8236b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract():\n",
    "    # download\n",
    "    get_datahub_csv(VUSA_DOWNLOAD_URL, 'vusa_holdings.csv')\n",
    "    get_datahub_csv(SP_DOWNLOAD_URL, 'time_data.csv')\n",
    "    \n",
    "    vusa_df = pd.read_csv(DATA_LOCATION + 'vusa_holdings.csv')\n",
    "    time_data_df = pd.read_csv(DATA_LOCATION + 'time_data.csv')\n",
    "    \n",
    "    #api\n",
    "    api_df = get_api_df(vusa_df)\n",
    "    \n",
    "    #investopedia\n",
    "    soup = get_response(HOMEPAGE_URL, HEADERS)\n",
    "\n",
    "    titles = get_card_titles(soup)\n",
    "    urls = get_card_urls(soup)\n",
    "\n",
    "    investopedia_df = pd.DataFrame({'url': urls, 'title': titles})\n",
    "\n",
    "    articles, raw_articles = get_url_content(investopedia_df['url'])\n",
    "    investopedia_df['article'] = articles\n",
    "    symbols = get_symbols(raw_articles, 'widgetsymbol')\n",
    "    investopedia_df['symbols'] = symbols\n",
    "    \n",
    "    #yahoo\n",
    "    yahoo_news_df = get_yahoo_news_articles_df()\n",
    "    \n",
    "    yahoo_news_articles, raw_yahoo_news_articles = scrape_articles(yahoo_news_df['url'])\n",
    "\n",
    "    yahoo_news_df['article'] = yahoo_news_articles\n",
    "    \n",
    "    return vusa_df, api_df, time_data_df, investopedia_df, yahoo_news_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e0b7ed49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  505 of 505 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "27 Failed downloads:\n",
      "['PBCT', 'ANTM', 'DISCK', 'BRK.B', 'ABC', 'CERN', 'WLTW', 'INFO', 'XLNX', 'BLL', 'DISCA', 'SIVB', 'ATVI', 'FRC', 'NLSN', 'KSU', 'FBHS', 'PKI', 'RE', 'FISV', 'TWTR', 'DRE', 'VIAC', 'FB', 'CTXS', 'NLOK']: Exception('%ticker%: No timezone found, symbol may be delisted')\n",
      "['BF.B']: Exception('%ticker%: No price data found, symbol may be delisted (1d 1925-02-08 -> 2024-01-16)')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress: [------------------->] 100%\n"
     ]
    }
   ],
   "source": [
    "vusa_df, api_df, time_data_df, investopedia_df, yahoo_news_df = extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "22768dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(vusa_df, api_df, time_data_df, investopedia_df, yahoo_news_df):\n",
    "    symbols_in_vusa = vusa_df['Symbol'].tolist()\n",
    "\n",
    "    investopedia_df['appears_in_vusa'] = investopedia_df['symbols'].apply(lambda symbols: any(symbol in symbols_in_vusa for symbol in symbols))\n",
    "    investopedia_df = investopedia_df[investopedia_df['appears_in_vusa']]\n",
    "\n",
    "    text_df = combine_text_dfs(investopedia_df, yahoo_news_df)\n",
    "\n",
    "    text_df['tokenized_article'] = clean_and_tokenize(text_df['article'])\n",
    "\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    scaler = MinMaxScaler()\n",
    "    text_df['positivity_score'] = [analyzer.polarity_scores(article)['pos'] for article in text_df['article']]\n",
    "    text_df['negativity_score'] = [analyzer.polarity_scores(article)['neg'] for article in text_df['article']]\n",
    "    \n",
    "    final_df = combine_dfs(api_df, text_df, time_data_df)\n",
    "    \n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aad5b2bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "MergeError",
     "evalue": "Not allowed to merge between different levels. (2 levels on the left, 1 on the right)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMergeError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m final_df \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvusa_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_data_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minvestopedia_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myahoo_news_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m final_df\n",
      "Cell \u001b[1;32mIn[43], line 16\u001b[0m, in \u001b[0;36mtransform\u001b[1;34m(vusa_df, api_df, time_data_df, investopedia_df, yahoo_news_df)\u001b[0m\n\u001b[0;32m     13\u001b[0m text_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpositivity_score\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [analyzer\u001b[38;5;241m.\u001b[39mpolarity_scores(article)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m text_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m     14\u001b[0m text_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegativity_score\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [analyzer\u001b[38;5;241m.\u001b[39mpolarity_scores(article)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m text_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m---> 16\u001b[0m final_df \u001b[38;5;241m=\u001b[39m \u001b[43mcombine_dfs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_data_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final_df\n",
      "Cell \u001b[1;32mIn[28], line 10\u001b[0m, in \u001b[0;36mcombine_dfs\u001b[1;34m(api_df, text_df, downloaded_df)\u001b[0m\n\u001b[0;32m      6\u001b[0m neg\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m neg\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[0;32m      8\u001b[0m pos_neg_row \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([pos, neg], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m prices_df \u001b[38;5;241m=\u001b[39m \u001b[43mapi_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdownloaded_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m prices_df \u001b[38;5;241m=\u001b[39m prices_df\u001b[38;5;241m.\u001b[39miloc[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     14\u001b[0m final_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([prices_df, pos_neg_row], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:10487\u001b[0m, in \u001b[0;36mDataFrame.merge\u001b[1;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m  10468\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m  10469\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m  10470\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10483\u001b[0m     validate: MergeValidate \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m  10484\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m  10485\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmerge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[1;32m> 10487\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  10488\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10496\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10497\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10501\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:169\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _cross_merge(\n\u001b[0;32m    155\u001b[0m         left_df,\n\u001b[0;32m    156\u001b[0m         right_df,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    166\u001b[0m         copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    167\u001b[0m     )\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_MergeOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result(copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:781\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    775\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _left\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m!=\u001b[39m _right\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels:\n\u001b[0;32m    776\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    777\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot allowed to merge between different levels. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    778\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_left\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m levels on the left, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    779\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_right\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on the right)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    780\u001b[0m     )\n\u001b[1;32m--> 781\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MergeError(msg)\n\u001b[0;32m    783\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_on, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_on \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_left_right_on(left_on, right_on)\n\u001b[0;32m    785\u001b[0m (\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_join_keys,\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_join_keys,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    790\u001b[0m     right_drop,\n\u001b[0;32m    791\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_merge_keys()\n",
      "\u001b[1;31mMergeError\u001b[0m: Not allowed to merge between different levels. (2 levels on the left, 1 on the right)"
     ]
    }
   ],
   "source": [
    "final_df = transform(vusa_df, api_df, time_data_df, investopedia_df, yahoo_news_df)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6a99b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = get_response(HOMEPAGE_URL, HEADERS)\n",
    "\n",
    "titles = get_card_titles(soup)\n",
    "urls = get_card_urls(soup)\n",
    "\n",
    "investopedia_df = pd.DataFrame({'url': urls, 'title': titles})\n",
    "\n",
    "articles, raw_articles = get_url_content(investopedia_df['url'])\n",
    "investopedia_df['article'] = articles\n",
    "symbols = get_symbols(raw_articles, 'widgetsymbol')\n",
    "investopedia_df['symbols'] = symbols\n",
    "\n",
    "# get_datahub_csv(VUSA_DOWNLOAD_URL, 'vusa_holdings.csv')\n",
    "# get_datahub_csv(SP_DOWNLOAD_URL, 'time_data.csv')\n",
    "\n",
    "time_data_df = pd.read_csv(DATA_LOCATION + 'time_data.csv')\n",
    "\n",
    "# vusa_df = pd.read_csv(DATA_LOCATION + 'vusa_holdings.csv')\n",
    "# symbols_in_vusa = vusa_df['Symbol'].tolist()\n",
    "\n",
    "# investopedia_df['appears_in_vusa'] = investopedia_df['symbols'].apply(lambda symbols: any(symbol in symbols_in_vusa for symbol in symbols))\n",
    "# investopedia_df = investopedia_df[investopedia_df['appears_in_vusa']]\n",
    "\n",
    "display(investopedia_df)\n",
    "display(time_data_df)\n",
    "\n",
    "# yahoo_news_df = get_yahoo_news_articles_df(\n",
    "# )\n",
    "# yahoo_news_articles, raw_yahoo_news_articles = scrape_articles(yahoo_news_df['url'])\n",
    "\n",
    "# yahoo_news_df['article'] = yahoo_news_articles\n",
    "\n",
    "# df = combine_dfs(investopedia_df, yahoo_news_df)\n",
    "\n",
    "# df['tokenized_article'] = clean_and_tokenize(df['article'])\n",
    "\n",
    "# analyzer = SentimentIntensityAnalyzer()\n",
    "# scaler = MinMaxScaler()\n",
    "# df['positivity_score'] = [analyzer.polarity_scores(article)['pos'] for article in df['article']]\n",
    "# df['negativity_score'] = [analyzer.polarity_scores(article)['neg'] for article in df['article']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e2c5e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
