{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a96dba24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "import re\n",
    "import requests\n",
    "import codecs\n",
    "import time\n",
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "import yfinance as yf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57d341c",
   "metadata": {},
   "source": [
    "# Probleemomschrijving\n",
    "Het doel is om een pipeline te creeën die data ophaalt, transformeert en laad om vervolgens gebruikt te kunnen worden om de prijs van de ETF VUSA te voorspellen. Hierbij gaan wij artikelen van investopedia en yahoo finance scrapen en hier sentiment analysis op toepassen. Ook halen wij de historische prijsdata op van de verschillende aandelen die zich in de ETF bevinden. En als laatst halen wij de historische prijsdata op van VUSA zelf op en voegen wij alles samen tot 1 DataFrame. Deze pipeline kan elke dag gerund worden om zo nieuwe artikelen op te halen en deze toe te voegen aan de dataset, zo krijg je een steeds beter bruikbare dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bdc67b",
   "metadata": {},
   "source": [
    "# Domeinonderzoek\n",
    "Vusa (IE00B3XXRP09) is een ETF die beheert wordt door Vanguard group (Fondsbeheerder|NOT,Disclosed|Vanguard S&P 500 UCITS ETF (EUR)|ISIN:IE00B3XXRP09, n.d.). Een ETF is een aandeel wat eigenlijk bestaat uit meerdere aandelen, obligaties of andere effecten die op de markt verhandelt worden (Wat Zijn ETF’s | Educatie | BlackRock, n.d.). VUSA specifiek volgt de marktindex van de SP500, dat zijn aandelen van de 500 grootste bedrijven van de Verenigde Staten. \n",
    "\n",
    "De SP500 geeft een goed beeld van de amerikaanse markt, aangezien het veel verschillende sectoren en aandelen van grote bedrijven binnen deze sectoren bevat. SP500 heeft daarom een unieke ligging in de aandelenmarkt, omdat het zo groot kunnen er duidelijke veranderingen in de amerikaanse markt worden gemeten naar aanleiding van de SP500. Omdat de SP500 zo'n goed beeld geeft van de amerikaanse markt worden veel beleggingen hiermee vergeleken om te kijken hoe goed het gaat. (S&P 500 | Wat Is De S&P 500? | Beleggingswiki - Semmie.nl, n.d.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebffe97",
   "metadata": {},
   "source": [
    "# Vereiste informatie\n",
    "Wij willen willen artikelen scrapen om daar sentiment analysis op toe te passen. Wij zijn van mening dat dit belangrijke data is aangezien veel aandelen verhandeld worden met bots. Deze bots gebruiken ook sentiment analyse om te kijken of een aandeel gekocht moet worden, hierdoor kan de prijs beinvloed worden door het sentiment van de artikelen over een bedrijf (SWOCC, 2020). \n",
    "\n",
    "Ook maken wij gebruik van de prijsgeschiedenis van de aandelen die zich in het ETF bevinden, dit omdat de ETF een verzameling is van alle bedrijven en dus zullen de individuele bedrijven de ETF ook beinvloeden. Deze data komt uit een database die elke dag dat de beurzen open zijn geupdate wordt.\n",
    "\n",
    "Als laatst maken wij gebruik van een data over de prijsgeschiedenis van de SP500 zelf, de patronen die hierin zitten kunnen namelijk een beeld geven van hoe de toekomst van de SP500 en de ETF's die dit tracken (zoals ons aandeel: VUSA) eruit kan zien.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca61ba5",
   "metadata": {},
   "source": [
    "# Databronnen en formaat ruwe data\n",
    "- https://www.investopedia.com/markets-news-4427704 (html)\n",
    "- https://finance.yahoo.com/ (html)\n",
    "- https://github.com/ranaroussi/yfinance (dataframe)\n",
    "- https://datahub.io/core/s-and-p-500 (csv)\n",
    "- https://datahub.io/core/s-and-p-500-companies (csv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dd2e4a",
   "metadata": {},
   "source": [
    "# passende technieken voor het extraheren\n",
    "Voor het extraheren van de data maken wij gebruik van 4 verschillende technieken.\n",
    "\n",
    "## get requests\n",
    "Wij gebruiken get requests voor het ophalen van pagina's die niet dynamisch geladen zijn en geen cookies nodige hebben om de html van de pagina te kunnen halen. Wij doen een get request doormiddel van de requests library en zetten de content vervolgens om naar een beautifulsoup object. Hierdoor kunnen we makkelijk door de html spitten om de juiste data te vinden.\n",
    "\n",
    "Dit wordt gedaan bij deze bronnen:\n",
    "- **investopedia:**<br>\n",
    "Hier worden de hoofdpagina opgehaald en de urls van de artikelen eruit gehaald. Deze urls worden daarna ook met get requests opgehaald om zo de artikelen eruit te kunnen halen\n",
    "- **datahub.io:**<br>\n",
    "Wij gebruiken datahub 2 keer, 1 keer voor de prijsgeschiedenis van de SP500 en 1 keer om de verschillende ticker symbolen van de SP500 bedrijven te achterhalen. Voor beide pagina's wordt een get request gebruikt om de downloadknop te zoeken en dan vervolgens is er nog een get requests om de url van de downloadknop op te halen en de CSV data die daar opgehaald wordt in een CSV bestand te schrijven.\n",
    "\n",
    "## Selenium (headless)\n",
    "Wij gebruiken een headless selenium browser om pagina's te scrapen die ofwel een knop hebben waar op geklikt moet worden of dynamisch geladen worden. Om het simpel te zeggen: pagina's waar we niet de juiste informatie kunnen ophalen zonder een actie uit te voeren. \n",
    "\n",
    "We gebruiken headers die moeten zorgen dat wij minder op een bot lijken en meer op een persoon die de website bezoekt via een normale browser.\n",
    "\n",
    "Dit wordt gedaan bij deze bronnen:\n",
    "- **Yahoo finance (hoofdpagina)**<br>\n",
    "Voor de website van yahoo finance gebruiken wij selenium omdat deze dynamisch geladen wordt. Dit houdt in dat wij eerst de hoofdpagina moeten laden en daar helemaal naar beneden moeten scrollen voordat de html alle data bevat van alle artikelen. Ook is het handig aangezien we dan niet met cookies hoeven te werken, we kunnen gewoon op de knop klikken die niet-essentiele cookies weigert en we worden toegelaten tot de website.\n",
    "- **Yahoo finance (artikelen)**<br>\n",
    "Als eenmaal alle urls van de artikelen opgehaald zijn gebruiken wij selenium om deze te laden. Dit doen we omdat als een artikel te lang is moet er op een knop gedrukt worden die de rest van het artikel toont. Wij kijken doormiddel van een try-except of die knop er is en zo ja dan wordt er op geklikt.\n",
    "\n",
    "## yfinance\n",
    "Voor de API gebruiken wij yfinance, dit is een publieke tool die ontwikkeld is om makkelijk met de API van yahoo finance te praten. \n",
    "\n",
    "Wij gebruiken yfinance om de verschillende prijsgeschiedenissen van de bedrijven in de SP500 op te halen. Deze komen uit de database van yahoo finance en worden elke dag dat de beurzen open zijn geupdate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00182b2a",
   "metadata": {},
   "source": [
    "# Webscraping:\n",
    "Voor het webscrapen hebben wij zoals eerder genoemd get requests en een headless selenium browser gebruikt. We zullen hier mee toelichting geven over hoe de webscraping wordt uigevoerd.\n",
    "\n",
    "- **investopedia**<br>\n",
    "Om te beginnen doen wij een get request naar de URL van investopedia (https://www.investopedia.com/markets-news-4427704). Hier maken wij een beautifulsoup object van om vervolgens zoeken wij alle urls op doormiddel van .find_all('a', class_='card'). Dit zijn de a tags die naar de artikelen leiden die wij willen hebben. We zetten de href attribuut en de text van deze tag apart in een dataframe met de naam investopedia_df. <br><br>Dan begint het scrapen van de artikelen. Wij doen get requests naar de url's met de artikelen en gebruiken .find('div', class_='article-content').find_all('p') om alle paragrafen van de artikelen uit het beautifulsoup object te halen die de html bevat. Deze worden samengevoegd tot 1 artikel en toegevoegd aan investopedia_df<br><br>Daarnaast heeft investopedia ook ticker symbolen in de artikelen staan van de bedrijven waar het over gaat. De ticker symbolen zijn allemaal ook a-tags omdat ze leiden naar de investopedia pagina van dat bedrijf. Wij scrapen de symbolen door dit te doen: [a_tag.text for a_tag in p.find_all('a') if re.search('widgetsymbol', a_tag['href'])], zo kunnen we alle text van de a-tags ophalen als de a-tag naar een webpagina leidt met \"widgetsymbol\" in de URL. Dit wordt gebruikt door investopedia voor alle pagina's van bedrijven met ticker symbolen. Ook deze symbolen worden toegevoegd aan investopedia_df<br><br>Als laatst word de investopedia_df gefilterd op artikelen waarvan de symbolen die erin voorkomen ook voorkomen in de SP500.<br><br>\n",
    "\n",
    "- **Yahoo finance**<br>\n",
    "Bij yahoo finance maken we gebruik van selenium, we laden de hoofdpagina, weigeren niet-essentiele cookies en scrollen naar beneden doormiddel van een ActionChains. In deze ActionChains kan je acties zetten die de browser voor je uitvoert. Wij zetten doormiddel van een for-loop 1500 keer in deze ActionChains dat er gescrolled moet worden: .scroll_by_amount(0, 500). Dit zorgt ervoor dat de webpagina altijd helemaal naar beneden gescrolled wordt voordat er gekeken wordt welke artikelen er allemaal zijn.<br><br>Vervolgens worden alle a-tags uit de html gehaald doormiddel van .find_elements(By.TAG_NAME, 'a'). Van deze a-tags worden de text, hrefs en css classes opgeslagen in een dataframe en dit wordt gefilterd op de css class van de a-tags die naar artikelen verwijzen en of de url \"/news/\" bevat. zie code: df[df['url'].str.contains('/news/') & df['classes'].str.contains('js-content-viewer')].<br><br>Nadat we alle urls van de artikelen in een dataframe hebben worden de artikelen gescraped. Dit doen we met een nieuwe selenium browser die eerst weer alle niet-essentiele cookies weigert en dan alle artikelen 1 voor 1 laadt. Op elke pagina wordt er gekeken of er een knop is die artikelen verder toont die ingedrukt moet worden en als die er is wordt het ingedrukt. Dit wordt gedaan in een try-except:.find_element(By.CLASS_NAME, 'collapse-button').click() die probeert op de knop te drukken en als dat niet gaat wordt pass gerunt dus gebeurd er niks. Dit zorgt ervoor dat we alle artikelen volledig kunnen zien. Deze artikelen worden vervolgens gescraped door .find_element(By.CLASS_NAME, 'caas-body').find_elements(By.TAG_NAME, 'p') te runnen om zo de paragrafen van de artikelen te verkrijgen. Dit wordt gecheckt op inhoud en samengevoegd op deze manier: [p.text for p in article_p_elements if p.text!='']\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c8fe54",
   "metadata": {},
   "source": [
    "# API:\n",
    "Voor de API maken wij gebruik van de Yahoo finance API doormiddel van yfinance. yfinance is een tool die het makkelijk maakt om informatie van yahoo finance te krijgen. Deze library doet dit door calls te maken naar yahoo finance waar je normaal gesproken autorisatie voor nodig hebt. calls worden gemaakt naar de REST API van yahoo finance, bijv. https://query2.finance.yahoo.com/v7/finance/options/ticker_symbol_example. De library maakt dit allemaal meer pythonic en makkelijker doordat je met deze library alleen yf.download(symbol_list, group_by=\"ticker\") hoeft te gebruiken. Dan krijg je meteen een dataframe terug met alle data van alle tickers die in je symbol_list staan. Dit is ook hoe wij de data ophalen uit de API. <br><br>Wij gebruiken de eerder vernoemde CSV van datahub.io met alle ticker symbolen in de SP500 en zetten dit in een lijst om zo allemaal te downloaden. De API geeft standaard een dataframe met multiIndex waar de eerste rij van de index de ticker is en de 2e rij de soort informatie van de ticker. Bij de 2e rij staan er voor elke ticker de volgende informatie in het DataFrame: Open, High, Low, Close, adjusted Close en Volume. Dit zijn veelvoorkomende gegevens die aangeven hoe een aandeel zich die dag heeft bewogen en zijn daarom erg belangrijk voor ons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6463da7d",
   "metadata": {},
   "source": [
    "# Opschonen van de data / uitleg text voorbewerking\n",
    "\n",
    "### text data\n",
    "- **Preprocessing**<br>\n",
    "Voor de text data is alle text uit de html gehaald doormiddel van beautifulsoup of selenium, dit is vervolgens schoongemaakt. Dit schoonmaken wordt gedaan door alle speciale tekens eruit te halen en alleen woorden en witte characters over te houden. Dit bereiken wij door regex te gebruiken re.sub(r'[^\\w\\s]', '', text). Deze manier om precies te zijn.<br><br>Als alle text schoon is moet deze worden getokenized zodat ze bruikbaar zijn in de sentiment analyse. Dit tokenizen doen wij met de NLTK library. Ook halen we de stopwoorden eruit om zo te zorgen dat de text gereduceert wordt tot alleen belangrijke dingen. De stopwoorden die wij eruit halen zijn te vinden in nltk.corpus.stopwords.words('english'). Nu we alleen nog maar belangrijke text hebben is het van belang te zorgen dat alle text uniform wordt, dit doen we door lemmatization met NLTK. Lemmatization zorgt ervoor dat de woorden allemaal teruggebracht worden naar een vorm die makkelijk te herkennen en te gebruiken is voor sentiment analyse. <br><br>\n",
    "\n",
    "- **feature engineering**<br>\n",
    "Na het preprocessen van de text wordt deze door de SentimentIntensityAnalyzer gehaald. Deze functie van NLTK geeft sentiment analyse scores terug van de texten. Wij halen de positiviteitsscore en negativiteitsscore hieruit en kijken naar de basisstatistieken van deze scores van alle artikelen die gescraped zijn. Zo hebben we de gemiddelde, standaard deviatie, de kwartielen, het maximum en het minimum van de positiviteit en de negativiteit. Deze zetten we in een dataframe met 1 rij zodat deze toegevoegd kan worden aan de uiteindelijke dataframe bij vandaag.\n",
    "\n",
    "### API data\n",
    "Deze data komt al netjes en schoon in de vorm van een dataframe.\n",
    "\n",
    "### Datahub.io\n",
    "Deze data downloaden wij als CSV en dit zorgt ervoor dat als wij deze CSV vervolgens gebruiken dat dit makkelijk en netjes in een dataframe gezet kan worden.\n",
    "\n",
    "### Uiteindelijke dataframe\n",
    "Na het transformeren en samenvoegen van alle data doen wij nog een aantal preprocessing stappen op het uiteindelijke dataframe om deze bruikbaar te maken voor machine learning.\n",
    "\n",
    "- **preprocessing**<br>\n",
    "Wij vervangen alle NaN waardes met 0 zodat het dataframe alleen nog maar numerieke waardes bevat, dit zodat het in een keer gebruikt kan worden voor machine learning. Ook verwijderen wij de kolom: \"Date\" die overgebleven is van het samenvoegen van de verschillende dataframes tot 1 dataframe. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "88047231",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    \"\"\"\n",
    "    Pipeline to get data relating to the stock vusa that tracks the SP500. Includes sentiment analysis from articles,\n",
    "    price history of stocks that are in the ETF and price history of vusa itself. \n",
    "    The class follows the Extract, Transform, Load functionality.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.HEADERS = {'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "           'Accept-Encoding': 'gzip, deflate, br',\n",
    "           'Accept-Language': 'en-US,en;q=0.9,nl;q=0.8',\n",
    "           'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'}\n",
    "\n",
    "        self.DATA_LOCATION = 'Datasets/'\n",
    "\n",
    "        self.HOMEPAGE_URL = 'https://www.investopedia.com/markets-news-4427704'\n",
    "\n",
    "        self.VUSA_DOWNLOAD_URL = 'https://datahub.io/core/s-and-p-500-companies'\n",
    "        self.SP_DOWNLOAD_URL = 'https://datahub.io/core/s-and-p-500'\n",
    "        self.DATAHUB_URL = 'https://datahub.io'\n",
    "\n",
    "        self.YAHOO_URL = 'https://finance.yahoo.com/'\n",
    "\n",
    "        self.OPTIONS = Options()\n",
    "        self.OPTIONS.add_argument('--headless')\n",
    "        self.OPTIONS.add_argument('--disable-gpu') \n",
    "        \n",
    "        nltk.download(['punkt', 'wordnet', 'stopwords', 'omw-1.4', 'vader_lexicon'], quiet=True)\n",
    "        \n",
    "        \n",
    "    def get_response(self, url, headers):\n",
    "        r = requests.get(url, headers=headers)\n",
    "        soup = bs(r.content)\n",
    "        return soup\n",
    "    \n",
    "    def get_datahub_csv(self, URL, filename):\n",
    "        soup = self.get_response(URL, self.HEADERS)\n",
    "        download_url = soup.find_all('table')[1].find_all('a')[1]['href']\n",
    "        download_url = self.DATAHUB_URL + download_url\n",
    "        with open(self.DATA_LOCATION + filename, 'wb') as f:\n",
    "            f.write(requests.get(download_url).content)\n",
    "    \n",
    "    def get_yahoo_news_articles_df(self):\n",
    "        yahoo_news_driver = webdriver.Chrome(options=self.OPTIONS)\n",
    "\n",
    "        yahoo_news_driver.get(self.YAHOO_URL)\n",
    "        time.sleep(1)\n",
    "        yahoo_news_driver.find_element(By.CLASS_NAME, 'reject-all').click()\n",
    "        actions = ActionChains(yahoo_news_driver)\n",
    "        for i in range(1500):\n",
    "            actions.scroll_by_amount(0, 500)\n",
    "        actions.perform()\n",
    "        actions.reset_actions()\n",
    "\n",
    "        links = self.get_article_links(yahoo_news_driver)\n",
    "        df = pd.DataFrame({'title': [link.text for link in links], \n",
    "                           'url': [link.get_attribute('href') for link in links], \n",
    "                           'classes': [link.get_attribute('class') for link in links]})\n",
    "\n",
    "        yahoo_news_driver.quit()\n",
    "        df = df[df['url'] \\\n",
    "                .str.contains('/news/') & df['classes'].str.contains('js-content-viewer')]\\\n",
    "                .drop_duplicates(subset='url', keep='last')\\\n",
    "                .reset_index(drop=True)\n",
    "        return df\n",
    "    \n",
    "    def scrape_articles(self, urls):\n",
    "        articles = []\n",
    "        raw_articles = []\n",
    "        yahoo_driver = webdriver.Chrome(options=self.OPTIONS)\n",
    "        yahoo_driver.get(self.YAHOO_URL)\n",
    "        yahoo_driver.find_element(By.CLASS_NAME, 'reject-all').click()\n",
    "\n",
    "        for index, url in enumerate(urls):\n",
    "            yahoo_driver.get(url)\n",
    "            try:\n",
    "                yahoo_driver.find_element(By.CLASS_NAME, 'collapse-button').click()\n",
    "            except:\n",
    "                pass\n",
    "            article_p_elements = yahoo_driver.find_element(By.CLASS_NAME, 'caas-body').find_elements(By.TAG_NAME, 'p')\n",
    "            article_by_paragraph = [p.text for p in article_p_elements if p.text!='']\n",
    "\n",
    "            raw_article = [bs(element.get_attribute('outerHTML'), \"html.parser\") for element in article_p_elements]\n",
    "            article = \" \".join(article_by_paragraph)\n",
    "            clean_article = self.clean_text(article)\n",
    "            articles.append(clean_article)\n",
    "            raw_articles.append(raw_article)\n",
    "            self.progress_bar(index+1, len(urls))\n",
    "\n",
    "\n",
    "        yahoo_driver.quit()\n",
    "\n",
    "        return articles, raw_articles\n",
    "    \n",
    "    def download_yahoo_data(self, file_path):\n",
    "        \"\"\"\n",
    "        Downloadt gegevens van Yahoo Finance voor tickers die zijn vermeld in het opgegeven CSV-bestand.\n",
    "\n",
    "        Parameters:\n",
    "        - file_path (str): Bestandspad naar het CSV-bestand met ticker-symbolen.\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame: Gefilterd DataFrame met gedownloade gegevens.\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(file_path)\n",
    "        symbol_list = df.iloc[:, 0].tolist()\n",
    "\n",
    "        download = yf.download(symbol_list, group_by=\"ticker\")\n",
    "        data = download.copy()\n",
    "\n",
    "        filtered_data = data.dropna(axis=1, how='all')\n",
    "        filtered_data.columns = filtered_data.columns.remove_unused_levels()\n",
    "\n",
    "        remaining_tickers = list(filtered_data.columns.levels[0])\n",
    "        missing_tickers = list(set(symbol_list) - set(remaining_tickers))\n",
    "\n",
    "        return filtered_data    \n",
    "    \n",
    "    def get_card_titles(self):\n",
    "        titles = []\n",
    "        a_tags = self.soup.find_all('a', class_='card')\n",
    "        for i in range(len(a_tags)):\n",
    "            titles.append(a_tags[i].span.text)\n",
    "        return titles\n",
    "\n",
    "    def get_card_urls(self):\n",
    "        urls = []\n",
    "        a_tags = self.soup.find_all('a', class_='card')\n",
    "        for i in range(len(a_tags)):\n",
    "            urls.append(a_tags[i]['href'])\n",
    "        return urls\n",
    "    \n",
    "    def get_url_content(self, urls):\n",
    "        articles = []\n",
    "        raw_articles = []\n",
    "        for url in urls:\n",
    "            soup = self.get_response(url, self.HEADERS)\n",
    "            paragraphs = soup.find('div', class_='article-content').find_all('p')\n",
    "            article = ''.join([p.get_text(separator=' ') for p in paragraphs])\n",
    "            raw_articles.append(paragraphs)\n",
    "            articles.append(self.clean_text(article))\n",
    "\n",
    "        return articles, raw_articles\n",
    "    \n",
    "    def get_symbols(self, raw_articles):\n",
    "        symbol_list = []\n",
    "        for p_list in raw_articles:\n",
    "            article_symbols = []\n",
    "            for p in p_list:\n",
    "                symbols = [a_tag.text for a_tag in p.find_all('a') if re.search('widgetsymbol', a_tag['href'])]\n",
    "                if symbols:\n",
    "                    for symbol in symbols:\n",
    "                        article_symbols.append(symbol)\n",
    "            symbol_list.append(article_symbols)\n",
    "        return symbol_list\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        return re.sub(r'[^\\w\\s]', '', text)\n",
    "            \n",
    "    def get_article_links(self, driver):\n",
    "        links = driver.find_elements(By.TAG_NAME, 'a')\n",
    "        return links\n",
    "    \n",
    "    def progress_bar(self, current, total, bar_length=20):\n",
    "        fraction = current / total\n",
    "\n",
    "        arrow = int(fraction * bar_length - 1) * '-' + '>'\n",
    "        padding = int(bar_length - len(arrow)) * ' '\n",
    "\n",
    "        ending = '\\n' if current == total else '\\r'\n",
    "\n",
    "        print(f'Progress: [{arrow}{padding}] {int(fraction*100)}%', end=ending)\n",
    "\n",
    "    def clean_and_tokenize(self, articles):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        tokenized_articles = []\n",
    "        for article in articles:\n",
    "            tokenized_article = nltk.word_tokenize(article)\n",
    "            tokenized_article = [token for token in tokenized_article if token not in stopwords.words('english')]\n",
    "            tokenized_article = [lemmatizer.lemmatize(token) for token in tokenized_article]\n",
    "\n",
    "            tokenized_articles.append(tokenized_article)\n",
    "\n",
    "        return tokenized_articles\n",
    "    \n",
    "    def combine_data(self, api_df, investopedia_df, yahoo_df, time_data):\n",
    "        df1 = investopedia_df.drop(['symbols'], axis=1)\n",
    "        df2 = yahoo_df.drop('classes', axis=1)\n",
    "        text_df = pd.concat([df1, df2])\n",
    "        text_df = text_df.reset_index(drop=True)\n",
    "        \n",
    "        # tokenize the articles and add them to a DataFrame\n",
    "        text_df['tokenized_article'] = self.clean_and_tokenize(text_df['article'])\n",
    "        \n",
    "        # apply sentiment analysis and add it to the DataFrame\n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "        text_df['positivity_score'] = [analyzer.polarity_scores(article)['pos'] for article in text_df['article']]\n",
    "        text_df['negativity_score'] = [analyzer.polarity_scores(article)['neg'] for article in text_df['article']]\n",
    "        \n",
    "        # get statistics from positive and negative sentiment analysis\n",
    "        pos = text_df.describe().T[:1].reset_index(drop=True)\n",
    "        neg = text_df.describe().T[1:].reset_index(drop=True)\n",
    "        \n",
    "        # rename columns in pos and neg so they can be concattinated\n",
    "        pos.columns = ['pos_' + col for col in pos.columns]\n",
    "        neg.columns = ['neg_' + col for col in neg.columns]\n",
    "        \n",
    "        # concattinate the positve and negative sentiment analysis statistics to a 1 row DataFrame\n",
    "        pos_neg_row = pd.concat([pos, neg], axis=1)\n",
    "        \n",
    "        api_df = api_df.iloc[::-1].reset_index(drop=False, names='Date')\n",
    "        \n",
    "        api_df.columns = ['_'.join(col).strip() for col in api_df.columns.values]\n",
    "        \n",
    "        combined_df = pd.concat([api_df, pos_neg_row], axis=1)\n",
    "        \n",
    "        combined_df['Date_'] = combined_df['Date_'].astype(str)\n",
    "        time_data['Date'] = time_data['Date'].astype(str)\n",
    "\n",
    "        final_df = combined_df.merge(time_data, how='left', left_on='Date_', right_on='Date')\n",
    "        \n",
    "        return final_df\n",
    "\n",
    "    def extract(self):\n",
    "        # get site data\n",
    "        self.soup = self.get_response(self.HOMEPAGE_URL, self.HEADERS)\n",
    "\n",
    "        # download csv data\n",
    "#         self.get_datahub_csv(self.VUSA_DOWNLOAD_URL, 'vusa_holdings.csv')\n",
    "#         self.get_datahub_csv(self.SP_DOWNLOAD_URL, 'time_data.csv')\n",
    "       \n",
    "        # get csv data\n",
    "        self.vusa_df = pd.read_csv(self.DATA_LOCATION + 'vusa_holdings.csv')\n",
    "        self.time_data_df = pd.read_csv(self.DATA_LOCATION + 'time_data.csv')\n",
    "\n",
    "        # get yahoo articles with titles, urls and css classes\n",
    "        self.yahoo_news_df = self.get_yahoo_news_articles_df()\n",
    "        self.yahoo_news_df = self.yahoo_news_df[:2]\n",
    "    \n",
    "        # scrape news articles from yahoo article urls\n",
    "        self.yahoo_news_articles, self.raw_yahoo_news_articles = self.scrape_articles(self.yahoo_news_df['url'])\n",
    "        \n",
    "        # get api data\n",
    "        self.api_df = self.download_yahoo_data('Datasets/vusa_holdings.csv')\n",
    "    \n",
    "    \n",
    "    def transform(self):        \n",
    "        # filter site data\n",
    "        titles = self.get_card_titles()\n",
    "        urls = self.get_card_urls()\n",
    "        \n",
    "        # transform filtered data to DataFrame(investopedia_df)\n",
    "        investopedia_df = pd.DataFrame({'url': urls, 'title': titles})\n",
    "        \n",
    "        # get articles from urls\n",
    "        articles, raw_articles = self.get_url_content(investopedia_df['url'])\n",
    "        \n",
    "        # filter symbols from the vusa_df csv data\n",
    "        symbols_in_vusa = self.vusa_df['Symbol'].tolist()\n",
    "\n",
    "        # add scraped articles to DataFrame\n",
    "        self.yahoo_news_df['article'] = self.yahoo_news_articles\n",
    " \n",
    "        # get symbols from article\n",
    "        symbols = self.get_symbols(raw_articles)\n",
    "        \n",
    "        # add articles and symbols to DataFrame(investopedia_df)\n",
    "        investopedia_df['article'] = articles\n",
    "        investopedia_df['symbols'] = symbols\n",
    "        \n",
    "        # filter DataFrame(investopedia_df) to only have symbols that occur in the symbols_in_vusa\n",
    "        investopedia_df = investopedia_df[investopedia_df['symbols'].apply(lambda symbols: any(symbol in symbols_in_vusa for symbol in symbols))]\n",
    "        \n",
    "        # combine DataFrames\n",
    "        self.df = self.combine_data(self.api_df, investopedia_df, self.yahoo_news_df, self.time_data_df)\n",
    "        self.df = self.df.fillna(0)\n",
    "        self.df = self.df.drop('Date', axis=1)\n",
    "          \n",
    "    def load(self, filename):\n",
    "        self.df.to_csv(filename)\n",
    "    \n",
    "    def ETL(self, filename):\n",
    "        self.extract()\n",
    "        self.transform()\n",
    "        self.load(filename)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be808ad6",
   "metadata": {},
   "source": [
    "# Het volledige proces\n",
    "Er is al veel uitgelegd in vorige beschrijvingen, toch vinden wij het belangrijk om alles nog 1 keer van begin tot eind te beschrijven zodat alle stappen duidelijk zijn die gevolgd worden.\n",
    "\n",
    "## ETL\n",
    "Wij maken in dit project gebruik van ETL (Extract, transform, load). Hiervoor hebben wij ook een ETL methode aangemaakt onderaan de class, hier worden de Extract, transform en load methodes aangeroepen die vervolgens de stappen uivoeren die nodig zijn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f73cc79",
   "metadata": {},
   "source": [
    "\n",
    "## Extract\n",
    "In de Extract methode halen wij alle ruwe data uit de beschikbare bronnen. \n",
    "\n",
    "Het begint met het aanroepen van de get_response methode met de URL voor investopedia (https://www.investopedia.com/markets-news-4427704), get_response doet hier een get request en zet de html om in een beautifulsoup object.\n",
    "\n",
    "vervolgens werden de csv bestanden gedownload van datahub.io maar aangezien dit niet meer beschikbaar is, is dit uigecomment. En in plaats hiervan worden de gedownloade bestanden ingelezen als dataframes en in de variabelen: vusa_df en time_data_df gezet.\n",
    "\n",
    "Dan wordt de yahoo finance website gescraped doormiddel van de get_yahoo_news_articles_df methode, deze methode weigert de niet-essentiele cookies en scrollt dan door de yahoo_finance website(https://finance.yahoo.com/) om zo alle URLs op te halen van de artikelen die op de website staan. Dit doen we doormiddel van een headless selenium browser, aangezien de website dynamisch geladen word en zonder scrollen niet alle URLs opgehaald kunnen worden. Deze URLs worden in een DataFrame gezet met de titels van de artikelen en css klassen van de artikelen op de website. We halen de artikelen en css klassen ook op om duidelijk te kunnen zien of alles goed is gegaan en problemen makkelijk op te lossen als het niet zou gaan zoals wij verwachten.\n",
    "\n",
    "Dan hebben we alle URLs van de artikelen maar de artikelen zelf hebben we nog niet, dus dat is het volgende wat wij doen. \n",
    "Dit doen we met de methode scrape_articles waar wij de URLs als parameter meegeven. Deze methode klikt eerst op het weigeren van niet-essentiele cookies en laadt vervolgens 1 voor 1 de verschillende URLs. Sommige van de artikelen zijn groter en yahoo finance laadt dan niet het gehele artikel zien, hiervoor moet eerst op een knop gedrukt worden met de class \n",
    "'collapse-button'. Wij maken gebruik van een try, except om deze knop in te drukken wat zorgt voor robuustheid zodat alle artikelen goed geladen kunnen worden. vervolgens worden de artikelen opgehaald en schoongemaakt doormiddel van de regex functie die in de methode clean_text gebruikt wordt. scrape_articles geeft ook de rauwe html van de artikelen terug om zo makkelijk te kunnen zien of alles volgens verwachting is verlopen en fouten makkelijk op te lossen.\n",
    "\n",
    "Als laatst word yfinance API gebruikt om de prijsgeschiedenis te downloaden, dit wordt gedaan in de download_yahoo_data methode. In deze methode wordt ook de locatie van het bestand met de tickers van de bedrijven in de SP500 meegegeven zodat de methode dit kan gebruiken om te weten welke prijsdata gedownload moet worden. In dit bestand staan de tickers van de bedrijven in de SP500 en dit wordt omgezet naar een lijst en gebruikt voor de finance API om te weten wat te downloaden. Dit wordt gedownload en automatisch in een DataFrame gezet. Ook geeft yfinance automatisch aan welke downloads niet gelukt zijn, dit is goed voor de robuustheid aangezien we dan makkelijk kunnen zien waar het wellicht misgaat. Wij hebben een aantal downloads die sowieso mislukken, dit zijn downloads van bedrijven die niet meer verhandelt worden. Een goed voorbeeld hiervan is facebook, facebook wordt niet meer verhandelt aangezien het bedrijf is veranderd naar meta.\n",
    "\n",
    "Ook worden de symbolen uit vusa_df gefilterd en deze worden in lijst symbols_in_vusa gezet. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa22c73",
   "metadata": {},
   "source": [
    "## Transform\n",
    "Nadat alle ruwe data is geëxtraheerd gaan we de data transformeren zodat het bruikbaar is voor machine learning. We voegen alle data samen, schonen dit op en vervangen lege waardes. \n",
    "\n",
    "We beginnen met het beautifulsoup object van de webpagina van investopedia(https://www.investopedia.com/markets-news-4427704). Met de methodes get_card_titles en get_card_urls halen wij de titels en URLs van de artikelen op vanuit de webpagina. De webpagina van investopedia is niet dynamisch geladen dus kunnen wij alle artikelen uit het beautifulsoup object halen. De titels en URLs worden beide opgehaald om makkelijk te kunnen checken of alle artikelen meegenomen zijn al worden er opmerkelijkheden gespot. Deze titels en URLs worden in een dataframe gezet met de naam investopedia_df. Nu we de URLs hebben gebruiken we de functie get_url_content om de artikelen van de website te scrapen doormiddel van beautifulsoup.\n",
    "De artikelen worden teruggegeven als ruwe artikelen en schone artikelen die schoongemaakt zijn door de clean_text methode.\n",
    "\n",
    "De ruwe artikelen worden ook gebruikt om de symbolen van de bedrijven te filteren die genoemd worden in het artikel, dit wordt gedaan met de get_symbols methode. Alle artikelen en symbolen worden toegevoegd aan de investopedia_df zodat daar alles instaat van investopedia. vervolgens word de DataFrame gefilterd op de artikelen die symbolen bevat die ook in symbols_in_vusa voorkomt.\n",
    "\n",
    "Daarna worden de artikelen van yahoo finance die eerder opgehaald zijn bij het extraheren toegevoegd aan de yahoo_news_df. \n",
    "\n",
    "Nu hebben we alle data in 4 verschillende DataFrames staan:\n",
    "- api_df: Hier staat de gedownloade prijsdata van de bedrijven in die voorkomen in de SP500\n",
    "- investopedia_df: Hier staan de artikelen van investopedia in, gefilterd op of de bedrijven die genoemt zijn in de artikelen voorkomen in de bedrijven van de SP500\n",
    "- yahoo_news_df: Hier staan de artikelen in van de yahoo finance website\n",
    "- time_data_df: Hier staat de prijsgeschiedenis van de SP500 zelf in.\n",
    "\n",
    "Deze 4 Dataframes worden samengevoegd in de combine_data methode, in deze methode wordt de laatste processing gedaan en alles samengevoegd.\n",
    "\n",
    "De combine_data methode worden eerst de investopedia_df en yahoo_news_df gefilterd en samengevoegt tot 1 text dataframe genaamd text_df. De artikelen uit de text_df worden schoongemaakt, getokenized, gefilterd en gelemmatized in de clean_and_tokenize methode en toegevoegd als nieuwe kolom aan de text_df. Dan wordt doormiddel van de SentimentIntensityAnalyzer de positieve en negatieve sentiment uit de artikelen gehaald en deze worden apart toegevoegd als kolommen in de text_df. Vervolgens moeten we de data zo transformeren dat het allemaal numeriek wordt en klaar om toegevoegd aan de uiteindelijke dataframe. Dit doen we door de basisstatistieken van de positieve en negatieve sentimentscores op te halen en samen te voegen tot een DataFrame van 1 rij. Nu hebben wij een DataFrame met de statistieken van de sentimentscores van vandaag.\n",
    "\n",
    "Vervolgens voegen wij de statistieken van de sentimentscores van de text toe bij vandaag in de api_df. Deze wordt dan vervolgens gemerged met de time_data op de key met de datums. Zo hebben we dus nu de prijsgeschiedenis van alle bedrijven in de SP500, de prijsgeschiedenis van de SP500 zelf en de statistieken van de sentimentscores van vandaag. Als dit elke dag gerunt word dan worden langzaam steeds meer rijen met statistieken van sentimentscores van alle dagen toegevoegd.\n",
    "\n",
    "Als laatst worden de missende waardes vervangen met fillna zodat het bruikbaar wordt voor machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c7fcb0",
   "metadata": {},
   "source": [
    "## Load\n",
    "In de load functie word er voor gezorgd dat de data makkelijk opgehaald en gebruikt kan worden voor machine learning.\n",
    "\n",
    "Wij doen dit door de DataFrame op te slaan in een CSV bestand wat later makkelijk gemimporteerd en gebruikt kan worden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e3225b65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "16129433",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [------------------->] 100%\n",
      "[*********************100%%**********************]  505 of 505 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "27 Failed downloads:\n",
      "['FRC', 'TWTR', 'RE', 'BRK.B', 'CERN', 'INFO', 'DRE', 'ANTM', 'DISCA', 'CTXS', 'ATVI', 'XLNX', 'WLTW', 'NLSN', 'FISV', 'ABC', 'FBHS', 'KSU', 'BLL', 'DISCK', 'PKI', 'SIVB', 'PBCT', 'NLOK', 'FB', 'VIAC']: Exception('%ticker%: No timezone found, symbol may be delisted')\n",
      "['BF.B']: Exception('%ticker%: No price data found, symbol may be delisted (1d 1925-02-26 -> 2024-02-03)')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline.ETL('Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ad26226c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date_</th>\n",
       "      <th>HCA_Open</th>\n",
       "      <th>HCA_High</th>\n",
       "      <th>HCA_Low</th>\n",
       "      <th>HCA_Close</th>\n",
       "      <th>HCA_Adj Close</th>\n",
       "      <th>HCA_Volume</th>\n",
       "      <th>BRO_Open</th>\n",
       "      <th>BRO_High</th>\n",
       "      <th>BRO_Low</th>\n",
       "      <th>...</th>\n",
       "      <th>neg_max</th>\n",
       "      <th>SP500</th>\n",
       "      <th>Dividend</th>\n",
       "      <th>Earnings</th>\n",
       "      <th>Consumer Price Index</th>\n",
       "      <th>Long Interest Rate</th>\n",
       "      <th>Real Price</th>\n",
       "      <th>Real Dividend</th>\n",
       "      <th>Real Earnings</th>\n",
       "      <th>PE10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-02-02</td>\n",
       "      <td>313.000000</td>\n",
       "      <td>313.000000</td>\n",
       "      <td>307.149994</td>\n",
       "      <td>309.420013</td>\n",
       "      <td>309.420013</td>\n",
       "      <td>1280100.0</td>\n",
       "      <td>77.860001</td>\n",
       "      <td>78.410004</td>\n",
       "      <td>77.650002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-02-01</td>\n",
       "      <td>303.350006</td>\n",
       "      <td>314.820007</td>\n",
       "      <td>303.350006</td>\n",
       "      <td>314.660004</td>\n",
       "      <td>314.660004</td>\n",
       "      <td>1739800.0</td>\n",
       "      <td>77.110001</td>\n",
       "      <td>77.879997</td>\n",
       "      <td>75.790001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-31</td>\n",
       "      <td>303.420013</td>\n",
       "      <td>309.329987</td>\n",
       "      <td>302.390015</td>\n",
       "      <td>304.899994</td>\n",
       "      <td>304.899994</td>\n",
       "      <td>2555800.0</td>\n",
       "      <td>78.540001</td>\n",
       "      <td>78.769997</td>\n",
       "      <td>77.480003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-30</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>304.950012</td>\n",
       "      <td>296.119995</td>\n",
       "      <td>301.589996</td>\n",
       "      <td>301.589996</td>\n",
       "      <td>2483200.0</td>\n",
       "      <td>77.199997</td>\n",
       "      <td>78.330002</td>\n",
       "      <td>77.160004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-29</td>\n",
       "      <td>282.420013</td>\n",
       "      <td>287.190002</td>\n",
       "      <td>281.609985</td>\n",
       "      <td>286.730011</td>\n",
       "      <td>286.730011</td>\n",
       "      <td>1742800.0</td>\n",
       "      <td>76.940002</td>\n",
       "      <td>77.209999</td>\n",
       "      <td>76.169998</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15624</th>\n",
       "      <td>1962-01-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15625</th>\n",
       "      <td>1962-01-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15626</th>\n",
       "      <td>1962-01-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15627</th>\n",
       "      <td>1962-01-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15628</th>\n",
       "      <td>1962-01-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15629 rows × 2894 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date_    HCA_Open    HCA_High     HCA_Low   HCA_Close  \\\n",
       "0      2024-02-02  313.000000  313.000000  307.149994  309.420013   \n",
       "1      2024-02-01  303.350006  314.820007  303.350006  314.660004   \n",
       "2      2024-01-31  303.420013  309.329987  302.390015  304.899994   \n",
       "3      2024-01-30  300.000000  304.950012  296.119995  301.589996   \n",
       "4      2024-01-29  282.420013  287.190002  281.609985  286.730011   \n",
       "...           ...         ...         ...         ...         ...   \n",
       "15624  1962-01-08    0.000000    0.000000    0.000000    0.000000   \n",
       "15625  1962-01-05    0.000000    0.000000    0.000000    0.000000   \n",
       "15626  1962-01-04    0.000000    0.000000    0.000000    0.000000   \n",
       "15627  1962-01-03    0.000000    0.000000    0.000000    0.000000   \n",
       "15628  1962-01-02    0.000000    0.000000    0.000000    0.000000   \n",
       "\n",
       "       HCA_Adj Close  HCA_Volume   BRO_Open   BRO_High    BRO_Low  ...  \\\n",
       "0         309.420013   1280100.0  77.860001  78.410004  77.650002  ...   \n",
       "1         314.660004   1739800.0  77.110001  77.879997  75.790001  ...   \n",
       "2         304.899994   2555800.0  78.540001  78.769997  77.480003  ...   \n",
       "3         301.589996   2483200.0  77.199997  78.330002  77.160004  ...   \n",
       "4         286.730011   1742800.0  76.940002  77.209999  76.169998  ...   \n",
       "...              ...         ...        ...        ...        ...  ...   \n",
       "15624       0.000000         0.0   0.000000   0.000000   0.000000  ...   \n",
       "15625       0.000000         0.0   0.000000   0.000000   0.000000  ...   \n",
       "15626       0.000000         0.0   0.000000   0.000000   0.000000  ...   \n",
       "15627       0.000000         0.0   0.000000   0.000000   0.000000  ...   \n",
       "15628       0.000000         0.0   0.000000   0.000000   0.000000  ...   \n",
       "\n",
       "       neg_max  SP500  Dividend  Earnings  Consumer Price Index  \\\n",
       "0         0.16    0.0       0.0       0.0                   0.0   \n",
       "1         0.00    0.0       0.0       0.0                   0.0   \n",
       "2         0.00    0.0       0.0       0.0                   0.0   \n",
       "3         0.00    0.0       0.0       0.0                   0.0   \n",
       "4         0.00    0.0       0.0       0.0                   0.0   \n",
       "...        ...    ...       ...       ...                   ...   \n",
       "15624     0.00    0.0       0.0       0.0                   0.0   \n",
       "15625     0.00    0.0       0.0       0.0                   0.0   \n",
       "15626     0.00    0.0       0.0       0.0                   0.0   \n",
       "15627     0.00    0.0       0.0       0.0                   0.0   \n",
       "15628     0.00    0.0       0.0       0.0                   0.0   \n",
       "\n",
       "       Long Interest Rate  Real Price  Real Dividend  Real Earnings  PE10  \n",
       "0                     0.0         0.0            0.0            0.0   0.0  \n",
       "1                     0.0         0.0            0.0            0.0   0.0  \n",
       "2                     0.0         0.0            0.0            0.0   0.0  \n",
       "3                     0.0         0.0            0.0            0.0   0.0  \n",
       "4                     0.0         0.0            0.0            0.0   0.0  \n",
       "...                   ...         ...            ...            ...   ...  \n",
       "15624                 0.0         0.0            0.0            0.0   0.0  \n",
       "15625                 0.0         0.0            0.0            0.0   0.0  \n",
       "15626                 0.0         0.0            0.0            0.0   0.0  \n",
       "15627                 0.0         0.0            0.0            0.0   0.0  \n",
       "15628                 0.0         0.0            0.0            0.0   0.0  \n",
       "\n",
       "[15629 rows x 2894 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d87208",
   "metadata": {},
   "source": [
    "# Referentielijst\n",
    "- Fondsbeheerder|NOT,Disclosed|Vanguard S&P 500 UCITS ETF (EUR)|ISIN:IE00B3XXRP09. (n.d.). https://www.morningstar.nl/nl/etf/snapshot/snapshot.aspx?id=0P0000YXKB&tab=4&InvestmentType=FE\n",
    "- Wat zijn ETF’s | Educatie | BlackRock. (n.d.). BlackRock. https://www.blackrock.com/be/individual/nl/educatie/etfs-uitgelegd#Wat-zijn-ETF'\n",
    "- S&P 500 | Wat is de S&P 500? | Beleggingswiki - Semmie.nl. (n.d.). Semmie. https://semmie.nl/wiki/sp-500/#:~:text=De%20S%26P%20500%20is%20een,ontwikkeling%20van%20de%20financi%C3%ABle%20markt.\n",
    "- SWOCC. (2020, June 8). De invloed van mediaberichtgeving op beurskoersen - SWOCC. https://www.swocc.nl/kennisbank-item/de-invloed-van-mediaberichtgeving-op-beurskoersen/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
