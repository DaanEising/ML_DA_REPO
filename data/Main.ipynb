{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a96dba24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "#!pip install jdc\n",
    "%reset\n",
    "import jdc\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "import nltk\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57d341c",
   "metadata": {},
   "source": [
    "# Probleemomschrijving\n",
    "Het doel is om een pipeline te creeën die data ophaalt, transformeert en laad om vervolgens gebruikt te kunnen worden om de prijs van de ETF VUSA te voorspellen. Hierbij gaan wij artikelen van investopedia en yahoo finance scrapen en hier sentiment analysis op toepassen. Ook halen wij de historische prijsdata op van de verschillende aandelen die zich in de ETF bevinden. En als laatst halen wij de historische prijsdata op van VUSA zelf op en voegen wij alles samen tot 1 DataFrame. Deze pipeline kan elke dag gerund worden om zo nieuwe artikelen op te halen en deze toe te voegen aan de dataset, zo krijg je een steeds beter bruikbare dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bdc67b",
   "metadata": {},
   "source": [
    "# Domeinonderzoek\n",
    "Vusa (IE00B3XXRP09) is een ETF die beheert wordt door Vanguard group (Fondsbeheerder|NOT,Disclosed|Vanguard S&P 500 UCITS ETF (EUR)|ISIN:IE00B3XXRP09, n.d.). Een ETF is een aandeel wat eigenlijk bestaat uit meerdere aandelen, obligaties of andere effecten die op de markt verhandelt worden (Wat Zijn ETF’s | Educatie | BlackRock, n.d.). VUSA specifiek volgt de marktindex van de SP500, dat zijn aandelen van de 500 grootste bedrijven van de Verenigde Staten. \n",
    "\n",
    "De SP500 geeft een goed beeld van de amerikaanse markt, aangezien het veel verschillende sectoren en aandelen van grote bedrijven binnen deze sectoren bevat. SP500 heeft daarom een unieke ligging in de aandelenmarkt, omdat het zo groot kunnen er duidelijke veranderingen in de amerikaanse markt worden gemeten naar aanleiding van de SP500. Omdat de SP500 zo'n goed beeld geeft van de amerikaanse markt worden veel beleggingen hiermee vergeleken om te kijken hoe goed het gaat. (S&P 500 | Wat Is De S&P 500? | Beleggingswiki - Semmie.nl, n.d.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebffe97",
   "metadata": {},
   "source": [
    "# Vereiste informatie, databronnen en formaten ruwe data\n",
    "Wij willen willen artikelen scrapen om daar sentiment analysis op toe te passen. Wij zijn van mening dat dit belangrijke data is en deze bron kan dat bevestigen: (SWOCC, 2020). De rauwe data komt in de vorm van text voor yahoo finance en een beautifulsoup object voor investopedia.\n",
    "- https://www.investopedia.com/markets-news-4427704\n",
    "- https://finance.yahoo.com/\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Ook willen wij gebruik maken van de prijsgeschiedenis van de aandelen die te vinden zijn in de VUSA ETF, de API geeft deze data als dataframe terug. Wij hebben de API hier gevonden:\n",
    "- https://github.com/ranaroussi/yfinance\n",
    "\n",
    "Dit hebben wij vervolgens via pip geinstalleerd en geimporteerd via deze code: \"import yfinance as yf\" die aan het begin van het bestand wordt gebruikt.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Als laatst willen wij de prijsgeschiedenis gebruiken van de VUSA zelf, hier hebben wij een publieke dataset voor gedownload van:\n",
    "- https://datahub.io/core/s-and-p-500\n",
    "\n",
    "Echter is de website veranderd en is deze dataset niet meer beschikbaar in deze vorm. Hieronder is een link die laat zien hoe de website er voorheen uitzag:\n",
    "\n",
    "- https://web.archive.org/web/20230402111356/https://datahub.io/core/s-and-p-500\n",
    "\n",
    "De data kwam in een csv bestand en aangezien wij dit destijds hadden gedownload gebruiken wij nu het gedownloade bestand in de pipeline.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Om de juiste data uit de yfinance API te halen en de artikelen van investopedia te filteren hebben wij gebruik gemaakt van een csv bestand wat wij ook gedownload hebben van datahub, aangezien de gehele website is veranderd is ook deze URL niet meer bruikbaar en gebruiken wij de voorheen gedownloade CSV. De originele URL is:\n",
    "\n",
    "- https://datahub.io/core/s-and-p-500-companies\n",
    "\n",
    "En om te zien hoe de website eruit zag kan je deze link volgen:\n",
    "- https://web.archive.org/web/20230601125536/https://datahub.io/core/s-and-p-500-companies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be808ad6",
   "metadata": {},
   "source": [
    "# Het proces\n",
    "In deze cell zullen wij uitleggen en beschrijven wat er allemaal in de class Pipeline gebeurt. We zullen de methodes uitleggen en hoe wij de de verschillende nodige technieken hebben toegepast.\n",
    "\n",
    "## ETL\n",
    "Wij maken in dit project gebruik van ETL (Extract, transform, load). Hiervoor hebben wij ook een ETL methode aangemaakt onderaan de class, hier worden de Extract, transform en load methodes aangeroepen die vervolgens de stappen uivoeren die nodig zijn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f73cc79",
   "metadata": {},
   "source": [
    "\n",
    "## Extract\n",
    "In de Extract methode halen wij alle ruwe data uit de beschikbare bronnen. \n",
    "\n",
    "Het begint met het aanroepen van de get_response methode met de URL voor investopedia (https://www.investopedia.com/markets-news-4427704), get_response doet hier een get request en zet de html om in een beautifulsoup object.\n",
    "\n",
    "vervolgens werden de csv bestanden gedownload van datahub.io maar aangezien dit niet meer beschikbaar is, is dit uigecomment. En in plaats hiervan worden de gedownloade bestanden ingelezen als dataframes en in de variabelen: vusa_df en time_data_df gezet.\n",
    "\n",
    "Dan wordt de yahoo finance website gescraped doormiddel van de get_yahoo_news_articles_df methode, deze methode weigert de niet-essentiele cookies en scrollt dan door de yahoo_finance website(https://finance.yahoo.com/) om zo alle URLs op te halen van de artikelen die op de website staan. Dit doen we doormiddel van een headless selenium browser, aangezien de website dynamisch geladen word en zonder scrollen niet alle URLs opgehaald kunnen worden. Deze URLs worden in een DataFrame gezet met de titels van de artikelen en css klassen van de artikelen op de website. We halen de artikelen en css klassen ook op om duidelijk te kunnen zien of alles goed is gegaan en problemen makkelijk op te lossen als het niet zou gaan zoals wij verwachten.\n",
    "\n",
    "Dan hebben we alle URLs van de artikelen maar de artikelen zelf hebben we nog niet, dus dat is het volgende wat wij doen. \n",
    "Dit doen we met de methode scrape_articles waar wij de URLs als parameter meegeven. Deze methode klikt eerst op het weigeren van niet-essentiele cookies en laadt vervolgens 1 voor 1 de verschillende URLs. Sommige van de artikelen zijn groter en yahoo finance laadt dan niet het gehele artikel zien, hiervoor moet eerst op een knop gedrukt worden met de class \n",
    "'collapse-button'. Wij maken gebruik van een try, except om deze knop in te drukken wat zorgt voor robuustheid zodat alle artikelen goed geladen kunnen worden. vervolgens worden de artikelen opgehaald en schoongemaakt doormiddel van de regex functie die in de methode clean_text gebruikt wordt. scrape_articles geeft ook de rauwe html van de artikelen terug om zo makkelijk te kunnen zien of alles volgens verwachting is verlopen en fouten makkelijk op te lossen.\n",
    "\n",
    "Als laatst word yfinance API gebruikt om de prijsgeschiedenis te downloaden, dit wordt gedaan in de download_yahoo_data methode. In deze methode wordt ook de locatie van het bestand met de tickers van de bedrijven in de SP500 meegegeven zodat de methode dit kan gebruiken om te weten welke prijsdata gedownload moet worden. In dit bestand staan de tickers van de bedrijven in de SP500 en dit wordt omgezet naar een lijst en gebruikt voor de finance API om te weten wat te downloaden. Dit wordt gedownload en automatisch in een DataFrame gezet. Ook geeft yfinance automatisch aan welke downloads niet gelukt zijn, dit is goed voor de robuustheid aangezien we dan makkelijk kunnen zien waar het wellicht misgaat. Wij hebben een aantal downloads die sowieso mislukken, dit zijn downloads van bedrijven die niet meer verhandelt worden. Een goed voorbeeld hiervan is facebook, facebook wordt niet meer verhandelt aangezien het bedrijf is veranderd naar meta.\n",
    "\n",
    "Ook worden de symbolen uit vusa_df gefilterd en deze worden in lijst symbols_in_vusa gezet. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa22c73",
   "metadata": {},
   "source": [
    "## Transform\n",
    "Nadat alle ruwe data is geëxtraheerd gaan we de data transformeren zodat het bruikbaar is voor machine learning. We voegen alle data samen, schonen dit op en vervangen lege waardes. \n",
    "\n",
    "We beginnen met het beautifulsoup object van de webpagina van investopedia(https://www.investopedia.com/markets-news-4427704). Met de methodes get_card_titles en get_card_urls halen wij de titels en URLs van de artikelen op vanuit de webpagina. De webpagina van investopedia is niet dynamisch geladen dus kunnen wij alle artikelen uit het beautifulsoup object halen. De titels en URLs worden beide opgehaald om makkelijk te kunnen checken of alle artikelen meegenomen zijn al worden er opmerkelijkheden gespot. Deze titels en URLs worden in een dataframe gezet met de naam investopedia_df. Nu we de URLs hebben gebruiken we de functie get_url_content om de artikelen van de website te scrapen doormiddel van beautifulsoup.\n",
    "De artikelen worden teruggegeven als ruwe artikelen en schone artikelen die schoongemaakt zijn door de clean_text methode.\n",
    "\n",
    "De ruwe artikelen worden ook gebruikt om de symbolen van de bedrijven te filteren die genoemd worden in het artikel, dit wordt gedaan met de get_symbols methode. Alle artikelen en symbolen worden toegevoegd aan de investopedia_df zodat daar alles instaat van investopedia. vervolgens word de DataFrame gefilterd op de artikelen die symbolen bevat die ook in symbols_in_vusa voorkomt.\n",
    "\n",
    "Daarna worden de artikelen van yahoo finance die eerder opgehaald zijn bij het extraheren toegevoegd aan de yahoo_news_df. \n",
    "\n",
    "Nu hebben we alle data in 4 verschillende DataFrames staan:\n",
    "- api_df: Hier staat de gedownloade prijsdata van de bedrijven in die voorkomen in de SP500\n",
    "- investopedia_df: Hier staan de artikelen van investopedia in, gefilterd op of de bedrijven die genoemt zijn in de artikelen voorkomen in de bedrijven van de SP500\n",
    "- yahoo_news_df: Hier staan de artikelen in van de yahoo finance website\n",
    "- time_data_df: Hier staat de prijsgeschiedenis van de SP500 zelf in.\n",
    "\n",
    "Deze 4 Dataframes worden samengevoegd in de combine_data methode, in deze methode wordt de laatste processing gedaan en alles samengevoegd.\n",
    "\n",
    "De combine_data methode worden eerst de investopedia_df en yahoo_news_df gefilterd en samengevoegt tot 1 text dataframe genaamd text_df. De artikelen uit de text_df worden schoongemaakt, getokenized, gefilterd en gelemmatized in de clean_and_tokenize methode en toegevoegd als nieuwe kolom aan de text_df. Dan wordt doormiddel van de SentimentIntensityAnalyzer de positieve en negatieve sentiment uit de artikelen gehaald en deze worden apart toegevoegd als kolommen in de text_df. Vervolgens moeten we de data zo transformeren dat het allemaal numeriek wordt en klaar om toegevoegd aan de uiteindelijke dataframe. Dit doen we door de basisstatistieken van de positieve en negatieve sentimentscores op te halen en samen te voegen tot een DataFrame van 1 rij. Nu hebben wij een DataFrame met de statistieken van de sentimentscores van vandaag.\n",
    "\n",
    "Vervolgens voegen wij de statistieken van de sentimentscores van de text toe bij vandaag in de api_df. Deze wordt dan vervolgens gemerged met de time_data op de key met de datums. Zo hebben we dus nu de prijsgeschiedenis van alle bedrijven in de SP500, de prijsgeschiedenis van de SP500 zelf en de statistieken van de sentimentscores van vandaag. Als dit elke dag gerunt word dan worden langzaam steeds meer rijen met statistieken van sentimentscores van alle dagen toegevoegd.\n",
    "\n",
    "Als laatst worden de missende waardes vervangen met fillna zodat het bruikbaar wordt voor machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c7fcb0",
   "metadata": {},
   "source": [
    "## Load\n",
    "In de load functie word er voor gezorgd dat de data makkelijk opgehaald en gebruikt kan worden voor machine learning.\n",
    "\n",
    "Wij doen dit door de DataFrame op te slaan in een CSV bestand wat later makkelijk gemimporteerd en gebruikt kan worden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ec52fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LOCATION = 'Datasets/'\n",
    "\n",
    "HEADERS = {'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "           'Accept-Encoding': 'gzip, deflate, br',\n",
    "           'Accept-Language': 'en-US,en;q=0.9,nl;q=0.8',\n",
    "           'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'}\n",
    "\n",
    "\n",
    "URLS = ['https://www.investopedia.com/markets-news-4427704', 'https://finance.yahoo.com/', \n",
    "        'https://datahub.io', 'https://datahub.io/core/s-and-p-500', 'https://datahub.io/core/s-and-p-500-companies']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e2d94b",
   "metadata": {},
   "source": [
    "# Set up Pipeline class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8723535",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    \"\"\"\n",
    "    Pipeline to get data relating to the stock vusa that tracks the SP500. Includes sentiment analysis from articles,\n",
    "    price history of stocks that are in the ETF and price history of vusa itself. \n",
    "    The class follows the Extract, Transform, Load functionality.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_location, headers, URLs):\n",
    "        self.DATA_LOCATION = data_location\n",
    "        \n",
    "        self.HEADERS = headers\n",
    "        \n",
    "        \n",
    "        self.INVESTOPEDIA_URL = URLs[0]\n",
    "        self.YAHOO_URL = URLs[1]\n",
    "        \n",
    "        self.DATAHUB_URL = URLs[2]\n",
    "        self.SP_DOWNLOAD_URL = URLs[3]\n",
    "        self.VUSA_DOWNLOAD_URL = URLs[4]\n",
    "        \n",
    "        \n",
    "        self.OPTIONS = self.set_options()\n",
    "        nltk.download(['punkt', 'wordnet', 'stopwords', 'omw-1.4', 'vader_lexicon'], quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60635408",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Pipeline\n",
    "def set_options(self):\n",
    "        \"\"\"\n",
    "        Set selenium browser options so that it runs headless\n",
    "        \n",
    "        Returns \n",
    "        -----------\n",
    "        OPTIONS : Options\n",
    "            selenium options with headless browser arguments\n",
    "        \"\"\"\n",
    "        OPTIONS = Options()\n",
    "        OPTIONS.add_argument('--headless')\n",
    "        OPTIONS.add_argument('--disable-gpu') \n",
    "        return OPTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2c70b0",
   "metadata": {},
   "source": [
    "# Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66fc5943",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Pipeline\n",
    "def progress_bar(self, current, total, bar_length=20):\n",
    "        fraction = current / total\n",
    "\n",
    "        arrow = int(fraction * bar_length - 1) * '-' + '>'\n",
    "        padding = int(bar_length - len(arrow)) * ' '\n",
    "\n",
    "        ending = '\\n' if current == total else '\\r'\n",
    "\n",
    "        print(f'Progress: [{arrow}{padding}] {int(fraction*100)}%', end=ending)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf09a17",
   "metadata": {},
   "source": [
    "# Extracting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a6cf51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Pipeline\n",
    "def get_response(self, URL):\n",
    "        \"\"\"\n",
    "        Does a get request and returns a beautifulsoup object of the webpage.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        URL : str\n",
    "            String of the url to get the content from.\n",
    "            \n",
    "        Returns\n",
    "        -----------\n",
    "        soup : BeautifulSoup\n",
    "            A Beautifulsoup object that contains the html of the webpage.\n",
    "        \"\"\"\n",
    "        headers = self.HEADERS\n",
    "        r = requests.get(URL, headers=headers)\n",
    "        soup = bs(r.content)\n",
    "        return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afd2dc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Pipeline\n",
    "def get_datahub_csv(self, URL, filename):\n",
    "        \"\"\"\n",
    "        Finds download URL on datahub.io, does a get request to that URL \n",
    "        and writes the downloaded CSV to the data location specified in self.DATA_LOCATION \n",
    "        with the name given in the parameters.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        URL : str\n",
    "            String of the url to get the content from.\n",
    "        filename : str\n",
    "            The name that should be given to the final downloaded file.\n",
    "        \n",
    "        \"\"\"\n",
    "        soup = self.get_response(URL)\n",
    "        download_url = soup.find_all('table')[1].find_all('a')[1]['href']\n",
    "        download_url = self.DATAHUB_URL + download_url\n",
    "        with open(self.DATA_LOCATION + filename, 'wb') as f:\n",
    "            f.write(requests.get(download_url).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe1aae31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Pipeline\n",
    "def get_yahoo_news_articles_df(self):\n",
    "        \"\"\"\n",
    "        Uses a selenium browser with the options specified in self.OPTIONS to open the self.YAHOO_URL, scroll down \n",
    "        and get all the titles, URLs and CSS classes of the articles and puts them in a pd.DataFrame.\n",
    "        \n",
    "        Returns\n",
    "        -----------\n",
    "        df : pd.DataFrame\n",
    "            DataFrame with the titles of articles, URLs of articles and CSS classes of articles from self.YAHOO_URL.\n",
    "        \n",
    "        \"\"\"\n",
    "        yahoo_news_driver = webdriver.Chrome(options=self.OPTIONS)\n",
    "\n",
    "        yahoo_news_driver.get(self.YAHOO_URL)\n",
    "        yahoo_news_driver.find_element(By.CLASS_NAME, 'reject-all').click()\n",
    "        actions = ActionChains(yahoo_news_driver)\n",
    "        for i in range(1500):\n",
    "            actions.scroll_by_amount(0, 500)\n",
    "        actions.perform()\n",
    "        actions.reset_actions()\n",
    "\n",
    "        links = self.get_article_links(yahoo_news_driver)\n",
    "        df = pd.DataFrame({'title': [link.text for link in links], \n",
    "                           'url': [link.get_attribute('href') for link in links], \n",
    "                           'classes': [link.get_attribute('class') for link in links]})\n",
    "\n",
    "        yahoo_news_driver.quit()\n",
    "        df = df[df['url'] \\\n",
    "                .str.contains('/news/') & df['classes'].str.contains('js-content-viewer')]\\\n",
    "                .drop_duplicates(subset='url', keep='last')\\\n",
    "                .reset_index(drop=True)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "675ac796",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Pipeline\n",
    "def scrape_articles(self, URLs):\n",
    "        \"\"\"\n",
    "        Uses a list of URLs of articles from yahoo finance to scrape the articles from the URLs, \n",
    "        clean them and return the cleaned and raw lists\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        URLs : list[str]\n",
    "            list of strings of URLs of articles.\n",
    "            \n",
    "        Returns\n",
    "        -----------\n",
    "        articles : list[str]\n",
    "            A list of articles cleaned to only have words- and whitespace characters.\n",
    "            \n",
    "        raw_articles : list[list[str]]\n",
    "            A list of lists of html from the paragraphs of the scraped articles.\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        articles = []\n",
    "        raw_articles = []\n",
    "        yahoo_driver = webdriver.Chrome(options=self.OPTIONS)\n",
    "        yahoo_driver.get(self.YAHOO_URL)\n",
    "        yahoo_driver.find_element(By.CLASS_NAME, 'reject-all').click()\n",
    "\n",
    "        for index, url in enumerate(URLs):\n",
    "            yahoo_driver.get(url)\n",
    "            try:\n",
    "                yahoo_driver.find_element(By.CLASS_NAME, 'collapse-button').click()\n",
    "            except:\n",
    "                pass\n",
    "            article_p_elements = yahoo_driver.find_element(By.CLASS_NAME, 'caas-body').find_elements(By.TAG_NAME, 'p')\n",
    "            article_by_paragraph = [p.text for p in article_p_elements if p.text!='']\n",
    "\n",
    "            raw_article = [bs(element.get_attribute('outerHTML'), \"html.parser\") for element in article_p_elements]\n",
    "            article = \" \".join(article_by_paragraph)\n",
    "            clean_article = self.clean_text(article)\n",
    "            articles.append(clean_article)\n",
    "            raw_articles.append(raw_article)\n",
    "            self.progress_bar(index+1, len(URLs))\n",
    "\n",
    "\n",
    "        yahoo_driver.quit()\n",
    "\n",
    "        return articles, raw_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83b7b902",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Pipeline\n",
    "def download_yahoo_data(self, file_path):\n",
    "        \"\"\"\n",
    "        Downloadt gegevens van Yahoo Finance voor tickers die zijn vermeld in het opgegeven CSV-bestand.\n",
    "        Downloads data from yahoo finance of tickers that appear in the file from the given file path\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        file_path : str\n",
    "            File path to a csv containing ticker symbols.\n",
    "\n",
    "        Returns\n",
    "        -----------\n",
    "        filtered_data : pd.DataFrame\n",
    "            Filtered DataFrame with downloaded data.\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(file_path)\n",
    "        symbol_list = df.iloc[:, 0].tolist()\n",
    "\n",
    "        download = yf.download(symbol_list, group_by=\"ticker\")\n",
    "        data = download.copy()\n",
    "\n",
    "        filtered_data = data.dropna(axis=1, how='all')\n",
    "        filtered_data.columns = filtered_data.columns.remove_unused_levels()\n",
    "\n",
    "        remaining_tickers = list(filtered_data.columns.levels[0])\n",
    "        missing_tickers = list(set(symbol_list) - set(remaining_tickers))\n",
    "\n",
    "        return filtered_data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37d34341",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Pipeline\n",
    "def extract(self):\n",
    "        # get site data\n",
    "        self.investopedia_soup = self.get_response(self.INVESTOPEDIA_URL)\n",
    "\n",
    "        # download csv data\n",
    "        # get_datahub_csv(self.VUSA_DOWNLOAD_URL, 'vusa_holdings.csv')\n",
    "        # get_datahub_csv(self.SP_DOWNLOAD_URL, 'time_data.csv')\n",
    "       \n",
    "        # get csv data\n",
    "        self.vusa_df = pd.read_csv(self.DATA_LOCATION + 'vusa_holdings.csv')\n",
    "        self.time_data_df = pd.read_csv(self.DATA_LOCATION + 'time_data.csv')\n",
    "\n",
    "        # get yahoo articles with titles, urls and css classes\n",
    "        self.yahoo_news_df = self.get_yahoo_news_articles_df()\n",
    "    \n",
    "        # scrape news articles from yahoo article urls\n",
    "        self.yahoo_news_articles, self.raw_yahoo_news_articles = self.scrape_articles(self.yahoo_news_df['url'])\n",
    "        \n",
    "        # get api data\n",
    "        self.api_df = self.download_yahoo_data('Datasets/vusa_holdings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ef8fe3",
   "metadata": {},
   "source": [
    "# Transforming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81bfe266",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Pipeline\n",
    "def get_card_titles(self):\n",
    "        \"\"\"\n",
    "        Get the titles from the a-tags with the CSS class \"card\"\n",
    "        \n",
    "        Returns\n",
    "        -----------\n",
    "        titles : list[str]\n",
    "            titles of the articles from the html\n",
    "        \"\"\"\n",
    "        titles = []\n",
    "        a_tags = self.investopedia_soup.find_all('a', class_='card')\n",
    "        for i in range(len(a_tags)):\n",
    "            titles.append(a_tags[i].span.text)\n",
    "        return titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfd276fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Pipeline\n",
    "def get_card_urls(self):\n",
    "        \"\"\"\n",
    "        Get the URLs from the a-tags with the CSS class \"card\"\n",
    "        \n",
    "        Returns\n",
    "        -----------\n",
    "        URLs : list[str]\n",
    "            URLs of the articles from the html\n",
    "        \"\"\"\n",
    "        URLs = []\n",
    "        a_tags = self.investopedia_soup.find_all('a', class_='card')\n",
    "        for i in range(len(a_tags)):\n",
    "            URLs.append(a_tags[i]['href'])\n",
    "        return URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b91c4ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Pipeline\n",
    "def get_url_content(self, urls):\n",
    "        articles = []\n",
    "        raw_articles = []\n",
    "        for url in urls:\n",
    "            soup = self.get_response(url)\n",
    "            paragraphs = soup.find('div', class_='article-content').find_all('p')\n",
    "            article = ''.join([p.get_text(separator=' ') for p in paragraphs])\n",
    "            raw_articles.append(paragraphs)\n",
    "            articles.append(self.clean_text(article))\n",
    "\n",
    "        return articles, raw_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab58bf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Pipeline\n",
    "def get_symbols(self, raw_articles, href_ref):\n",
    "        symbol_list = []\n",
    "        for p_list in raw_articles:\n",
    "            article_symbols = []\n",
    "            for p in p_list:\n",
    "                symbols = [a_tag.text for a_tag in p.find_all('a') if re.search(href_ref, a_tag['href'])]\n",
    "                if symbols:\n",
    "                    for symbol in symbols:\n",
    "                        article_symbols.append(symbol)\n",
    "            symbol_list.append(article_symbols)\n",
    "        return symbol_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58b3e0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Pipeline\n",
    "def clean_text(self, text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da9f14e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Pipeline\n",
    "def get_article_links(self, driver):\n",
    "    links = driver.find_elements(By.TAG_NAME, 'a')\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d6b99fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Pipeline\n",
    "def clean_and_tokenize(self, articles):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        tokenized_articles = []\n",
    "        for article in articles:\n",
    "            tokenized_article = nltk.word_tokenize(article)\n",
    "            tokenized_article = [token for token in tokenized_article if token not in stopwords.words('english')]\n",
    "            tokenized_article = [lemmatizer.lemmatize(token) for token in tokenized_article]\n",
    "\n",
    "            tokenized_articles.append(tokenized_article)\n",
    "\n",
    "        return tokenized_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f82a902",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Pipeline\n",
    "def combine_data(self, api_df, investopedia_df, yahoo_df, time_data):\n",
    "        df1 = investopedia_df.drop(['symbols'], axis=1)\n",
    "        df2 = yahoo_df.drop('classes', axis=1)\n",
    "        text_df = pd.concat([df1, df2])\n",
    "        text_df = text_df.reset_index(drop=True)\n",
    "        \n",
    "        # tokenize the articles and add them to a DataFrame\n",
    "        text_df['tokenized_article'] = self.clean_and_tokenize(text_df['article'])\n",
    "        \n",
    "        # apply sentiment analysis and add it to the DataFrame\n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "        text_df['positivity_score'] = [analyzer.polarity_scores(article)['pos'] for article in text_df['article']]\n",
    "        text_df['negativity_score'] = [analyzer.polarity_scores(article)['neg'] for article in text_df['article']]\n",
    "        \n",
    "        # get statistics from positive and negative sentiment analysis\n",
    "        pos = text_df.describe().T[:1].reset_index(drop=True)\n",
    "        neg = text_df.describe().T[1:].reset_index(drop=True)\n",
    "        \n",
    "        # rename columns in pos and neg so they can be concattinated\n",
    "        pos.columns = ['pos_' + col for col in pos.columns]\n",
    "        neg.columns = ['neg_' + col for col in neg.columns]\n",
    "        \n",
    "        # concattinate the positve and negative sentiment analysis statistics to a 1 row DataFrame\n",
    "        pos_neg_row = pd.concat([pos, neg], axis=1)\n",
    "        \n",
    "        api_df = api_df.iloc[::-1].reset_index(drop=False, names='Date')\n",
    "        \n",
    "        api_df.columns = ['_'.join(col).strip() for col in api_df.columns.values]\n",
    "        \n",
    "        combined_df = pd.concat([api_df, pos_neg_row], axis=1)\n",
    "        \n",
    "        combined_df['Date_'] = combined_df['Date_'].astype(str)\n",
    "        time_data['Date'] = time_data['Date'].astype(str)\n",
    "\n",
    "        final_df = combined_df.merge(time_data, how='left', left_on='Date_', right_on='Date')\n",
    "        \n",
    "        return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbc1af54",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Pipeline\n",
    "def transform(self):        \n",
    "        # filter site data\n",
    "        titles = self.get_card_titles()\n",
    "        urls = self.get_card_urls()\n",
    "        \n",
    "        # transform filtered data to DataFrame(investopedia_df)\n",
    "        investopedia_df = pd.DataFrame({'url': urls, 'title': titles})\n",
    "        \n",
    "        # get articles from urls\n",
    "        articles, raw_articles = self.get_url_content(investopedia_df['url'])\n",
    "                \n",
    "        # get symbols from article\n",
    "        symbols = self.get_symbols(raw_articles, 'widgetsymbol')\n",
    "        \n",
    "        # filter symbols from the vusa_df csv data\n",
    "        symbols_in_vusa = self.vusa_df['Symbol'].tolist()\n",
    "        \n",
    "        # add articles and symbols to DataFrame(investopedia_df)\n",
    "        investopedia_df['article'] = articles\n",
    "        investopedia_df['symbols'] = symbols\n",
    "\n",
    "        # filter DataFrame(investopedia_df) to only have symbols that occur in the symbols_in_vusa\n",
    "        investopedia_df = investopedia_df[investopedia_df['symbols'].apply(lambda symbols: any(symbol in symbols_in_vusa for symbol in symbols))]\n",
    "                \n",
    "        # add scraped articles to DataFrame\n",
    "        self.yahoo_news_df['article'] = self.yahoo_news_articles\n",
    "\n",
    "        # combine DataFrames\n",
    "        self.df = self.combine_data(self.api_df, investopedia_df, self.yahoo_news_df, self.time_data_df)\n",
    "        self.df = self.df.dropna(axis=0, thresh=int(self.df.shape[1]*0.2))\n",
    "        self.df = self.df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9999cf",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a2536b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Pipeline\n",
    "def load(self, filename):\n",
    "        self.df.to_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5871eb",
   "metadata": {},
   "source": [
    "# ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e474c313",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Pipeline\n",
    "def ETL(self, filename):\n",
    "    self.extract()\n",
    "    self.transform()\n",
    "    self.load(filename)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3225b65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(DATA_LOCATION, HEADERS, URLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16129433",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [------------------->] 100%\n",
      "[*********************100%%**********************]  505 of 505 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "27 Failed downloads:\n",
      "['CTXS', 'FB', 'VIAC', 'INFO', 'FBHS', 'WLTW', 'DRE', 'KSU', 'ANTM', 'FRC', 'BRK.B', 'DISCK', 'FISV', 'DISCA', 'PKI', 'ATVI', 'NLOK', 'PBCT', 'SIVB', 'TWTR', 'ABC', 'NLSN', 'RE', 'BLL', 'CERN', 'XLNX']: Exception('%ticker%: No timezone found, symbol may be delisted')\n",
      "['BF.B']: Exception('%ticker%: No price data found, symbol may be delisted (1d 1925-02-24 -> 2024-02-01)')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline.ETL('Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad26226c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date_</th>\n",
       "      <th>KMI_Open</th>\n",
       "      <th>KMI_High</th>\n",
       "      <th>KMI_Low</th>\n",
       "      <th>KMI_Close</th>\n",
       "      <th>KMI_Adj Close</th>\n",
       "      <th>KMI_Volume</th>\n",
       "      <th>NRG_Open</th>\n",
       "      <th>NRG_High</th>\n",
       "      <th>NRG_Low</th>\n",
       "      <th>...</th>\n",
       "      <th>Date</th>\n",
       "      <th>SP500</th>\n",
       "      <th>Dividend</th>\n",
       "      <th>Earnings</th>\n",
       "      <th>Consumer Price Index</th>\n",
       "      <th>Long Interest Rate</th>\n",
       "      <th>Real Price</th>\n",
       "      <th>Real Dividend</th>\n",
       "      <th>Real Earnings</th>\n",
       "      <th>PE10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-02-01</td>\n",
       "      <td>16.920000</td>\n",
       "      <td>17.049999</td>\n",
       "      <td>16.870001</td>\n",
       "      <td>17.045000</td>\n",
       "      <td>17.045000</td>\n",
       "      <td>2315307.0</td>\n",
       "      <td>53.080002</td>\n",
       "      <td>53.509998</td>\n",
       "      <td>52.869999</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-31</td>\n",
       "      <td>17.219999</td>\n",
       "      <td>17.250000</td>\n",
       "      <td>16.900000</td>\n",
       "      <td>16.920000</td>\n",
       "      <td>16.920000</td>\n",
       "      <td>16870100.0</td>\n",
       "      <td>53.779999</td>\n",
       "      <td>54.119999</td>\n",
       "      <td>52.619999</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-30</td>\n",
       "      <td>17.049999</td>\n",
       "      <td>17.230000</td>\n",
       "      <td>17.010000</td>\n",
       "      <td>17.209999</td>\n",
       "      <td>17.209999</td>\n",
       "      <td>12446200.0</td>\n",
       "      <td>54.389999</td>\n",
       "      <td>54.639999</td>\n",
       "      <td>53.700001</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-29</td>\n",
       "      <td>17.440001</td>\n",
       "      <td>17.490000</td>\n",
       "      <td>17.250000</td>\n",
       "      <td>17.370001</td>\n",
       "      <td>17.087002</td>\n",
       "      <td>13665900.0</td>\n",
       "      <td>54.310001</td>\n",
       "      <td>54.689999</td>\n",
       "      <td>53.450001</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-26</td>\n",
       "      <td>17.330000</td>\n",
       "      <td>17.469999</td>\n",
       "      <td>17.299999</td>\n",
       "      <td>17.440001</td>\n",
       "      <td>17.155861</td>\n",
       "      <td>14261800.0</td>\n",
       "      <td>53.740002</td>\n",
       "      <td>54.270000</td>\n",
       "      <td>53.529999</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11058</th>\n",
       "      <td>1980-03-21</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11059</th>\n",
       "      <td>1980-03-20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11060</th>\n",
       "      <td>1980-03-19</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11061</th>\n",
       "      <td>1980-03-18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11062</th>\n",
       "      <td>1980-03-17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11063 rows × 2895 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date_   KMI_Open   KMI_High    KMI_Low  KMI_Close  KMI_Adj Close  \\\n",
       "0      2024-02-01  16.920000  17.049999  16.870001  17.045000      17.045000   \n",
       "1      2024-01-31  17.219999  17.250000  16.900000  16.920000      16.920000   \n",
       "2      2024-01-30  17.049999  17.230000  17.010000  17.209999      17.209999   \n",
       "3      2024-01-29  17.440001  17.490000  17.250000  17.370001      17.087002   \n",
       "4      2024-01-26  17.330000  17.469999  17.299999  17.440001      17.155861   \n",
       "...           ...        ...        ...        ...        ...            ...   \n",
       "11058  1980-03-21   0.000000   0.000000   0.000000   0.000000       0.000000   \n",
       "11059  1980-03-20   0.000000   0.000000   0.000000   0.000000       0.000000   \n",
       "11060  1980-03-19   0.000000   0.000000   0.000000   0.000000       0.000000   \n",
       "11061  1980-03-18   0.000000   0.000000   0.000000   0.000000       0.000000   \n",
       "11062  1980-03-17   0.000000   0.000000   0.000000   0.000000       0.000000   \n",
       "\n",
       "       KMI_Volume   NRG_Open   NRG_High    NRG_Low  ...  Date  SP500  \\\n",
       "0       2315307.0  53.080002  53.509998  52.869999  ...     0    0.0   \n",
       "1      16870100.0  53.779999  54.119999  52.619999  ...     0    0.0   \n",
       "2      12446200.0  54.389999  54.639999  53.700001  ...     0    0.0   \n",
       "3      13665900.0  54.310001  54.689999  53.450001  ...     0    0.0   \n",
       "4      14261800.0  53.740002  54.270000  53.529999  ...     0    0.0   \n",
       "...           ...        ...        ...        ...  ...   ...    ...   \n",
       "11058         0.0   0.000000   0.000000   0.000000  ...     0    0.0   \n",
       "11059         0.0   0.000000   0.000000   0.000000  ...     0    0.0   \n",
       "11060         0.0   0.000000   0.000000   0.000000  ...     0    0.0   \n",
       "11061         0.0   0.000000   0.000000   0.000000  ...     0    0.0   \n",
       "11062         0.0   0.000000   0.000000   0.000000  ...     0    0.0   \n",
       "\n",
       "       Dividend  Earnings  Consumer Price Index  Long Interest Rate  \\\n",
       "0           0.0       0.0                   0.0                 0.0   \n",
       "1           0.0       0.0                   0.0                 0.0   \n",
       "2           0.0       0.0                   0.0                 0.0   \n",
       "3           0.0       0.0                   0.0                 0.0   \n",
       "4           0.0       0.0                   0.0                 0.0   \n",
       "...         ...       ...                   ...                 ...   \n",
       "11058       0.0       0.0                   0.0                 0.0   \n",
       "11059       0.0       0.0                   0.0                 0.0   \n",
       "11060       0.0       0.0                   0.0                 0.0   \n",
       "11061       0.0       0.0                   0.0                 0.0   \n",
       "11062       0.0       0.0                   0.0                 0.0   \n",
       "\n",
       "       Real Price  Real Dividend  Real Earnings  PE10  \n",
       "0             0.0            0.0            0.0   0.0  \n",
       "1             0.0            0.0            0.0   0.0  \n",
       "2             0.0            0.0            0.0   0.0  \n",
       "3             0.0            0.0            0.0   0.0  \n",
       "4             0.0            0.0            0.0   0.0  \n",
       "...           ...            ...            ...   ...  \n",
       "11058         0.0            0.0            0.0   0.0  \n",
       "11059         0.0            0.0            0.0   0.0  \n",
       "11060         0.0            0.0            0.0   0.0  \n",
       "11061         0.0            0.0            0.0   0.0  \n",
       "11062         0.0            0.0            0.0   0.0  \n",
       "\n",
       "[11063 rows x 2895 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d87208",
   "metadata": {},
   "source": [
    "# Referentielijst\n",
    "- Fondsbeheerder|NOT,Disclosed|Vanguard S&P 500 UCITS ETF (EUR)|ISIN:IE00B3XXRP09. (n.d.). https://www.morningstar.nl/nl/etf/snapshot/snapshot.aspx?id=0P0000YXKB&tab=4&InvestmentType=FE\n",
    "- Wat zijn ETF’s | Educatie | BlackRock. (n.d.). BlackRock. https://www.blackrock.com/be/individual/nl/educatie/etfs-uitgelegd#Wat-zijn-ETF'\n",
    "- S&P 500 | Wat is de S&P 500? | Beleggingswiki - Semmie.nl. (n.d.). Semmie. https://semmie.nl/wiki/sp-500/#:~:text=De%20S%26P%20500%20is%20een,ontwikkeling%20van%20de%20financi%C3%ABle%20markt.\n",
    "- SWOCC. (2020, June 8). De invloed van mediaberichtgeving op beurskoersen - SWOCC. https://www.swocc.nl/kennisbank-item/de-invloed-van-mediaberichtgeving-op-beurskoersen/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
