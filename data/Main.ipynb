{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a96dba24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "import re\n",
    "import requests\n",
    "import codecs\n",
    "import time\n",
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "import yfinance as yf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57d341c",
   "metadata": {},
   "source": [
    "# Probleemomschrijving\n",
    "Het doel is om een pipeline te creeën die data ophaalt, transformeert en laad om vervolgens gebruikt te kunnen worden om de prijs van de ETF VUSA te voorspellen. Hierbij gaan wij artikelen van investopedia en yahoo finance scrapen en hier sentiment analysis op toepassen. Ook halen wij de historische prijsdata op van de verschillende aandelen die zich in de ETF bevinden. En als laatst halen wij de historische prijsdata op van VUSA zelf op en voegen wij alles samen tot 1 DataFrame. Deze pipeline kan elke dag gerund worden om zo nieuwe artikelen op te halen en deze toe te voegen aan de dataset, zo krijg je een steeds beter bruikbare dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bdc67b",
   "metadata": {},
   "source": [
    "# Domeinonderzoek\n",
    "Vusa (IE00B3XXRP09) is een ETF die beheert wordt door Vanguard group (Fondsbeheerder|NOT,Disclosed|Vanguard S&P 500 UCITS ETF (EUR)|ISIN:IE00B3XXRP09, n.d.). Een ETF is een aandeel wat eigenlijk bestaat uit meerdere aandelen, obligaties of andere effecten die op de markt verhandelt worden (Wat Zijn ETF’s | Educatie | BlackRock, n.d.). VUSA specifiek volgt de marktindex van de SP500, dat zijn aandelen van de 500 grootste bedrijven van de Verenigde Staten. \n",
    "\n",
    "De SP500 geeft een goed beeld van de amerikaanse markt, aangezien het veel verschillende sectoren en aandelen van grote bedrijven binnen deze sectoren bevat. SP500 heeft daarom een unieke ligging in de aandelenmarkt, omdat het zo groot kunnen er duidelijke veranderingen in de amerikaanse markt worden gemeten naar aanleiding van de SP500. Omdat de SP500 zo'n goed beeld geeft van de amerikaanse markt worden veel beleggingen hiermee vergeleken om te kijken hoe goed het gaat. (S&P 500 | Wat Is De S&P 500? | Beleggingswiki - Semmie.nl, n.d.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebffe97",
   "metadata": {},
   "source": [
    "# Vereiste informatie, databronnen en formaten ruwe data\n",
    "Wij willen willen artikelen scrapen om daar sentiment analysis op toe te passen. Wij zijn van mening dat dit belangrijke data is en deze bron kan dat bevestigen: (SWOCC, 2020). De rauwe data komt in de vorm van text voor yahoo finance en een beautifulsoup object voor investopedia.\n",
    "- https://www.investopedia.com/markets-news-4427704\n",
    "- https://finance.yahoo.com/\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Ook willen wij gebruik maken van de prijsgeschiedenis van de aandelen die te vinden zijn in de VUSA ETF, de API geeft deze data als dataframe terug. Wij hebben de API hier gevonden:\n",
    "- https://github.com/ranaroussi/yfinance\n",
    "\n",
    "Dit hebben wij vervolgens via pip geinstalleerd en geimporteerd via deze code: \"import yfinance as yf\" die aan het begin van het bestand wordt gebruikt.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Als laatst willen wij de prijsgeschiedenis gebruiken van de VUSA zelf, hier hebben wij een publieke dataset voor gedownload van:\n",
    "- https://datahub.io/core/s-and-p-500\n",
    "\n",
    "Echter is de website veranderd en is deze dataset niet meer beschikbaar in deze vorm. Hieronder is een link die laat zien hoe de website er voorheen uitzag:\n",
    "\n",
    "- https://web.archive.org/web/20230402111356/https://datahub.io/core/s-and-p-500\n",
    "\n",
    "De data kwam in een csv bestand en aangezien wij dit destijds hadden gedownload gebruiken wij nu het gedownloade bestand in de pipeline.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Om de juiste data uit de yfinance API te halen en de artikelen van investopedia te filteren hebben wij gebruik gemaakt van een csv bestand wat wij ook gedownload hebben van datahub, aangezien de gehele website is veranderd is ook deze URL niet meer bruikbaar en gebruiken wij de voorheen gedownloade CSV. De originele URL is:\n",
    "\n",
    "- https://datahub.io/core/s-and-p-500-companies\n",
    "\n",
    "En om te zien hoe de website eruit zag kan je deze link volgen:\n",
    "- https://web.archive.org/web/20230601125536/https://datahub.io/core/s-and-p-500-companies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be808ad6",
   "metadata": {},
   "source": [
    "# Het proces\n",
    "In deze cell zullen wij uitleggen en beschrijven wat er allemaal in de class Pipeline gebeurt. We zullen de methodes uitleggen en hoe wij de de verschillende nodige technieken hebben toegepast.\n",
    "\n",
    "## ETL\n",
    "Wij maken in dit project gebruik van ETL (Extract, transform, load). Hiervoor hebben wij ook een ETL methode aangemaakt onderaan de class, hier worden de Extract, transform en load methodes aangeroepen die vervolgens de stappen uivoeren die nodig zijn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f73cc79",
   "metadata": {},
   "source": [
    "\n",
    "## Extract\n",
    "In de Extract methode halen wij alle ruwe data uit de beschikbare bronnen. \n",
    "\n",
    "Het begint met het aanroepen van de get_response methode met de URL voor investopedia (https://www.investopedia.com/markets-news-4427704), get_response doet hier een get request en zet de html om in een beautifulsoup object.\n",
    "\n",
    "vervolgens werden de csv bestanden gedownload van datahub.io maar aangezien dit niet meer beschikbaar is, is dit uigecomment. En in plaats hiervan worden de gedownloade bestanden ingelezen als dataframes en in de variabelen: vusa_df en time_data_df gezet.\n",
    "\n",
    "Dan wordt de yahoo finance website gescraped doormiddel van de get_yahoo_news_articles_df methode, deze methode weigert de niet-essentiele cookies en scrollt dan door de yahoo_finance website(https://finance.yahoo.com/) om zo alle URLs op te halen van de artikelen die op de website staan. Dit doen we doormiddel van een headless selenium browser, aangezien de website dynamisch geladen word en zonder scrollen niet alle URLs opgehaald kunnen worden. Deze URLs worden in een DataFrame gezet met de titels van de artikelen en css klassen van de artikelen op de website. We halen de artikelen en css klassen ook op om duidelijk te kunnen zien of alles goed is gegaan en problemen makkelijk op te lossen als het niet zou gaan zoals wij verwachten.\n",
    "\n",
    "Dan hebben we alle URLs van de artikelen maar de artikelen zelf hebben we nog niet, dus dat is het volgende wat wij doen. \n",
    "Dit doen we met de methode scrape_articles waar wij de URLs als parameter meegeven. Deze methode klikt eerst op het weigeren van niet-essentiele cookies en laadt vervolgens 1 voor 1 de verschillende URLs. Sommige van de artikelen zijn groter en yahoo finance laadt dan niet het gehele artikel zien, hiervoor moet eerst op een knop gedrukt worden met de class \n",
    "'collapse-button'. Wij maken gebruik van een try, except om deze knop in te drukken wat zorgt voor robuustheid zodat alle artikelen goed geladen kunnen worden. vervolgens worden de artikelen opgehaald en schoongemaakt doormiddel van de regex functie die in de methode clean_text gebruikt wordt. scrape_articles geeft ook de rauwe html van de artikelen terug om zo makkelijk te kunnen zien of alles volgens verwachting is verlopen en fouten makkelijk op te lossen.\n",
    "\n",
    "Als laatst word yfinance API gebruikt om de prijsgeschiedenis te downloaden, dit wordt gedaan in de download_yahoo_data methode. In deze methode wordt ook de locatie van het bestand met de tickers van de bedrijven in de SP500 meegegeven zodat de methode dit kan gebruiken om te weten welke prijsdata gedownload moet worden. In dit bestand staan de tickers van de bedrijven in de SP500 en dit wordt omgezet naar een lijst en gebruikt voor de finance API om te weten wat te downloaden. Dit wordt gedownload en automatisch in een DataFrame gezet. Ook geeft yfinance automatisch aan welke downloads niet gelukt zijn, dit is goed voor de robuustheid aangezien we dan makkelijk kunnen zien waar het wellicht misgaat. Wij hebben een aantal downloads die sowieso mislukken, dit zijn downloads van bedrijven die niet meer verhandelt worden. Een goed voorbeeld hiervan is facebook, facebook wordt niet meer verhandelt aangezien het bedrijf is veranderd naar meta.\n",
    "\n",
    "Ook worden de symbolen uit vusa_df gefilterd en deze worden in lijst symbols_in_vusa gezet. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa22c73",
   "metadata": {},
   "source": [
    "## Transform\n",
    "Nadat alle ruwe data is geëxtraheerd gaan we de data transformeren zodat het bruikbaar is voor machine learning. We voegen alle data samen, schonen dit op en vervangen lege waardes. \n",
    "\n",
    "We beginnen met het beautifulsoup object van de webpagina van investopedia(https://www.investopedia.com/markets-news-4427704). Met de methodes get_card_titles en get_card_urls halen wij de titels en URLs van de artikelen op vanuit de webpagina. De webpagina van investopedia is niet dynamisch geladen dus kunnen wij alle artikelen uit het beautifulsoup object halen. De titels en URLs worden beide opgehaald om makkelijk te kunnen checken of alle artikelen meegenomen zijn al worden er opmerkelijkheden gespot. Deze titels en URLs worden in een dataframe gezet met de naam investopedia_df. Nu we de URLs hebben gebruiken we de functie get_url_content om de artikelen van de website te scrapen doormiddel van beautifulsoup.\n",
    "De artikelen worden teruggegeven als ruwe artikelen en schone artikelen die schoongemaakt zijn door de clean_text methode.\n",
    "\n",
    "De ruwe artikelen worden ook gebruikt om de symbolen van de bedrijven te filteren die genoemd worden in het artikel, dit wordt gedaan met de get_symbols methode. Alle artikelen en symbolen worden toegevoegd aan de investopedia_df zodat daar alles instaat van investopedia. vervolgens word de DataFrame gefilterd op de artikelen die symbolen bevat die ook in symbols_in_vusa voorkomt.\n",
    "\n",
    "Daarna worden de artikelen van yahoo finance die eerder opgehaald zijn bij het extraheren toegevoegd aan de yahoo_news_df. \n",
    "\n",
    "Nu hebben we alle data in 4 verschillende DataFrames staan:\n",
    "- api_df: Hier staat de gedownloade prijsdata van de bedrijven in die voorkomen in de SP500\n",
    "- investopedia_df: Hier staan de artikelen van investopedia in, gefilterd op of de bedrijven die genoemt zijn in de artikelen voorkomen in de bedrijven van de SP500\n",
    "- yahoo_news_df: Hier staan de artikelen in van de yahoo finance website\n",
    "- time_data_df: Hier staat de prijsgeschiedenis van de SP500 zelf in.\n",
    "\n",
    "Deze 4 Dataframes worden samengevoegd in de combine_data methode, in deze methode wordt de laatste processing gedaan en alles samengevoegd.\n",
    "\n",
    "De combine_data methode worden eerst de investopedia_df en yahoo_news_df gefilterd en samengevoegt tot 1 text dataframe genaamd text_df. De artikelen uit de text_df worden schoongemaakt, getokenized, gefilterd en gelemmatized in de clean_and_tokenize methode en toegevoegd als nieuwe kolom aan de text_df. Dan wordt doormiddel van de SentimentIntensityAnalyzer de positieve en negatieve sentiment uit de artikelen gehaald en deze worden apart toegevoegd als kolommen in de text_df. Vervolgens moeten we de data zo transformeren dat het allemaal numeriek wordt en klaar om toegevoegd aan de uiteindelijke dataframe. Dit doen we door de basisstatistieken van de positieve en negatieve sentimentscores op te halen en samen te voegen tot een DataFrame van 1 rij. Nu hebben wij een DataFrame met de statistieken van de sentimentscores van vandaag.\n",
    "\n",
    "Vervolgens voegen wij de statistieken van de sentimentscores van de text toe bij vandaag in de api_df. Deze wordt dan vervolgens gemerged met de time_data op de key met de datums. Zo hebben we dus nu de prijsgeschiedenis van alle bedrijven in de SP500, de prijsgeschiedenis van de SP500 zelf en de statistieken van de sentimentscores van vandaag. Als dit elke dag gerunt word dan worden langzaam steeds meer rijen met statistieken van sentimentscores van alle dagen toegevoegd.\n",
    "\n",
    "Als laatst worden de missende waardes vervangen met fillna zodat het bruikbaar wordt voor machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c7fcb0",
   "metadata": {},
   "source": [
    "## Load\n",
    "In de load functie word er voor gezorgd dat de data makkelijk opgehaald en gebruikt kan worden voor machine learning.\n",
    "\n",
    "Wij doen dit door de DataFrame op te slaan in een CSV bestand wat later makkelijk gemimporteerd en gebruikt kan worden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e474c313",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    \"\"\"\n",
    "    Pipeline to get data relating to the stock vusa that tracks the SP500. Includes sentiment analysis from articles,\n",
    "    price history of stocks that are in the ETF and price history of vusa itself. \n",
    "    The class follows the Extract, Transform, Load functionality.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.HEADERS = {'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "           'Accept-Encoding': 'gzip, deflate, br',\n",
    "           'Accept-Language': 'en-US,en;q=0.9,nl;q=0.8',\n",
    "           'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'}\n",
    "\n",
    "        self.DATA_LOCATION = 'Datasets/'\n",
    "\n",
    "        self.HOMEPAGE_URL = 'https://www.investopedia.com/markets-news-4427704'\n",
    "\n",
    "        self.VUSA_DOWNLOAD_URL = 'https://datahub.io/core/s-and-p-500-companies'\n",
    "        self.SP_DOWNLOAD_URL = 'https://datahub.io/core/s-and-p-500'\n",
    "        self.DATAHUB_URL = 'https://datahub.io'\n",
    "\n",
    "        self.YAHOO_URL = 'https://finance.yahoo.com/'\n",
    "\n",
    "        self.OPTIONS = Options()\n",
    "        self.OPTIONS.add_argument('--headless')\n",
    "        self.OPTIONS.add_argument('--disable-gpu') \n",
    "        \n",
    "        nltk.download(['punkt', 'wordnet', 'stopwords', 'omw-1.4', 'vader_lexicon'], quiet=True)\n",
    "        \n",
    "        \n",
    "    def get_response(self, url, headers):\n",
    "        r = requests.get(url, headers=headers)\n",
    "        soup = bs(r.content)\n",
    "        return soup\n",
    "    \n",
    "    def get_datahub_csv(self, URL, filename):\n",
    "        soup = self.get_response(URL, self.HEADERS)\n",
    "        download_url = soup.find_all('table')[1].find_all('a')[1]['href']\n",
    "        download_url = self.DATAHUB_URL + download_url\n",
    "        with open(self.DATA_LOCATION + filename, 'wb') as f:\n",
    "            f.write(requests.get(download_url).content)\n",
    "    \n",
    "    def get_yahoo_news_articles_df(self):\n",
    "        yahoo_news_driver = webdriver.Chrome(options=self.OPTIONS)\n",
    "\n",
    "        yahoo_news_driver.get(self.YAHOO_URL)\n",
    "        time.sleep(1)\n",
    "        yahoo_news_driver.find_element(By.CLASS_NAME, 'reject-all').click()\n",
    "        actions = ActionChains(yahoo_news_driver)\n",
    "        for i in range(1500):\n",
    "            actions.scroll_by_amount(0, 500)\n",
    "        actions.perform()\n",
    "        actions.reset_actions()\n",
    "\n",
    "        links = self.get_article_links(yahoo_news_driver)\n",
    "        df = pd.DataFrame({'title': [link.text for link in links], \n",
    "                           'url': [link.get_attribute('href') for link in links], \n",
    "                           'classes': [link.get_attribute('class') for link in links]})\n",
    "\n",
    "        yahoo_news_driver.quit()\n",
    "        df = df[df['url'] \\\n",
    "                .str.contains('/news/') & df['classes'].str.contains('js-content-viewer')]\\\n",
    "                .drop_duplicates(subset='url', keep='last')\\\n",
    "                .reset_index(drop=True)\n",
    "        return df\n",
    "    \n",
    "    def scrape_articles(self, urls):\n",
    "        articles = []\n",
    "        raw_articles = []\n",
    "        yahoo_driver = webdriver.Chrome(options=self.OPTIONS)\n",
    "        yahoo_driver.get(self.YAHOO_URL)\n",
    "        yahoo_driver.find_element(By.CLASS_NAME, 'reject-all').click()\n",
    "\n",
    "        for index, url in enumerate(urls):\n",
    "            yahoo_driver.get(url)\n",
    "            try:\n",
    "                yahoo_driver.find_element(By.CLASS_NAME, 'collapse-button').click()\n",
    "            except:\n",
    "                pass\n",
    "            article_p_elements = yahoo_driver.find_element(By.CLASS_NAME, 'caas-body').find_elements(By.TAG_NAME, 'p')\n",
    "            article_by_paragraph = [p.text for p in article_p_elements if p.text!='']\n",
    "\n",
    "            raw_article = [bs(element.get_attribute('outerHTML'), \"html.parser\") for element in article_p_elements]\n",
    "            article = \" \".join(article_by_paragraph)\n",
    "            clean_article = self.clean_text(article)\n",
    "            articles.append(clean_article)\n",
    "            raw_articles.append(raw_article)\n",
    "            self.progress_bar(index+1, len(urls))\n",
    "\n",
    "\n",
    "        yahoo_driver.quit()\n",
    "\n",
    "        return articles, raw_articles\n",
    "    \n",
    "    def download_yahoo_data(self, file_path):\n",
    "        \"\"\"\n",
    "        Downloadt gegevens van Yahoo Finance voor tickers die zijn vermeld in het opgegeven CSV-bestand.\n",
    "\n",
    "        Parameters:\n",
    "        - file_path (str): Bestandspad naar het CSV-bestand met ticker-symbolen.\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame: Gefilterd DataFrame met gedownloade gegevens.\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(file_path)\n",
    "        symbol_list = df.iloc[:, 0].tolist()\n",
    "\n",
    "        download = yf.download(symbol_list, group_by=\"ticker\")\n",
    "        data = download.copy()\n",
    "\n",
    "        filtered_data = data.dropna(axis=1, how='all')\n",
    "        filtered_data.columns = filtered_data.columns.remove_unused_levels()\n",
    "\n",
    "        remaining_tickers = list(filtered_data.columns.levels[0])\n",
    "        missing_tickers = list(set(symbol_list) - set(remaining_tickers))\n",
    "\n",
    "        return filtered_data    \n",
    "    \n",
    "    def get_card_titles(self):\n",
    "        titles = []\n",
    "        a_tags = self.soup.find_all('a', class_='card')\n",
    "        for i in range(len(a_tags)):\n",
    "            titles.append(a_tags[i].span.text)\n",
    "        return titles\n",
    "\n",
    "    def get_card_urls(self):\n",
    "        urls = []\n",
    "        a_tags = self.soup.find_all('a', class_='card')\n",
    "        for i in range(len(a_tags)):\n",
    "            urls.append(a_tags[i]['href'])\n",
    "        return urls\n",
    "    \n",
    "    def get_url_content(self, urls):\n",
    "        articles = []\n",
    "        raw_articles = []\n",
    "        for url in urls:\n",
    "            soup = self.get_response(url, self.HEADERS)\n",
    "            paragraphs = soup.find('div', class_='article-content').find_all('p')\n",
    "            article = ''.join([p.get_text(separator=' ') for p in paragraphs])\n",
    "            raw_articles.append(paragraphs)\n",
    "            articles.append(self.clean_text(article))\n",
    "\n",
    "        return articles, raw_articles\n",
    "    \n",
    "    def get_symbols(self, raw_articles, href_ref):\n",
    "        symbol_list = []\n",
    "        for p_list in raw_articles:\n",
    "            article_symbols = []\n",
    "            for p in p_list:\n",
    "                symbols = [a_tag.text for a_tag in p.find_all('a') if re.search(href_ref, a_tag['href'])]\n",
    "                if symbols:\n",
    "                    for symbol in symbols:\n",
    "                        article_symbols.append(symbol)\n",
    "            symbol_list.append(article_symbols)\n",
    "        return symbol_list\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        return re.sub(r'[^\\w\\s]', '', text)\n",
    "            \n",
    "    def get_article_links(self, driver):\n",
    "        links = driver.find_elements(By.TAG_NAME, 'a')\n",
    "        return links\n",
    "    \n",
    "    def progress_bar(self, current, total, bar_length=20):\n",
    "        fraction = current / total\n",
    "\n",
    "        arrow = int(fraction * bar_length - 1) * '-' + '>'\n",
    "        padding = int(bar_length - len(arrow)) * ' '\n",
    "\n",
    "        ending = '\\n' if current == total else '\\r'\n",
    "\n",
    "        print(f'Progress: [{arrow}{padding}] {int(fraction*100)}%', end=ending)\n",
    "\n",
    "    def clean_and_tokenize(self, articles):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        tokenized_articles = []\n",
    "        for article in articles:\n",
    "            tokenized_article = nltk.word_tokenize(article)\n",
    "            tokenized_article = [token for token in tokenized_article if token not in stopwords.words('english')]\n",
    "            tokenized_article = [lemmatizer.lemmatize(token) for token in tokenized_article]\n",
    "\n",
    "            tokenized_articles.append(tokenized_article)\n",
    "\n",
    "        return tokenized_articles\n",
    "    \n",
    "    def combine_data(self, api_df, investopedia_df, yahoo_df, time_data):\n",
    "        df1 = investopedia_df.drop(['symbols'], axis=1)\n",
    "        df2 = yahoo_df.drop('classes', axis=1)\n",
    "        text_df = pd.concat([df1, df2])\n",
    "        text_df = text_df.reset_index(drop=True)\n",
    "        \n",
    "        # tokenize the articles and add them to a DataFrame\n",
    "        text_df['tokenized_article'] = self.clean_and_tokenize(text_df['article'])\n",
    "        \n",
    "        # apply sentiment analysis and add it to the DataFrame\n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "        text_df['positivity_score'] = [analyzer.polarity_scores(article)['pos'] for article in text_df['article']]\n",
    "        text_df['negativity_score'] = [analyzer.polarity_scores(article)['neg'] for article in text_df['article']]\n",
    "        \n",
    "        # get statistics from positive and negative sentiment analysis\n",
    "        pos = text_df.describe().T[:1].reset_index(drop=True)\n",
    "        neg = text_df.describe().T[1:].reset_index(drop=True)\n",
    "        \n",
    "        # rename columns in pos and neg so they can be concattinated\n",
    "        pos.columns = ['pos_' + col for col in pos.columns]\n",
    "        neg.columns = ['neg_' + col for col in neg.columns]\n",
    "        \n",
    "        # concattinate the positve and negative sentiment analysis statistics to a 1 row DataFrame\n",
    "        pos_neg_row = pd.concat([pos, neg], axis=1)\n",
    "        \n",
    "        api_df = api_df.iloc[::-1].reset_index(drop=False, names='Date')\n",
    "        \n",
    "        api_df.columns = ['_'.join(col).strip() for col in api_df.columns.values]\n",
    "        \n",
    "        combined_df = pd.concat([api_df, pos_neg_row], axis=1)\n",
    "        \n",
    "        combined_df['Date_'] = combined_df['Date_'].astype(str)\n",
    "        time_data['Date'] = time_data['Date'].astype(str)\n",
    "\n",
    "        final_df = combined_df.merge(time_data, how='left', left_on='Date_', right_on='Date')\n",
    "        \n",
    "        return final_df\n",
    "\n",
    "    def extract(self):\n",
    "        # get site data\n",
    "        self.soup = self.get_response(self.HOMEPAGE_URL, self.HEADERS)\n",
    "\n",
    "        # download csv data\n",
    "        # get_datahub_csv(self.VUSA_DOWNLOAD_URL, 'vusa_holdings.csv')\n",
    "        # get_datahub_csv(self.SP_DOWNLOAD_URL, 'time_data.csv')\n",
    "       \n",
    "        # get csv data\n",
    "        self.vusa_df = pd.read_csv(self.DATA_LOCATION + 'vusa_holdings.csv')\n",
    "        self.time_data_df = pd.read_csv(self.DATA_LOCATION + 'time_data.csv')\n",
    "\n",
    "        # get yahoo articles with titles, urls and css classes\n",
    "        self.yahoo_news_df = self.get_yahoo_news_articles_df()\n",
    "        self.yahoo_news_df = self.yahoo_news_df[:2]\n",
    "    \n",
    "        # scrape news articles from yahoo article urls\n",
    "        self.yahoo_news_articles, self.raw_yahoo_news_articles = self.scrape_articles(self.yahoo_news_df['url'])\n",
    "        \n",
    "        # get api data\n",
    "        self.api_df = self.download_yahoo_data('Datasets/vusa_holdings.csv')\n",
    "    \n",
    "    \n",
    "    def transform(self):        \n",
    "        # filter site data\n",
    "        titles = self.get_card_titles()\n",
    "        urls = self.get_card_urls()\n",
    "        \n",
    "        # transform filtered data to DataFrame(investopedia_df)\n",
    "        investopedia_df = pd.DataFrame({'url': urls, 'title': titles})\n",
    "        \n",
    "        # get articles from urls\n",
    "        articles, raw_articles = self.get_url_content(investopedia_df['url'])\n",
    "        \n",
    "        # filter symbols from the vusa_df csv data\n",
    "        symbols_in_vusa = self.vusa_df['Symbol'].tolist()\n",
    "\n",
    "        # add scraped articles to DataFrame\n",
    "        self.yahoo_news_df['article'] = self.yahoo_news_articles\n",
    "\n",
    "        \n",
    "        \n",
    "        # get symbols from article\n",
    "        symbols = self.get_symbols(raw_articles, 'widgetsymbol')\n",
    "        \n",
    "        # add articles and symbols to DataFrame(investopedia_df)\n",
    "        investopedia_df['article'] = articles\n",
    "        investopedia_df['symbols'] = symbols\n",
    "        \n",
    "        # filter DataFrame(investopedia_df) to only have symbols that occur in the symbols_in_vusa\n",
    "        investopedia_df = investopedia_df[investopedia_df['symbols'].apply(lambda symbols: any(symbol in symbols_in_vusa for symbol in symbols))]\n",
    "        \n",
    "        # combine DataFrames\n",
    "        self.df = self.combine_data(self.api_df, investopedia_df, self.yahoo_news_df, self.time_data_df)\n",
    "        self.df = self.df.fillna(0)\n",
    "          \n",
    "    def load(self, filename):\n",
    "        self.df.to_csv(filename)\n",
    "    \n",
    "    def ETL(self, filename):\n",
    "        self.extract()\n",
    "        self.transform()\n",
    "        self.load(filename)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3225b65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16129433",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [------------------->] 100%\n",
      "[*********************100%%**********************]  505 of 505 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "27 Failed downloads:\n",
      "['FBHS', 'RE', 'PKI', 'DISCK', 'BLL', 'FRC', 'ATVI', 'PBCT', 'FISV', 'SIVB', 'ANTM', 'TWTR', 'XLNX', 'VIAC', 'NLSN', 'WLTW', 'BRK.B', 'DRE', 'ABC', 'DISCA', 'CERN', 'KSU', 'CTXS', 'INFO', 'NLOK', 'FB']: Exception('%ticker%: No timezone found, symbol may be delisted')\n",
      "['BF.B']: Exception('%ticker%: No price data found, symbol may be delisted (1d 1925-02-25 -> 2024-02-02)')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline.ETL('Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad26226c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date_</th>\n",
       "      <th>NXPI_Open</th>\n",
       "      <th>NXPI_High</th>\n",
       "      <th>NXPI_Low</th>\n",
       "      <th>NXPI_Close</th>\n",
       "      <th>NXPI_Adj Close</th>\n",
       "      <th>NXPI_Volume</th>\n",
       "      <th>PM_Open</th>\n",
       "      <th>PM_High</th>\n",
       "      <th>PM_Low</th>\n",
       "      <th>...</th>\n",
       "      <th>Date</th>\n",
       "      <th>SP500</th>\n",
       "      <th>Dividend</th>\n",
       "      <th>Earnings</th>\n",
       "      <th>Consumer Price Index</th>\n",
       "      <th>Long Interest Rate</th>\n",
       "      <th>Real Price</th>\n",
       "      <th>Real Dividend</th>\n",
       "      <th>Real Earnings</th>\n",
       "      <th>PE10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-02-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-02-01</td>\n",
       "      <td>211.690002</td>\n",
       "      <td>214.039993</td>\n",
       "      <td>209.440002</td>\n",
       "      <td>213.009995</td>\n",
       "      <td>213.009995</td>\n",
       "      <td>1968200.0</td>\n",
       "      <td>91.230003</td>\n",
       "      <td>93.290001</td>\n",
       "      <td>90.949997</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-31</td>\n",
       "      <td>212.880005</td>\n",
       "      <td>215.320007</td>\n",
       "      <td>209.750000</td>\n",
       "      <td>210.570007</td>\n",
       "      <td>210.570007</td>\n",
       "      <td>3150700.0</td>\n",
       "      <td>91.860001</td>\n",
       "      <td>91.889999</td>\n",
       "      <td>90.790001</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-30</td>\n",
       "      <td>215.550003</td>\n",
       "      <td>217.250000</td>\n",
       "      <td>214.229996</td>\n",
       "      <td>215.169998</td>\n",
       "      <td>215.169998</td>\n",
       "      <td>1708200.0</td>\n",
       "      <td>91.339996</td>\n",
       "      <td>91.930000</td>\n",
       "      <td>90.480003</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-29</td>\n",
       "      <td>216.929993</td>\n",
       "      <td>217.570007</td>\n",
       "      <td>213.289993</td>\n",
       "      <td>217.470001</td>\n",
       "      <td>217.470001</td>\n",
       "      <td>1828000.0</td>\n",
       "      <td>91.209999</td>\n",
       "      <td>91.800003</td>\n",
       "      <td>90.970001</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15624</th>\n",
       "      <td>1962-01-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15625</th>\n",
       "      <td>1962-01-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15626</th>\n",
       "      <td>1962-01-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15627</th>\n",
       "      <td>1962-01-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15628</th>\n",
       "      <td>1962-01-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15629 rows × 2895 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date_   NXPI_Open   NXPI_High    NXPI_Low  NXPI_Close  \\\n",
       "0      2024-02-02    0.000000    0.000000    0.000000    0.000000   \n",
       "1      2024-02-01  211.690002  214.039993  209.440002  213.009995   \n",
       "2      2024-01-31  212.880005  215.320007  209.750000  210.570007   \n",
       "3      2024-01-30  215.550003  217.250000  214.229996  215.169998   \n",
       "4      2024-01-29  216.929993  217.570007  213.289993  217.470001   \n",
       "...           ...         ...         ...         ...         ...   \n",
       "15624  1962-01-08    0.000000    0.000000    0.000000    0.000000   \n",
       "15625  1962-01-05    0.000000    0.000000    0.000000    0.000000   \n",
       "15626  1962-01-04    0.000000    0.000000    0.000000    0.000000   \n",
       "15627  1962-01-03    0.000000    0.000000    0.000000    0.000000   \n",
       "15628  1962-01-02    0.000000    0.000000    0.000000    0.000000   \n",
       "\n",
       "       NXPI_Adj Close  NXPI_Volume    PM_Open    PM_High     PM_Low  ...  \\\n",
       "0            0.000000          0.0   0.000000   0.000000   0.000000  ...   \n",
       "1          213.009995    1968200.0  91.230003  93.290001  90.949997  ...   \n",
       "2          210.570007    3150700.0  91.860001  91.889999  90.790001  ...   \n",
       "3          215.169998    1708200.0  91.339996  91.930000  90.480003  ...   \n",
       "4          217.470001    1828000.0  91.209999  91.800003  90.970001  ...   \n",
       "...               ...          ...        ...        ...        ...  ...   \n",
       "15624        0.000000          0.0   0.000000   0.000000   0.000000  ...   \n",
       "15625        0.000000          0.0   0.000000   0.000000   0.000000  ...   \n",
       "15626        0.000000          0.0   0.000000   0.000000   0.000000  ...   \n",
       "15627        0.000000          0.0   0.000000   0.000000   0.000000  ...   \n",
       "15628        0.000000          0.0   0.000000   0.000000   0.000000  ...   \n",
       "\n",
       "       Date  SP500  Dividend  Earnings  Consumer Price Index  \\\n",
       "0         0    0.0       0.0       0.0                   0.0   \n",
       "1         0    0.0       0.0       0.0                   0.0   \n",
       "2         0    0.0       0.0       0.0                   0.0   \n",
       "3         0    0.0       0.0       0.0                   0.0   \n",
       "4         0    0.0       0.0       0.0                   0.0   \n",
       "...     ...    ...       ...       ...                   ...   \n",
       "15624     0    0.0       0.0       0.0                   0.0   \n",
       "15625     0    0.0       0.0       0.0                   0.0   \n",
       "15626     0    0.0       0.0       0.0                   0.0   \n",
       "15627     0    0.0       0.0       0.0                   0.0   \n",
       "15628     0    0.0       0.0       0.0                   0.0   \n",
       "\n",
       "       Long Interest Rate  Real Price  Real Dividend  Real Earnings  PE10  \n",
       "0                     0.0         0.0            0.0            0.0   0.0  \n",
       "1                     0.0         0.0            0.0            0.0   0.0  \n",
       "2                     0.0         0.0            0.0            0.0   0.0  \n",
       "3                     0.0         0.0            0.0            0.0   0.0  \n",
       "4                     0.0         0.0            0.0            0.0   0.0  \n",
       "...                   ...         ...            ...            ...   ...  \n",
       "15624                 0.0         0.0            0.0            0.0   0.0  \n",
       "15625                 0.0         0.0            0.0            0.0   0.0  \n",
       "15626                 0.0         0.0            0.0            0.0   0.0  \n",
       "15627                 0.0         0.0            0.0            0.0   0.0  \n",
       "15628                 0.0         0.0            0.0            0.0   0.0  \n",
       "\n",
       "[15629 rows x 2895 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d87208",
   "metadata": {},
   "source": [
    "# Referentielijst\n",
    "- Fondsbeheerder|NOT,Disclosed|Vanguard S&P 500 UCITS ETF (EUR)|ISIN:IE00B3XXRP09. (n.d.). https://www.morningstar.nl/nl/etf/snapshot/snapshot.aspx?id=0P0000YXKB&tab=4&InvestmentType=FE\n",
    "- Wat zijn ETF’s | Educatie | BlackRock. (n.d.). BlackRock. https://www.blackrock.com/be/individual/nl/educatie/etfs-uitgelegd#Wat-zijn-ETF'\n",
    "- S&P 500 | Wat is de S&P 500? | Beleggingswiki - Semmie.nl. (n.d.). Semmie. https://semmie.nl/wiki/sp-500/#:~:text=De%20S%26P%20500%20is%20een,ontwikkeling%20van%20de%20financi%C3%ABle%20markt.\n",
    "- SWOCC. (2020, June 8). De invloed van mediaberichtgeving op beurskoersen - SWOCC. https://www.swocc.nl/kennisbank-item/de-invloed-van-mediaberichtgeving-op-beurskoersen/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
