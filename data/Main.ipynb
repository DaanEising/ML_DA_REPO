{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "a96dba24",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "import httpx\n",
    "import re\n",
    "import requests\n",
    "import codecs\n",
    "import time\n",
    "import nltk\n",
    "\n",
    "import threading\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "import yfinance as yf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "eebd4d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #constants\n",
    "# HEADERS = {'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "#            'Accept-Encoding': 'gzip, deflate, br',\n",
    "#            'Accept-Language': 'en-US,en;q=0.9,nl;q=0.8',\n",
    "#            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'}\n",
    "\n",
    "# DATA_LOCATION = 'Datasets/'\n",
    "\n",
    "# HOMEPAGE_URL = 'https://www.investopedia.com/markets-news-4427704'\n",
    "\n",
    "# VUSA_DOWNLOAD_URL = 'https://datahub.io/core/s-and-p-500-companies'\n",
    "# SP_DOWNLOAD_URL = 'https://datahub.io/core/s-and-p-500'\n",
    "# DATAHUB_URL = 'https://datahub.io'\n",
    "\n",
    "# YAHOO_URL = 'https://finance.yahoo.com/'\n",
    "\n",
    "# #don\n",
    "# nltk.download(['punkt', 'wordnet', 'stopwords', 'omw-1.4', 'vader_lexicon'])\n",
    "\n",
    "# options = Options()\n",
    "# options.add_argument('--headless')\n",
    "# options.add_argument('--disable-gpu') \n",
    "\n",
    "# soup = get_response(HOMEPAGE_URL, HEADERS)\n",
    "\n",
    "# titles = get_card_titles()\n",
    "# urls = get_card_urls()\n",
    "\n",
    "# investopedia_df = pd.DataFrame({'url': urls, 'title': titles})\n",
    "\n",
    "# articles, raw_articles = get_url_content(investopedia_df['url'])\n",
    "\n",
    "# investopedia_df['article'] = articles\n",
    "\n",
    "# symbols = get_symbols(raw_articles, 'widgetsymbol')\n",
    "\n",
    "# investopedia_df['symbols'] = symbols\n",
    "\n",
    "# #holdings\n",
    "# get_datahub_csv(VUSA_DOWNLOAD_URL, 'vusa_holdings.csv')\n",
    "\n",
    "# #publieke dataset\n",
    "# get_datahub_csv(SP_DOWNLOAD_URL, 'time_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "28046216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# options = Options()\n",
    "# options.add_argument('--headless')\n",
    "# options.add_argument('--disable-gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "8569ca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_response(url, cookies={}, headers=HEADERS):\n",
    "#     r = httpx.get(url, cookies=cookies, headers=headers, follow_redirects=True)\n",
    "#     soup = bs(r.content)\n",
    "#     return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "c09c9c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup = get_response(HOMEPAGE_URL, HEADERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "146a16d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_card_titles():\n",
    "#     titles = []\n",
    "#     a_tags = soup.find_all('a', class_='card')\n",
    "#     for i in range(len(a_tags)):\n",
    "#         titles.append(a_tags[i].span.text)\n",
    "#     return titles\n",
    "\n",
    "# def get_card_urls():\n",
    "#     urls = []\n",
    "#     a_tags = soup.find_all('a', class_='card')\n",
    "#     for i in range(len(a_tags)):\n",
    "#         urls.append(a_tags[i]['href'])\n",
    "#     return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "8b05953a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# titles = get_card_titles()\n",
    "# urls = get_card_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "6a3b8fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# investopedia_df = pd.DataFrame({'url': urls, 'title': titles})\n",
    "# display(investopedia_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "58369553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_text(text):\n",
    "#     return re.sub(r'[^\\w\\s]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "00c0fc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_url_content(urls):\n",
    "#     articles = []\n",
    "#     raw_articles = []\n",
    "#     for url in urls:\n",
    "#         soup = get_response(url)\n",
    "#         paragraphs = soup.find('div', class_='article-content').find_all('p')\n",
    "#         article = ''.join([p.get_text(separator=' ') for p in paragraphs])\n",
    "#         raw_articles.append(paragraphs)\n",
    "#         articles.append(clean_text(article))\n",
    "        \n",
    "#     return articles, raw_articles\n",
    "\n",
    "# def get_symbols(raw_articles, href_ref):\n",
    "#     symbol_list = []\n",
    "#     for p_list in raw_articles:\n",
    "#         article_symbols = []\n",
    "#         for p in p_list:\n",
    "#             symbols = [a_tag.text for a_tag in p.find_all('a') if re.search(href_ref, a_tag['href'])]\n",
    "#             if symbols:\n",
    "#                 for symbol in symbols:\n",
    "#                     article_symbols.append(symbol)\n",
    "#         symbol_list.append(article_symbols)\n",
    "#     return symbol_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "8ac26660",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# articles, raw_articles = get_url_content(investopedia_df['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "ef5dcd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# investopedia_df['article'] = articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "17bf5e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# symbols = get_symbols(raw_articles, 'widgetsymbol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "bed7eae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# investopedia_df['symbols'] = symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "aeb93652",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# display(investopedia_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "a8d83664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_datahub_csv(URL, filename):\n",
    "#     soup = get_response(URL)\n",
    "#     download_url = soup.find_all('table')[1].find_all('a')[1]['href']\n",
    "#     download_url = DATAHUB_URL + download_url\n",
    "#     with open(DATA_LOCATION + filename, 'wb') as f:\n",
    "#         f.write(requests.get(download_url).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "41fb11cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get_datahub_csv(VUSA_DOWNLOAD_URL, 'vusa_holdings.csv')\n",
    "# get_datahub_csv(SP_DOWNLOAD_URL, 'time_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "ef3be3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vusa_df = pd.read_csv(DATA_LOCATION + 'vusa_holdings.csv')\n",
    "# symbols_in_vusa = vusa_df['Symbol'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "977dd48a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#investopedia filteren op vusa symbols\n",
    "# investopedia_df = investopedia_df[investopedia_df['symbols'].apply(lambda symbols: any(symbol in symbols_in_vusa for symbol in symbols))]\n",
    "# #combine dataframes\n",
    "# df = combine_dfs(investopedia_df, yahoo_news_df)\n",
    "\n",
    "# #clean and tokenize the articles\n",
    "# df['tokenized_article'] = clean_and_tokenize(df['article'])\n",
    "\n",
    "# #do sentiment analysis\n",
    "# analyzer = SentimentIntensityAnalyzer()\n",
    "# df['positivity_score'] = [analyzer.polarity_scores(article)['pos'] for article in df['article']]\n",
    "# df['negativity_score'] = [analyzer.polarity_scores(article)['neg'] for article in df['article']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "bd721962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_article_links(driver):\n",
    "#     links = driver.find_elements(By.TAG_NAME, 'a')\n",
    "#     return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "669ceeef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def get_yahoo_news_articles_df():\n",
    "#     yahoo_news_driver = webdriver.Chrome(options=options)\n",
    "\n",
    "#     yahoo_news_driver.get(YAHOO_URL)\n",
    "#     time.sleep(1)\n",
    "#     yahoo_news_driver.find_element(By.CLASS_NAME, 'reject-all').click()\n",
    "#     actions = ActionChains(yahoo_news_driver)\n",
    "#     for i in range(1500):\n",
    "#         actions.scroll_by_amount(0, 500)\n",
    "#     actions.perform()\n",
    "#     actions.reset_actions()\n",
    "\n",
    "#     links = get_article_links(yahoo_news_driver)\n",
    "#     df = pd.DataFrame({'title': [link.text for link in links], \n",
    "#                        'url': [link.get_attribute('href') for link in links], \n",
    "#                        'classes': [link.get_attribute('class') for link in links]})\n",
    "    \n",
    "#     yahoo_news_driver.quit()\n",
    "#     df = df[df['url'] \\\n",
    "#             .str.contains('/news/') & df['classes'].str.contains('js-content-viewer')]\\\n",
    "#             .drop_duplicates(subset='url', keep='last')\\\n",
    "#             .reset_index(drop=True)\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "082622a0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# yahoo_news_df = get_yahoo_news_articles_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "dc814d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(yahoo_news_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "0d71d904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def progress_bar(current, total, bar_length=20):\n",
    "#     fraction = current / total\n",
    "\n",
    "#     arrow = int(fraction * bar_length - 1) * '-' + '>'\n",
    "#     padding = int(bar_length - len(arrow)) * ' '\n",
    "\n",
    "#     ending = '\\n' if current == total else '\\r'\n",
    "\n",
    "#     print(f'Progress: [{arrow}{padding}] {int(fraction*100)}%', end=ending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "d1b2a369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scrape_articles(urls):\n",
    "#     articles = []\n",
    "#     raw_articles = []\n",
    "#     yahoo_driver = webdriver.Chrome(options=options)\n",
    "#     yahoo_driver.get(YAHOO_URL)\n",
    "#     yahoo_driver.find_element(By.CLASS_NAME, 'reject-all').click()\n",
    "\n",
    "#     for index, url in enumerate(urls):\n",
    "#         yahoo_driver.get(url)\n",
    "#         try:\n",
    "#             yahoo_driver.find_element(By.CLASS_NAME, 'collapse-button').click()\n",
    "#         except:\n",
    "#             pass\n",
    "#         article_p_elements = yahoo_driver.find_element(By.CLASS_NAME, 'caas-body').find_elements(By.TAG_NAME, 'p')\n",
    "#         article_by_paragraph = [p.text for p in article_p_elements if p.text!='']\n",
    "        \n",
    "#         raw_article = [bs(element.get_attribute('outerHTML'), \"html.parser\") for element in article_p_elements]\n",
    "#         article = \" \".join(article_by_paragraph)\n",
    "#         clean_article = clean_text(article)\n",
    "#         articles.append(clean_article)\n",
    "#         raw_articles.append(raw_article)\n",
    "#         progress_bar(index+1, len(urls))\n",
    "    \n",
    "        \n",
    "#     yahoo_driver.quit()\n",
    "    \n",
    "#     return articles, raw_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "fabc771b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# yahoo_news_articles, raw_yahoo_news_articles = scrape_articles(yahoo_news_df['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "d75618b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yahoo_news_df['article'] = yahoo_news_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "83c82f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yahoo_news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "0754b62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def combine_dfs(investopedia_df, yahoo_df):\n",
    "#     df1 = investopedia_df.drop(['symbols'], axis=1)\n",
    "#     df2 = yahoo_df.drop('classes', axis=1)\n",
    "#     article_df = pd.concat([df1, df2])\n",
    "#     article_df = article_df.reset_index(drop=True)\n",
    "#     return article_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "9fd17a24",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# display(investopedia_df)\n",
    "# df = combine_dfs(investopedia_df, yahoo_news_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "2f477d9a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# def clean_and_tokenize(articles):\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "#     tokenized_articles = []\n",
    "#     for article in articles:\n",
    "#         tokenized_article = nltk.word_tokenize(article)\n",
    "#         tokenized_article = [token for token in tokenized_article if token not in stopwords.words('english')]\n",
    "#         tokenized_article = [lemmatizer.lemmatize(token) for token in tokenized_article]\n",
    "        \n",
    "#         tokenized_articles.append(tokenized_article)\n",
    "        \n",
    "#     return tokenized_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "2a8e4eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['tokenized_article'] = clean_and_tokenize(df['article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "217c5875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "895b007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyzer = SentimentIntensityAnalyzer()\n",
    "# df['positivity_score'] = [analyzer.polarity_scores(article)['pos'] for article in df['article']]\n",
    "# df['negativity_score'] = [analyzer.polarity_scores(article)['neg'] for article in df['article']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "667613b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos = df.describe().T[:1].reset_index(drop=True)\n",
    "# neg = df.describe().T[1:].reset_index(drop=True)\n",
    "\n",
    "# pos.columns = ['pos_' + col for col in pos.columns]\n",
    "# neg.columns = ['neg_' + col for col in neg.columns]\n",
    "\n",
    "# pd.concat([pos, neg], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "e2cacf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df2 = pd.read_csv(DATA_LOCATION + 'time_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "cc7ed872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "e474c313",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    def __init__(self):\n",
    "        self.HEADERS = {'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "           'Accept-Encoding': 'gzip, deflate, br',\n",
    "           'Accept-Language': 'en-US,en;q=0.9,nl;q=0.8',\n",
    "           'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'}\n",
    "\n",
    "        self.DATA_LOCATION = 'Datasets/'\n",
    "\n",
    "        self.HOMEPAGE_URL = 'https://www.investopedia.com/markets-news-4427704'\n",
    "\n",
    "        self.VUSA_DOWNLOAD_URL = 'https://datahub.io/core/s-and-p-500-companies'\n",
    "        self.SP_DOWNLOAD_URL = 'https://datahub.io/core/s-and-p-500'\n",
    "        self.DATAHUB_URL = 'https://datahub.io'\n",
    "\n",
    "        self.YAHOO_URL = 'https://finance.yahoo.com/'\n",
    "\n",
    "        self.OPTIONS = Options()\n",
    "        self.OPTIONS.add_argument('--headless')\n",
    "        self.OPTIONS.add_argument('--disable-gpu') \n",
    "        \n",
    "        nltk.download(['punkt', 'wordnet', 'stopwords', 'omw-1.4', 'vader_lexicon'])\n",
    "        \n",
    "        \n",
    "    def get_response(self, url, headers, cookies={}):\n",
    "        r = httpx.get(url, cookies=cookies, headers=headers, follow_redirects=True)\n",
    "        soup = bs(r.content)\n",
    "        return soup\n",
    "    \n",
    "    def get_card_titles(self):\n",
    "        titles = []\n",
    "        a_tags = self.soup.find_all('a', class_='card')\n",
    "        for i in range(len(a_tags)):\n",
    "            titles.append(a_tags[i].span.text)\n",
    "        return titles\n",
    "\n",
    "    def get_card_urls(self):\n",
    "        urls = []\n",
    "        a_tags = self.soup.find_all('a', class_='card')\n",
    "        for i in range(len(a_tags)):\n",
    "            urls.append(a_tags[i]['href'])\n",
    "        return urls\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        return re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    def get_url_content(self, urls):\n",
    "        articles = []\n",
    "        raw_articles = []\n",
    "        for url in urls:\n",
    "            soup = self.get_response(url, self.HEADERS)\n",
    "            paragraphs = soup.find('div', class_='article-content').find_all('p')\n",
    "            article = ''.join([p.get_text(separator=' ') for p in paragraphs])\n",
    "            raw_articles.append(paragraphs)\n",
    "            articles.append(self.clean_text(article))\n",
    "\n",
    "        return articles, raw_articles\n",
    "\n",
    "    def get_symbols(self, raw_articles, href_ref):\n",
    "        symbol_list = []\n",
    "        for p_list in raw_articles:\n",
    "            article_symbols = []\n",
    "            for p in p_list:\n",
    "                symbols = [a_tag.text for a_tag in p.find_all('a') if re.search(href_ref, a_tag['href'])]\n",
    "                if symbols:\n",
    "                    for symbol in symbols:\n",
    "                        article_symbols.append(symbol)\n",
    "            symbol_list.append(article_symbols)\n",
    "        return symbol_list\n",
    "    \n",
    "    def get_datahub_csv(self, URL, filename):\n",
    "        soup = self.get_response(URL, self.HEADERS)\n",
    "        download_url = soup.find_all('table')[1].find_all('a')[1]['href']\n",
    "        download_url = self.DATAHUB_URL + download_url\n",
    "        with open(self.DATA_LOCATION + filename, 'wb') as f:\n",
    "            f.write(requests.get(download_url).content)\n",
    "            \n",
    "    def get_article_links(self, driver):\n",
    "        links = driver.find_elements(By.TAG_NAME, 'a')\n",
    "        return links\n",
    "    \n",
    "    def get_yahoo_news_articles_df(self):\n",
    "        yahoo_news_driver = webdriver.Chrome(options=self.OPTIONS)\n",
    "\n",
    "        yahoo_news_driver.get(self.YAHOO_URL)\n",
    "        time.sleep(1)\n",
    "        yahoo_news_driver.find_element(By.CLASS_NAME, 'reject-all').click()\n",
    "        actions = ActionChains(yahoo_news_driver)\n",
    "        for i in range(1500):\n",
    "            actions.scroll_by_amount(0, 500)\n",
    "        actions.perform()\n",
    "        actions.reset_actions()\n",
    "\n",
    "        links = self.get_article_links(yahoo_news_driver)\n",
    "        df = pd.DataFrame({'title': [link.text for link in links], \n",
    "                           'url': [link.get_attribute('href') for link in links], \n",
    "                           'classes': [link.get_attribute('class') for link in links]})\n",
    "\n",
    "        yahoo_news_driver.quit()\n",
    "        df = df[df['url'] \\\n",
    "                .str.contains('/news/') & df['classes'].str.contains('js-content-viewer')]\\\n",
    "                .drop_duplicates(subset='url', keep='last')\\\n",
    "                .reset_index(drop=True)\n",
    "        return df\n",
    "    \n",
    "    def progress_bar(self, current, total, bar_length=20):\n",
    "        fraction = current / total\n",
    "\n",
    "        arrow = int(fraction * bar_length - 1) * '-' + '>'\n",
    "        padding = int(bar_length - len(arrow)) * ' '\n",
    "\n",
    "        ending = '\\n' if current == total else '\\r'\n",
    "\n",
    "        print(f'Progress: [{arrow}{padding}] {int(fraction*100)}%', end=ending)\n",
    "    \n",
    "    def scrape_articles(self, urls):\n",
    "        articles = []\n",
    "        raw_articles = []\n",
    "        yahoo_driver = webdriver.Chrome(options=self.OPTIONS)\n",
    "        yahoo_driver.get(self.YAHOO_URL)\n",
    "        yahoo_driver.find_element(By.CLASS_NAME, 'reject-all').click()\n",
    "\n",
    "        for index, url in enumerate(urls):\n",
    "            yahoo_driver.get(url)\n",
    "            try:\n",
    "                yahoo_driver.find_element(By.CLASS_NAME, 'collapse-button').click()\n",
    "            except:\n",
    "                pass\n",
    "            article_p_elements = yahoo_driver.find_element(By.CLASS_NAME, 'caas-body').find_elements(By.TAG_NAME, 'p')\n",
    "            article_by_paragraph = [p.text for p in article_p_elements if p.text!='']\n",
    "\n",
    "            raw_article = [bs(element.get_attribute('outerHTML'), \"html.parser\") for element in article_p_elements]\n",
    "            article = \" \".join(article_by_paragraph)\n",
    "            clean_article = self.clean_text(article)\n",
    "            articles.append(clean_article)\n",
    "            raw_articles.append(raw_article)\n",
    "            self.progress_bar(index+1, len(urls))\n",
    "\n",
    "\n",
    "        yahoo_driver.quit()\n",
    "\n",
    "        return articles, raw_articles\n",
    "\n",
    "    def clean_and_tokenize(self, articles):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        tokenized_articles = []\n",
    "        for article in articles:\n",
    "            tokenized_article = nltk.word_tokenize(article)\n",
    "            tokenized_article = [token for token in tokenized_article if token not in stopwords.words('english')]\n",
    "            tokenized_article = [lemmatizer.lemmatize(token) for token in tokenized_article]\n",
    "\n",
    "            tokenized_articles.append(tokenized_article)\n",
    "\n",
    "        return tokenized_articles\n",
    "    \n",
    "    def download_yahoo_data(self, file_path):\n",
    "        \"\"\"\n",
    "        Downloadt gegevens van Yahoo Finance voor tickers die zijn vermeld in het opgegeven CSV-bestand.\n",
    "\n",
    "        Parameters:\n",
    "        - file_path (str): Bestandspad naar het CSV-bestand met ticker-symbolen.\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame: Gefilterd DataFrame met gedownloade gegevens.\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(file_path)\n",
    "        symbol_list = df.iloc[:, 0].tolist()\n",
    "\n",
    "        download = yf.download(symbol_list, group_by=\"ticker\")\n",
    "        data = download.copy()\n",
    "\n",
    "        filtered_data = data.dropna(axis=1, how='all')\n",
    "        filtered_data.columns = filtered_data.columns.remove_unused_levels()\n",
    "\n",
    "        remaining_tickers = list(filtered_data.columns.levels[0])\n",
    "        missing_tickers = list(set(symbol_list) - set(remaining_tickers))\n",
    "\n",
    "#         print(\"Ontbrekende tickers en het aantal:\", missing_tickers, len(missing_tickers))\n",
    "#         display(filtered_data, filtered_data.shape)\n",
    "\n",
    "        return filtered_data    \n",
    "    \n",
    "    def combine_data(self, api_df, investopedia_df, yahoo_df, time_data):\n",
    "        df1 = investopedia_df.drop(['symbols'], axis=1)\n",
    "        df2 = yahoo_df.drop('classes', axis=1)\n",
    "        text_df = pd.concat([df1, df2])\n",
    "        text_df = text_df.reset_index(drop=True)\n",
    "        \n",
    "        # tokenize the articles and add them to a DataFrame\n",
    "        text_df['tokenized_article'] = self.clean_and_tokenize(text_df['article'])\n",
    "        \n",
    "        # apply sentiment analysis and add it to the DataFrame\n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "        text_df['positivity_score'] = [analyzer.polarity_scores(article)['pos'] for article in text_df['article']]\n",
    "        text_df['negativity_score'] = [analyzer.polarity_scores(article)['neg'] for article in text_df['article']]\n",
    "        \n",
    "        # get statistics from positive and negative sentiment analysis\n",
    "        pos = text_df.describe().T[:1].reset_index(drop=True)\n",
    "        neg = text_df.describe().T[1:].reset_index(drop=True)\n",
    "        \n",
    "        # rename columns in pos and neg so they can be concattinated\n",
    "        pos.columns = ['pos_' + col for col in pos.columns]\n",
    "        neg.columns = ['neg_' + col for col in neg.columns]\n",
    "        \n",
    "        # concattinate the positve and negative sentiment analysis statistics to a 1 row DataFrame\n",
    "        pos_neg_row = pd.concat([pos, neg], axis=1)\n",
    "        \n",
    "        api_df = api_df.iloc[::-1].reset_index(drop=False)\n",
    "\n",
    "        combined_df = pd.concat([api_df, pos_neg_row], axis=1)\n",
    "        \n",
    "        final_df = combined_df.merge(time_data, how='left', on='Date')\n",
    "        \n",
    "        return combined_df\n",
    "\n",
    "    def extract(self):\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "        # get site data\n",
    "        self.soup = self.get_response(self.HOMEPAGE_URL, self.HEADERS)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------   \n",
    "        # download csv data\n",
    "        # get_datahub_csv(self.VUSA_DOWNLOAD_URL, 'vusa_holdings.csv')\n",
    "        # get_datahub_csv(self.SP_DOWNLOAD_URL, 'time_data.csv')\n",
    "       \n",
    "        # get csv data\n",
    "        self.vusa_df = pd.read_csv(self.DATA_LOCATION + 'vusa_holdings.csv')\n",
    "        self.time_data_df = pd.read_csv(self.DATA_LOCATION + 'time_data.csv')\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "        # get yahoo articles with titles, urls and css classes\n",
    "        self.yahoo_news_df = self.get_yahoo_news_articles_df()\n",
    "    \n",
    "        # scrape news articles from yahoo article urls\n",
    "        self.yahoo_news_articles, self.raw_yahoo_news_articles = self.scrape_articles(self.yahoo_news_df['url'])\n",
    "        \n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "        # get api data\n",
    "        self.api_df = self.download_yahoo_data('Datasets/vusa_holdings.csv')\n",
    "    \n",
    "#-------------------------------------------------------------------------------------------------------  \n",
    "    def transform(self):        \n",
    "        \n",
    "#-------------------------------------------------------------------------------------------------------          \n",
    "        # filter site data\n",
    "        titles = self.get_card_titles()\n",
    "        urls = self.get_card_urls()\n",
    "        \n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "        # filter symbols from the vusa_df csv data\n",
    "        symbols_in_vusa = self.vusa_df['Symbol'].tolist()\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "        # add scraped articles to DataFrame\n",
    "        self.yahoo_news_df['article'] = self.yahoo_news_articles\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "        # transform filtered data to DataFrame(investopedia_df)\n",
    "        investopedia_df = pd.DataFrame({'url': urls, 'title': titles})\n",
    "        \n",
    "        # get articles from urls\n",
    "        articles, raw_articles = self.get_url_content(investopedia_df['url'])\n",
    "        \n",
    "        # get symbols from article\n",
    "        symbols = self.get_symbols(raw_articles, 'widgetsymbol')\n",
    "        \n",
    "        # add articles and symbols to DataFrame(investopedia_df)\n",
    "        investopedia_df['article'] = articles\n",
    "        investopedia_df['symbols'] = symbols\n",
    "        \n",
    "        # filter DataFrame(investopedia_df) to only have symbols that occur in the symbols_in_vusa\n",
    "        investopedia_df = investopedia_df[investopedia_df['symbols'].apply(lambda symbols: any(symbol in symbols_in_vusa for symbol in symbols))]\n",
    "        \n",
    "#-------------------------------------------------------------------------------------------------------    \n",
    "        # combine DataFrames\n",
    "        self.df = self.combine_data(self.api_df, investopedia_df, self.yahoo_news_df, self.time_data_df)\n",
    "        \n",
    "#-------------------------------------------------------------------------------------------------------   \n",
    "    def load(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "6bee3355",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mark\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Mark\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mark\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Mark\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Mark\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9407f93",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [----->              ] 34%\r"
     ]
    }
   ],
   "source": [
    "pipeline.extract()\n",
    "pipeline.transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee33d829",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
