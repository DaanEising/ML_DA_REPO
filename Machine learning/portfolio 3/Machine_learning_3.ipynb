{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88e23500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from librosa import load as lr_load\n",
    "import librosa as lr\n",
    "from librosa import amplitude_to_db\n",
    "from librosa.feature import melspectrogram, mfcc, chroma_stft, spectral_flatness, zero_crossing_rate, tempogram\n",
    "from librosa.beat import tempo, beat_track\n",
    "from librosa.feature import spectral_contrast as lf_spectral_contrast\n",
    "from librosa.feature import spectral_rolloff as lf_spectral_rolloff\n",
    "from librosa import pyin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import IPython.display as ipd\n",
    "from librosa import pyin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c348538",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\latic\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa as lr\n",
    "from IPython.display import display\n",
    "from librosa import amplitude_to_db\n",
    "from librosa.feature import chroma_stft, melspectrogram, mfcc, tempogram, zero_crossing_rate\n",
    "from pydub import AudioSegment\n",
    "from scipy.io import wavfile\n",
    "class AudioFeatureExtractor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def mfccs(self, data, sfreq):\n",
    "        mfcc_result = mfcc(y=data, sr=sfreq)\n",
    "        datadict = {}\n",
    "        for var in range(len(mfcc_result)):\n",
    "            datadict[f'mfcc{var + 1}_mean'] = np.mean(mfcc_result[var, :])\n",
    "        return datadict\n",
    "\n",
    "    def calculate_spectrograms(self, audio_clips, n_fft=2048, hop_length=512, win_length=None):\n",
    "        spectrograms = []\n",
    "        spectrograms_db = []\n",
    "\n",
    "        for clip in audio_clips:\n",
    "            stft_matrix = lr.stft(y=clip, n_fft=n_fft, hop_length=hop_length, win_length=win_length)\n",
    "            spectrogram = np.abs(stft_matrix)\n",
    "            spec_db = amplitude_to_db(S=spectrogram, ref=np.max)\n",
    "            spectrograms.append(spectrogram)\n",
    "            spectrograms_db.append(spec_db)\n",
    "\n",
    "        return spectrograms, spectrograms_db\n",
    "\n",
    "    def calculate_spectral_features(self, spectrograms):\n",
    "        bandwidths = []\n",
    "        centroids = []\n",
    "\n",
    "        for spectrogram in spectrograms:\n",
    "            spec_bw = lr.feature.spectral_bandwidth(S=spectrogram)\n",
    "            spec_cn = lr.feature.spectral_centroid(S=spectrogram)\n",
    "            bandwidths.append(spec_bw)\n",
    "            centroids.append(spec_cn)\n",
    "\n",
    "        return bandwidths, centroids\n",
    "\n",
    "    def calculate_spectral_contrast(self, data, sr, n_fft=2048, hop_length=512):\n",
    "        spectral_contrast = lr.feature.spectral_contrast(y=data, sr=sr, n_fft=n_fft, hop_length=hop_length)\n",
    "        return spectral_contrast\n",
    "\n",
    "    def calculate_tonnetz(self, data, sr):\n",
    "        tonnetz = lr.feature.tonnetz(y=data, sr=sr)\n",
    "        return tonnetz\n",
    "\n",
    "    def calculate_spectral_rolloff(self, data, sr, roll_percent=0.85, n_fft=2048, hop_length=512):\n",
    "        spectral_rolloff = lr.feature.spectral_rolloff(y=data, sr=sr, roll_percent=roll_percent, n_fft=n_fft, hop_length=hop_length)\n",
    "        return spectral_rolloff\n",
    "\n",
    "    def calculate_chroma_features(self, data, sr, n_fft=2048, hop_length=512):\n",
    "        chromagram = chroma_stft(y=data, sr=sr, n_fft=n_fft, hop_length=hop_length)\n",
    "        return chromagram\n",
    "\n",
    "    def calculate_mel_spectral_contrast(self, data, sr, n_fft=2048, hop_length=512):\n",
    "        mel_spectrogram = melspectrogram(y=data, sr=sr, n_fft=n_fft, hop_length=hop_length)\n",
    "        mel_spectral_contrast = lr.feature.spectral_contrast(S=mel_spectrogram)\n",
    "        return mel_spectral_contrast\n",
    "\n",
    "    def calculate_spectral_flatness(self, data, sr, n_fft=2048, hop_length=512):\n",
    "        flatness = lr.feature.spectral_flatness(y=data, n_fft=n_fft, hop_length=hop_length)\n",
    "        return flatness\n",
    "\n",
    "    def zero_crossing_rate_features(self, data, n_fft=2048, hop_length=512):\n",
    "        zcr = zero_crossing_rate(y=data, frame_length=n_fft, hop_length=hop_length)\n",
    "        return zcr\n",
    "\n",
    "    def calculate_tempogram(self, data, sr):\n",
    "        onset_env = lr.onset.onset_strength(y=data, sr=sr)\n",
    "        tempogram_result = lr.feature.tempogram(onset_envelope=onset_env, sr=sr)\n",
    "        return tempogram_result\n",
    "\n",
    "    def calculate_polyphonic_pitch(self, data, sr):\n",
    "        f0, voiced_flag, voiced_probs = pyin(y=data, fmin=lr.note_to_hz('C1'), fmax=lr.note_to_hz('C8'))\n",
    "        return f0, voiced_flag, voiced_probs\n",
    "\n",
    "    def rms_energy_features(self, data):\n",
    "        rms_energy = lr.feature.rms(y=data)[0]\n",
    "        return rms_energy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bf0717c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the AudioFeatureExtractor\n",
    "audio_extractor = AudioFeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff2c287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "335a8bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     filename    genre  mfcc1_mean  mfcc2_mean  mfcc3_mean  mfcc4_mean  \\\n",
      "0  m00248.wav    metal  -75.517509   81.911423  -22.081079   69.876999   \n",
      "1  m00230.wav  country  -89.736382   36.286201   11.469535   37.495590   \n",
      "2  m00637.wav   hiphop -122.780525   95.061287  -29.363251   46.780045   \n",
      "3  m00627.wav    metal  -57.683388  101.432320  -41.485245   55.130600   \n",
      "4  m00138.wav   reggae -198.632797  102.413582  -10.031448   30.802383   \n",
      "\n",
      "   mfcc5_mean  mfcc6_mean  mfcc7_mean  mfcc8_mean  ...  mfcc15_mean  \\\n",
      "0  -11.740438   25.740246  -18.518965   27.027710  ...   -10.154532   \n",
      "1    9.199136    3.740519    2.840358    6.811539  ...     1.711515   \n",
      "2  -15.998563   27.117586  -13.113779   20.258003  ...    -0.753110   \n",
      "3  -23.349279   28.151098  -12.139105   18.150204  ...   -12.406642   \n",
      "4   -3.310606   20.276924   -5.637373   28.095631  ...    -3.469470   \n",
      "\n",
      "   mfcc16_mean  mfcc17_mean  mfcc18_mean  mfcc19_mean  mfcc20_mean  \\\n",
      "0     6.781166   -10.487830     4.372530    -5.466021     2.970636   \n",
      "1    -1.367268    -4.376964    -2.601609    -3.528446    -4.227250   \n",
      "2     9.420004     1.262813     1.674718     0.486051     1.613319   \n",
      "3     5.426150    -8.844725     0.059281    -3.178474    -3.361844   \n",
      "4     2.016131    -6.987337     1.113921    -1.494757     1.955205   \n",
      "\n",
      "   mean_bandwidth                                   tempogram_result  \\\n",
      "0     2337.227557  [[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...   \n",
      "1     3053.066772  [[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...   \n",
      "2     2181.589027  [[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...   \n",
      "3     2092.439557  [[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...   \n",
      "4     2295.466240  [[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...   \n",
      "\n",
      "                                    polyphonic_pitch  \\\n",
      "0  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
      "1  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
      "2  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
      "3  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
      "4  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
      "\n",
      "                                          rms_energy  \n",
      "0  [0.065193295, 0.08010671, 0.11185529, 0.114852...  \n",
      "1  [0.061427664, 0.0676363, 0.07326668, 0.0588762...  \n",
      "2  [0.09934213, 0.11397154, 0.12187654, 0.0945995...  \n",
      "3  [0.08083046, 0.100887805, 0.11514855, 0.114735...  \n",
      "4  [0.041874763, 0.050757855, 0.056820925, 0.0582...  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the CSV file and the audio directory\n",
    "csv_path = \"labels_new.csv\"\n",
    "audio_directory = \"labeled\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Create an instance of the AudioFeatureExtractor class\n",
    "audio_extractor = AudioFeatureExtractor()\n",
    "\n",
    "# Audio features storage\n",
    "audio_features = []\n",
    "\n",
    "# Set the target length for audio truncation or padding\n",
    "target_length = 30 * 22050\n",
    "\n",
    "# Loop through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Construct the file path for the audio file\n",
    "    file_path = os.path.join(audio_directory, row['filename'])\n",
    "\n",
    "    # Load the audio file using librosa\n",
    "    data, sfreq = lr.load(file_path, sr=None)\n",
    "\n",
    "    # Truncate or pad the audio to the target length\n",
    "    if len(data) > target_length:\n",
    "        data = data[:target_length]\n",
    "    elif len(data) < target_length:\n",
    "        padding = target_length - len(data)\n",
    "        data = np.pad(data, (0, padding), mode='constant')\n",
    "\n",
    "    # Extract MFCC features\n",
    "    mfcc_features = audio_extractor.mfccs(data, sfreq)\n",
    "\n",
    "    # Extract other features using the AudioFeatureExtractor class\n",
    "    spectrograms, _ = audio_extractor.calculate_spectrograms([data])\n",
    "    bandwidths, centroids = audio_extractor.calculate_spectral_features(spectrograms)\n",
    "    spectral_contrast = audio_extractor.calculate_spectral_contrast(data, sfreq)\n",
    "    tonnetz = audio_extractor.calculate_tonnetz(data, sfreq)\n",
    "    spectral_rolloff = audio_extractor.calculate_spectral_rolloff(data, sfreq)\n",
    "    chroma_features = audio_extractor.calculate_chroma_features(data, sfreq)\n",
    "    mel_spectral_contrast = audio_extractor.calculate_mel_spectral_contrast(data, sfreq)\n",
    "    spectral_flatness = audio_extractor.calculate_spectral_flatness(data, sfreq)\n",
    "    zcr = audio_extractor.zero_crossing_rate_features(data)\n",
    "    tempogram_result = audio_extractor.calculate_tempogram(data, sfreq)\n",
    "    f0, _, _ = audio_extractor.calculate_polyphonic_pitch(data, sfreq)\n",
    "    rms_energy = audio_extractor.rms_energy_features(data)\n",
    "\n",
    "    # Combine all features into a dictionary\n",
    "    features_dict = {\n",
    "        **mfcc_features,\n",
    "        'mean_bandwidth': np.mean(bandwidths),\n",
    "        # ... (add other features to the dictionary)\n",
    "        'tempogram_result': tempogram_result,\n",
    "        'polyphonic_pitch': f0,\n",
    "        'rms_energy': rms_energy\n",
    "    }\n",
    "\n",
    "    # Append the features dictionary to the list\n",
    "    audio_features.append(features_dict)\n",
    "\n",
    "# Convert the list of feature dictionaries to a DataFrame\n",
    "audio_features_df = pd.DataFrame(audio_features)\n",
    "\n",
    "# Concatenate the original DataFrame with the new features DataFrame\n",
    "df = pd.concat([df, audio_features_df], axis=1)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96ab06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering door Mark\n",
    "\n",
    "class Clustering:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.cluster_labels = None\n",
    "\n",
    "    def cluster_kmeans(self, n_clusters=3):\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        self.cluster_labels = kmeans.fit_predict(self.scaled_data)\n",
    "        return np.column_stack((self.data, self.cluster_labels))\n",
    "    \n",
    "    def cluster_agglomerative(self, n_clusters=3):\n",
    "        agglomerative = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "        self.cluster_labels = agglomerative.fit_predict(self.scaled_data)\n",
    "        return np.column_stack((self.data, self.cluster_labels))\n",
    "    \n",
    "    def cluster_dbscan(self, eps=0.5, min_samples=5):\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        self.cluster_labels = dbscan.fit_predict(self.data)\n",
    "        return np.column_stack((self.data, self.cluster_labels))\n",
    "\n",
    "    def cluster_birch(self, n_clusters=3):\n",
    "        birch = Birch(n_clusters=n_clusters)\n",
    "        self.cluster_labels = birch.fit_predict(self.data)\n",
    "        return np.column_stack((self.data, self.cluster_labels))\n",
    "\n",
    "    def cluster_meanshift(self, bandwidth=0.5):\n",
    "        meanshift = MeanShift(bandwidth=bandwidth)\n",
    "        self.cluster_labels = meanshift.fit_predict(self.data)\n",
    "        return np.column_stack((self.data, self.cluster_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
